<!DOCTYPE html>
<html>
<head>
<title>
DLStudio-2.5.5.html
</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
p.morelinespace {
    line-height: 130%;
    font-weight: bold;
}
body {
    background-color: #f0f0f8;
}
hr.myhr1 {
    width:100%;
    height:8px;
    border:4px solid red;
}
</style>
</head>

<body>  
<hr class="myhr1">
<div style="color:blue; font-size:300%">  
  <strong>DLStudio</strong></div>
<div style="color:blue; font-size:150%"> Version 2.5.5, &nbsp; 2025-May-28<br>
<font="-1">A software platform for teaching the Deep Learning class at Purdue University</font><br>
</div>
<hr class="myhr1">
<br>
<div style="font-size:125%; line-height:130%; font-weight: bold">
DLStudio.py<br>
Version:&nbsp;&nbsp;2.5.5<br>
Author:&nbsp;&nbsp;Avinash&nbsp;Kak&nbsp;(kak@purdue.edu)<br>
Date:&nbsp;&nbsp;2025-May-28<br>
</div>
<br>
<table>
<tr>
<th style="text-align:left vertical-align:top">
<div style="font-size:125%">
<b>Download Version 2.5.5:</b>&nbsp;&nbsp;&nbsp;&nbsp;    
<a HREF="https://engineering.purdue.edu/kak/distDLS/DLStudio-2.5.5.tar.gz?download">gztar</a> 
</div>
<br>
<br>
<br>
</th>
<td style="text-align:center vertical-align:top padding:0">
<div style="text-align:center">
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
Total number of downloads (all versions) from this website: 
<?php   
    $file = fopen("HowManyCounts.txt", "r") or exit("Unable to open file!");
    echo fgets($file);
    fclose($file);
?>
<div style="color:red; font-size:80%">
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
This count is automatically updated at every rotation of
<br> 
&nbsp;&nbsp;&nbsp;&nbsp;  &nbsp;&nbsp;&nbsp;&nbsp;
the weblogs (normally once every two to four days)
<br>
&nbsp;&nbsp;&nbsp;&nbsp;  &nbsp;&nbsp;&nbsp;&nbsp;
Last updated:
<?php   
    $file = fopen("LastUpdated.txt", "r") or exit("Unable to open file!");
    echo fgets($file);
    fclose($file);
?>
</div>
</div>
</td>
</tr>
<tr>
<td>
<div style="color:red">
<a HREF="DLStudio-2.5.5_CodeOnly.html">View the main module code file in your browser</a> 
&nbsp;<br>
<a HREF="AdversarialLearning-2.5.5_CodeOnly.html">View the Adversarial Learning code file in your browser</a>&nbsp;
&nbsp;<br>
<a HREF="Seq2SeqLearning-2.5.5_CodeOnly.html">View the Seq2Seq Learning code file in your browser</a>
&nbsp;<br>
<a HREF="DataPrediction-2.5.5_CodeOnly.html">View the Data Prediction code file in your browser</a>&nbsp;
&nbsp;<br>
<a HREF="Transformers-2.5.5_CodeOnly.html">View the Transformers code file in your browser</a>&nbsp;
&nbsp;<br>
<a HREF="MetricLearning-2.5.5_CodeOnly.html">View the Metric Learning code file in your browser</a>&nbsp;
&nbsp;<br>
<a HREF="GenerativeDiffusion-2.5.5_CodeOnly.html">View the Generative Diffusion code file in your browser</a>&nbsp;<br> 
&nbsp;<br>
<a HREF="datasets_for_DLStudio.tar.gz">Download the image datasets for the main DLStudio module</a> 
&nbsp;<br>
<a HREF="datasets_for_AdversarialNetworks.tar.gz">Download the image datasets for adversarial learning and diffusion</a> 
&nbsp;<br>
<a HREF="text_datasets_for_DLStudio.tar.gz">Download the datasets for text classification</a> 
&nbsp;<br>
<a HREF="en_es_corpus_for_seq2seq_learning.tar.gz">Download the dataset for sequence-to-sequence learning</a> 
&nbsp;<br>
<a HREF="dataset_for_DataPrediction.tar.gz">Download the dataset for data prediction</a> 
&nbsp;<br>
<a HREF="en_es_corpus_for_learning_with_Transformers.tar.gz">Download the datasets for transformer based learning</a> 
</div>
</td>
<td>
</td>  
</tr>
</table>
<br>
<br>
<br>
<span style="color:red; font-size:150%"><strong>CONTENTS:</strong></span>
<br>
<br>
<div style="font-size:100%; line-height:180%; font-weight: bold">

<a href=#100>CHANGE&nbsp;LOG</a><br><a href=#101>INTRODUCTION</a><br><a href=#102>&nbsp;&nbsp;&nbsp;&nbsp;SKIP&nbsp;CONNECTIONS</a><br><a href=#103>&nbsp;&nbsp;&nbsp;&nbsp;OBJECT&nbsp;DETECTION&nbsp;AND&nbsp;LOCALIZATION</a><br><a href=#104>&nbsp;&nbsp;&nbsp;&nbsp;NOISY&nbsp;OBJECT&nbsp;DETECTION&nbsp;AND&nbsp;LOCALIZATION</a><br><a href=#105>&nbsp;&nbsp;&nbsp;&nbsp;IoU&nbsp;REGRESSION&nbsp;FOR&nbsp;OBJECT&nbsp;DETECTION&nbsp;AND&nbsp;LOCALIZATION</a><br><a href=#106>&nbsp;&nbsp;&nbsp;&nbsp;SEMANTIC&nbsp;SEGMENTATION</a><br><a href=#107>&nbsp;&nbsp;&nbsp;&nbsp;TEXT&nbsp;CLASSIFICATION</a><br><a href=#108>&nbsp;&nbsp;&nbsp;&nbsp;DATA&nbsp;MODELING&nbsp;WITH&nbsp;ADVERSARIAL&nbsp;LEARNING</a><br><a href=#109>&nbsp;&nbsp;&nbsp;&nbsp;DATA&nbsp;MODELING&nbsp;WITH&nbsp;DIFFUSION</a><br><a href=#110>&nbsp;&nbsp;&nbsp;&nbsp;SEQUENCE-TO-SEQUENCE&nbsp;LEARNING&nbsp;WITH&nbsp;ATTENTION</a><br><a href=#111>&nbsp;&nbsp;&nbsp;&nbsp;DATA&nbsp;PREDICTION</a><br><a href=#112>&nbsp;&nbsp;&nbsp;&nbsp;TRANSFORMERS</a><br><a href=#113>&nbsp;&nbsp;&nbsp;&nbsp;METRIC&nbsp;LEARNING</a><br><a href=#114>INSTALLATION</a><br><a href=#115>USAGE</a><br><a href=#116>CONSTRUCTOR&nbsp;PARAMETERS</a><br><a href=#117>PUBLIC&nbsp;METHODS</a><br><a href=#118>THE&nbsp;MAIN&nbsp;INNER&nbsp;CLASSES&nbsp;OF&nbsp;THE&nbsp;DLStudio&nbsp;CLASS</a><br><a href=#119>MODULES&nbsp;IN&nbsp;THE&nbsp;DLStudio&nbsp;PLATFORM</a><br><a href=#120>Examples&nbsp;DIRECTORY</a><br><a href=#121>ExamplesAdversarialLearning&nbsp;DIRECTORY</a><br><a href=#122>ExamplesDiffusion&nbsp;DIRECTORY</a><br><a href=#123>ExamplesSeq2SeqLearning&nbsp;DIRECTORY</a><br><a href=#124>ExamplesDataPrediction&nbsp;DIRECTORY</a><br><a href=#125>ExamplesTransformers&nbsp;DIRECTORY</a><br><a href=#126>ExamplesMetricLearning&nbsp;DIRECTORY</a><br><a href=#127>THE&nbsp;DATASETS&nbsp;INCLUDED</a><br><a href=#128>&nbsp;&nbsp;&nbsp;&nbsp;FOR&nbsp;THE&nbsp;MAIN&nbsp;DLStudio&nbsp;CLASS&nbsp;and&nbsp;its&nbsp;INNER&nbsp;CLASSES</a><br><a href=#129>&nbsp;&nbsp;&nbsp;&nbsp;FOR&nbsp;Seq2Seq&nbsp;LEARNING</a><br><a href=#130>&nbsp;&nbsp;&nbsp;&nbsp;FOR&nbsp;ADVERSARIAL&nbsp;LEARNING&nbsp;AND&nbsp;DIFFUSION</a><br><a href=#131>&nbsp;&nbsp;&nbsp;&nbsp;FOR&nbsp;DATA&nbsp;PREDICTION</a><br><a href=#132>&nbsp;&nbsp;&nbsp;&nbsp;FOR&nbsp;TRANSFORMERS</a><br><a href=#133>BUGS</a><br><a href=#134>ACKNOWLEDGMENTS</a><br><a href=#135>ABOUT&nbsp;THE&nbsp;AUTHOR</a><br><a href=#136>COPYRIGHT</a><br><br></div>
&nbsp;<br>
<span style="color:red; font-size:150%"><strong><a id="100">CHANGE LOG</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.5.5:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;With&nbsp;the&nbsp;inclusion&nbsp;of&nbsp;VQGAN,&nbsp;this&nbsp;version&nbsp;of&nbsp;DLStudio&nbsp;incorporates&nbsp;all&nbsp;three<br>
&nbsp;&nbsp;&nbsp;&nbsp;foundational&nbsp;concepts&nbsp;in&nbsp;variational&nbsp;autoencoding:&nbsp;VAE,&nbsp;VQVAE&nbsp;and&nbsp;VQGAN&nbsp;.&nbsp;&nbsp;The<br>
&nbsp;&nbsp;&nbsp;&nbsp;last,&nbsp;VQGAN,&nbsp;occupies&nbsp;a&nbsp;very&nbsp;special&nbsp;place&nbsp;in&nbsp;Deep&nbsp;Learning&nbsp;because&nbsp;it<br>
&nbsp;&nbsp;&nbsp;&nbsp;established&nbsp;unequivocally&nbsp;that,&nbsp;after&nbsp;tokenization,&nbsp;attention&nbsp;based&nbsp;processing<br>
&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;transformers&nbsp;can&nbsp;be&nbsp;made&nbsp;to&nbsp;work&nbsp;exactly&nbsp;the&nbsp;same&nbsp;for&nbsp;both&nbsp;languages&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;images.&nbsp;&nbsp;This&nbsp;is&nbsp;in&nbsp;keeping&nbsp;with&nbsp;the&nbsp;belief&nbsp;held&nbsp;by&nbsp;researchers&nbsp;(psychologists,<br>
&nbsp;&nbsp;&nbsp;&nbsp;psychophysicists,&nbsp;cognitive&nbsp;scientists,&nbsp;etc.)&nbsp;that&nbsp;after&nbsp;the&nbsp;lowest&nbsp;levels&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;sensory&nbsp;processing,&nbsp;the&nbsp;brain&nbsp;uses&nbsp;the&nbsp;same&nbsp;structures&nbsp;for&nbsp;drawing&nbsp;high&nbsp;level<br>
&nbsp;&nbsp;&nbsp;&nbsp;inferences&nbsp;from&nbsp;different&nbsp;types&nbsp;of&nbsp;sensory&nbsp;signals.&nbsp;And&nbsp;the&nbsp;fact&nbsp;that&nbsp;many&nbsp;people<br>
&nbsp;&nbsp;&nbsp;&nbsp;tend&nbsp;to&nbsp;be&nbsp;highly&nbsp;imagistic&nbsp;readers&nbsp;lends&nbsp;further&nbsp;support&nbsp;to&nbsp;this&nbsp;idea&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;presence&nbsp;of&nbsp;unified&nbsp;structures&nbsp;in&nbsp;the&nbsp;brain&nbsp;for&nbsp;high-level&nbsp;processing&nbsp;of&nbsp;sensory<br>
&nbsp;&nbsp;&nbsp;&nbsp;information&nbsp;regardless&nbsp;of&nbsp;the&nbsp;sensors&nbsp;involved.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.5.4:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;I&nbsp;have&nbsp;significantly&nbsp;redesigned&nbsp;the&nbsp;neural&nbsp;networks&nbsp;for&nbsp;the&nbsp;autoencoding&nbsp;classes<br>
&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;DLStudio.&nbsp;&nbsp;These&nbsp;classes&nbsp;are&nbsp;Autoencoder,&nbsp;VAE,&nbsp;and&nbsp;VQVAE.&nbsp;These&nbsp;networks&nbsp;can<br>
&nbsp;&nbsp;&nbsp;&nbsp;now&nbsp;be&nbsp;made&nbsp;arbitrarily&nbsp;deep&nbsp;with&nbsp;a&nbsp;single&nbsp;parameter&nbsp;named&nbsp;num_repeats&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;controls&nbsp;the&nbsp;number&nbsp;of&nbsp;repetitions&nbsp;of&nbsp;the&nbsp;SkipBlock&nbsp;instances&nbsp;for&nbsp;which&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;channel&nbsp;dimensionality&nbsp;remains&nbsp;unchanged&nbsp;from&nbsp;the&nbsp;input&nbsp;to&nbsp;the&nbsp;output.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.5.3:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;I&nbsp;have&nbsp;fixed&nbsp;the&nbsp;v2.5.2&nbsp;module&nbsp;packaging&nbsp;error&nbsp;in&nbsp;the&nbsp;new&nbsp;version.&nbsp;&nbsp;In&nbsp;addition,<br>
&nbsp;&nbsp;&nbsp;&nbsp;I&nbsp;have&nbsp;also&nbsp;changed&nbsp;the&nbsp;tensor&nbsp;axis&nbsp;used&nbsp;for&nbsp;nn.Softmax&nbsp;normalization&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;"Q.K^T"&nbsp;dot-products&nbsp;from&nbsp;the&nbsp;word&nbsp;axis&nbsp;in&nbsp;the&nbsp;Q-tensors&nbsp;to&nbsp;the&nbsp;word-axis&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;K-tensors.&nbsp;&nbsp;This&nbsp;could&nbsp;potentially&nbsp;lead&nbsp;to&nbsp;superior&nbsp;results&nbsp;when&nbsp;solving&nbsp;problems<br>
&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;which&nbsp;the&nbsp;cross-attention&nbsp;plays&nbsp;a&nbsp;significantly&nbsp;larger&nbsp;role&nbsp;than<br>
&nbsp;&nbsp;&nbsp;&nbsp;self-attention.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.5.2:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;In&nbsp;this&nbsp;version,&nbsp;I&nbsp;have&nbsp;simplified&nbsp;the&nbsp;code&nbsp;in&nbsp;the&nbsp;AdversarialLearning&nbsp;module&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;DLStudio.&nbsp;&nbsp;This&nbsp;I&nbsp;did&nbsp;by&nbsp;dropping&nbsp;what&nbsp;turned&nbsp;out&nbsp;to&nbsp;be&nbsp;a&nbsp;not-really-needed<br>
&nbsp;&nbsp;&nbsp;&nbsp;container&nbsp;class&nbsp;for&nbsp;the&nbsp;rest&nbsp;of&nbsp;the&nbsp;code&nbsp;in&nbsp;the&nbsp;module.&nbsp;&nbsp;Another&nbsp;change&nbsp;made&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;version&nbsp;is&nbsp;the&nbsp;addition&nbsp;of&nbsp;VQ-VAE&nbsp;code&nbsp;in&nbsp;the&nbsp;main&nbsp;DLStudio&nbsp;class&nbsp;file.<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;added&nbsp;code&nbsp;is&nbsp;in&nbsp;the&nbsp;form&nbsp;of&nbsp;a&nbsp;new&nbsp;class&nbsp;called&nbsp;VQVAE.&nbsp;(BTW,&nbsp;VQ&nbsp;stands&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;Vector&nbsp;Quantization.&nbsp;&nbsp;It&nbsp;is&nbsp;an&nbsp;important&nbsp;concept&nbsp;that&nbsp;is&nbsp;at&nbsp;the&nbsp;heart&nbsp;of&nbsp;what's<br>
&nbsp;&nbsp;&nbsp;&nbsp;known&nbsp;as&nbsp;Codebook&nbsp;Learning&nbsp;for&nbsp;more&nbsp;efficient&nbsp;discrete&nbsp;representation&nbsp;of&nbsp;images<br>
&nbsp;&nbsp;&nbsp;&nbsp;using&nbsp;a&nbsp;finite&nbsp;vocabulary&nbsp;of&nbsp;embedding&nbsp;vectors.)&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.5.1:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;With&nbsp;this&nbsp;version,&nbsp;DLStudio&nbsp;now&nbsp;comes&nbsp;with&nbsp;an&nbsp;implementation&nbsp;for&nbsp;Variational<br>
&nbsp;&nbsp;&nbsp;&nbsp;Auto-Encoding&nbsp;(VAE)&nbsp;for&nbsp;images.&nbsp;&nbsp;The&nbsp;VAE&nbsp;Encoder&nbsp;learns&nbsp;the&nbsp;latent&nbsp;distribution<br>
&nbsp;&nbsp;&nbsp;&nbsp;(as&nbsp;represented&nbsp;by&nbsp;the&nbsp;mean&nbsp;and&nbsp;the&nbsp;variance&nbsp;of&nbsp;a&nbsp;presumed&nbsp;isotropic&nbsp;Gaussian)<br>
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;a&nbsp;training&nbsp;dataset.&nbsp;&nbsp;Subsequently,&nbsp;the&nbsp;VAE&nbsp;Decoder&nbsp;samples&nbsp;this&nbsp;distribution<br>
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;generating&nbsp;new&nbsp;images&nbsp;that&nbsp;can&nbsp;be&nbsp;useful&nbsp;variants&nbsp;of&nbsp;the&nbsp;input&nbsp;images.&nbsp;&nbsp;My<br>
&nbsp;&nbsp;&nbsp;&nbsp;VAE&nbsp;implementation&nbsp;is&nbsp;in&nbsp;the&nbsp;form&nbsp;of&nbsp;two&nbsp;additional&nbsp;inner&nbsp;classes&nbsp;in&nbsp;the&nbsp;main<br>
&nbsp;&nbsp;&nbsp;&nbsp;DLStudio&nbsp;class:&nbsp;Autoencoder&nbsp;and&nbsp;VAE.&nbsp;&nbsp;The&nbsp;former&nbsp;serves&nbsp;as&nbsp;the&nbsp;base&nbsp;class&nbsp;for&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;latter.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.5.0:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;An&nbsp;encoder/decoder&nbsp;component&nbsp;of&nbsp;a&nbsp;typical&nbsp;Transformer&nbsp;consists&nbsp;of&nbsp;an&nbsp;attention<br>
&nbsp;&nbsp;&nbsp;&nbsp;layer&nbsp;followed&nbsp;by&nbsp;an&nbsp;FFN&nbsp;(Feed&nbsp;Forward&nbsp;Network)&nbsp;layer.&nbsp;&nbsp;There&nbsp;was&nbsp;an&nbsp;error&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;FFN&nbsp;layer&nbsp;that&nbsp;I&nbsp;have&nbsp;fixed&nbsp;in&nbsp;this&nbsp;version&nbsp;of&nbsp;DLStudio.&nbsp;&nbsp;While&nbsp;the&nbsp;previous<br>
&nbsp;&nbsp;&nbsp;&nbsp;versions&nbsp;also&nbsp;worked&nbsp;from&nbsp;the&nbsp;standpoint&nbsp;of&nbsp;overall&nbsp;transformer-based&nbsp;learning,<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;error&nbsp;caused&nbsp;the&nbsp;number&nbsp;of&nbsp;learnable&nbsp;parameters&nbsp;to&nbsp;depend&nbsp;strongly&nbsp;on&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;maximum&nbsp;sequence&nbsp;length&nbsp;at&nbsp;the&nbsp;input&nbsp;to&nbsp;the&nbsp;transformer.&nbsp;&nbsp;You&nbsp;will&nbsp;not&nbsp;see&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;problem&nbsp;in&nbsp;Version&nbsp;2.5.0.&nbsp;&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.4.9:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;version&nbsp;contains&nbsp;fixes&nbsp;for&nbsp;the&nbsp;pathname&nbsp;errors&nbsp;in&nbsp;the&nbsp;Transformers&nbsp;module&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;DLStudio.&nbsp;The&nbsp;errors&nbsp;were&nbsp;related&nbsp;to&nbsp;where&nbsp;the&nbsp;models&nbsp;and&nbsp;the&nbsp;checkpoints&nbsp;were<br>
&nbsp;&nbsp;&nbsp;&nbsp;supposed&nbsp;to&nbsp;be&nbsp;stored&nbsp;during&nbsp;training.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.4.8:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;In&nbsp;this&nbsp;version,&nbsp;I&nbsp;have&nbsp;made&nbsp;two&nbsp;important&nbsp;changes&nbsp;to&nbsp;the&nbsp;Transformers&nbsp;module&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;DLStudio:&nbsp;(1)&nbsp;The&nbsp;Transformers&nbsp;module&nbsp;now&nbsp;includes&nbsp;a&nbsp;new&nbsp;class&nbsp;that&nbsp;I&nbsp;have&nbsp;named<br>
&nbsp;&nbsp;&nbsp;&nbsp;visTransformer&nbsp;that&nbsp;works&nbsp;like&nbsp;the&nbsp;famous&nbsp;Vision&nbsp;Transformer&nbsp;(ViT)&nbsp;proposed&nbsp;by<br>
&nbsp;&nbsp;&nbsp;&nbsp;Dosovitskiy&nbsp;et&nbsp;al.&nbsp;And&nbsp;(2)&nbsp;I&nbsp;have&nbsp;made&nbsp;changes&nbsp;to&nbsp;the&nbsp;QKV&nbsp;code&nbsp;for&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;calculation&nbsp;of&nbsp;self&nbsp;and&nbsp;cross&nbsp;attention&nbsp;in&nbsp;all&nbsp;of&nbsp;the&nbsp;Transformer&nbsp;classes&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;module.&nbsp;The&nbsp;attention&nbsp;calculations&nbsp;should&nbsp;now&nbsp;execute&nbsp;faster,&nbsp;which&nbsp;is&nbsp;a&nbsp;very<br>
&nbsp;&nbsp;&nbsp;&nbsp;important&nbsp;consideration&nbsp;in&nbsp;any&nbsp;transformer&nbsp;based&nbsp;learning.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.4.3:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;diffusion&nbsp;modeling&nbsp;part&nbsp;of&nbsp;the&nbsp;code&nbsp;should&nbsp;now&nbsp;accept&nbsp;training&nbsp;images&nbsp;of&nbsp;any<br>
&nbsp;&nbsp;&nbsp;&nbsp;size.&nbsp;&nbsp;Previously&nbsp;it&nbsp;was&nbsp;limited&nbsp;to&nbsp;images&nbsp;of&nbsp;size&nbsp;64x64.&nbsp;&nbsp;Note&nbsp;that&nbsp;this&nbsp;change<br>
&nbsp;&nbsp;&nbsp;&nbsp;is&nbsp;not&nbsp;as&nbsp;significant&nbsp;as&nbsp;you&nbsp;might&nbsp;think&nbsp;because,&nbsp;under&nbsp;the&nbsp;hood,&nbsp;the&nbsp;actual<br>
&nbsp;&nbsp;&nbsp;&nbsp;input&nbsp;image&nbsp;size&nbsp;is&nbsp;changed&nbsp;to&nbsp;the&nbsp;size&nbsp;64x64&nbsp;for&nbsp;diffusion&nbsp;modeling.&nbsp;&nbsp;So&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;change&nbsp;is&nbsp;more&nbsp;for&nbsp;your&nbsp;convenience&nbsp;than&nbsp;anything&nbsp;else.&nbsp;&nbsp;I&nbsp;have&nbsp;also&nbsp;improved&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;image&nbsp;visualization&nbsp;code&nbsp;in&nbsp;the&nbsp;ExamplesDiffusion&nbsp;directory.&nbsp;The&nbsp;new<br>
&nbsp;&nbsp;&nbsp;&nbsp;implementation&nbsp;of&nbsp;the&nbsp;script&nbsp;VisualizeSamples.py&nbsp;automatically&nbsp;generates&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;collage&nbsp;of&nbsp;the&nbsp;images&nbsp;generated&nbsp;from&nbsp;noise&nbsp;by&nbsp;the&nbsp;script<br>
&nbsp;&nbsp;&nbsp;&nbsp;GenerateNewImageSamples.py.&nbsp;&nbsp;Other&nbsp;changes&nbsp;include&nbsp;minor&nbsp;clean-up&nbsp;of&nbsp;the&nbsp;main&nbsp;doc<br>
&nbsp;&nbsp;&nbsp;&nbsp;page&nbsp;for&nbsp;GenerativeDiffusion&nbsp;module&nbsp;and&nbsp;of&nbsp;a&nbsp;couple&nbsp;of&nbsp;the&nbsp;functions&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;module.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.4.2:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;DLStudio&nbsp;now&nbsp;includes&nbsp;a&nbsp;new&nbsp;module&nbsp;devoted&nbsp;to&nbsp;data&nbsp;modeling&nbsp;with&nbsp;diffusion.&nbsp;&nbsp;This<br>
&nbsp;&nbsp;&nbsp;&nbsp;module,&nbsp;named&nbsp;GenerativeDiffusion,&nbsp;is&nbsp;a&nbsp;module&nbsp;in&nbsp;the&nbsp;overall&nbsp;DLStudio&nbsp;platform.<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;GenerativeDiffusion&nbsp;class&nbsp;resides&nbsp;at&nbsp;the&nbsp;same&nbsp;level&nbsp;of&nbsp;software&nbsp;abstraction<br>
&nbsp;&nbsp;&nbsp;&nbsp;as&nbsp;the&nbsp;main&nbsp;DLStudio&nbsp;class&nbsp;in&nbsp;the&nbsp;platform.&nbsp;&nbsp;See&nbsp;the&nbsp;README&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;ExamplesDiffusion&nbsp;directory&nbsp;of&nbsp;the&nbsp;distribution&nbsp;for&nbsp;how&nbsp;to&nbsp;experiment&nbsp;with&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;diffusion&nbsp;based&nbsp;code&nbsp;in&nbsp;DLStudio.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.3.6:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Gets&nbsp;rid&nbsp;of&nbsp;the&nbsp;inadvertently&nbsp;hardcoded&nbsp;value&nbsp;for&nbsp;the&nbsp;batch&nbsp;size&nbsp;in&nbsp;the&nbsp;testing<br>
&nbsp;&nbsp;&nbsp;&nbsp;part&nbsp;of&nbsp;the&nbsp;code&nbsp;for&nbsp;Semantic&nbsp;Segmentation.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.3.5:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;In&nbsp;this&nbsp;version&nbsp;I&nbsp;have&nbsp;improved&nbsp;the&nbsp;quality&nbsp;of&nbsp;the&nbsp;code&nbsp;in&nbsp;the&nbsp;Semantic<br>
&nbsp;&nbsp;&nbsp;&nbsp;Segmentation&nbsp;inner&nbsp;class&nbsp;of&nbsp;DLStudio.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.3.4:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Contains&nbsp;a&nbsp;cleaner&nbsp;design&nbsp;for&nbsp;the&nbsp;SkipBlock&nbsp;network.&nbsp;That&nbsp;led&nbsp;to&nbsp;improved&nbsp;design<br>
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;some&nbsp;of&nbsp;the&nbsp;larger&nbsp;networks&nbsp;in&nbsp;DLStudio&nbsp;that&nbsp;use&nbsp;the&nbsp;SkipBlock&nbsp;as&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;building-block.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.3.3:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;version&nbsp;fixes&nbsp;a&nbsp;couple&nbsp;of&nbsp;bugs&nbsp;in&nbsp;DLStudio.&nbsp;&nbsp;The&nbsp;newer&nbsp;versions&nbsp;of&nbsp;PyTorch<br>
&nbsp;&nbsp;&nbsp;&nbsp;do&nbsp;not&nbsp;like&nbsp;it&nbsp;if,&nbsp;during&nbsp;the&nbsp;forward&nbsp;flow&nbsp;of&nbsp;data&nbsp;through&nbsp;a&nbsp;network,&nbsp;you&nbsp;make<br>
&nbsp;&nbsp;&nbsp;&nbsp;in-place&nbsp;changes&nbsp;to&nbsp;the&nbsp;data&nbsp;objects&nbsp;that&nbsp;are&nbsp;in&nbsp;the&nbsp;computational&nbsp;graph.<br>
&nbsp;&nbsp;&nbsp;&nbsp;Examples&nbsp;of&nbsp;in-place&nbsp;operations&nbsp;are&nbsp;those&nbsp;that&nbsp;result&nbsp;from&nbsp;using&nbsp;compound<br>
&nbsp;&nbsp;&nbsp;&nbsp;assignment&nbsp;operators&nbsp;like&nbsp;'+=',&nbsp;'*=',&nbsp;etc.,&nbsp;and&nbsp;those&nbsp;that&nbsp;result&nbsp;from&nbsp;the&nbsp;use&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;slice&nbsp;operators&nbsp;on&nbsp;arrays.&nbsp;&nbsp;Such&nbsp;bugs&nbsp;are&nbsp;difficult&nbsp;to&nbsp;troubleshoot&nbsp;because&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;error&nbsp;messages&nbsp;returned&nbsp;by&nbsp;PyTorch&nbsp;are&nbsp;as&nbsp;unhelpful&nbsp;as&nbsp;they&nbsp;can&nbsp;be&nbsp;---&nbsp;they&nbsp;give<br>
&nbsp;&nbsp;&nbsp;&nbsp;you&nbsp;no&nbsp;indication&nbsp;at&nbsp;all&nbsp;as&nbsp;to&nbsp;the&nbsp;location&nbsp;of&nbsp;the&nbsp;fault.&nbsp;This&nbsp;version&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;DLStudio&nbsp;was&nbsp;tested&nbsp;with&nbsp;Version&nbsp;2.0.1&nbsp;of&nbsp;PyTorch.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.3.2:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;version&nbsp;of&nbsp;DLStudio&nbsp;includes&nbsp;a&nbsp;new&nbsp;Metric&nbsp;Learning&nbsp;module&nbsp;(name&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;class:&nbsp;MetricLearning).&nbsp;The&nbsp;main&nbsp;idea&nbsp;of&nbsp;metric&nbsp;learning&nbsp;is&nbsp;to&nbsp;learn&nbsp;a&nbsp;mapping<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;the&nbsp;images&nbsp;to&nbsp;their&nbsp;embedding&nbsp;vector&nbsp;representations&nbsp;in&nbsp;such&nbsp;a&nbsp;way&nbsp;that&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;embeddings&nbsp;for&nbsp;what&nbsp;are&nbsp;supposed&nbsp;to&nbsp;be&nbsp;similar&nbsp;images&nbsp;are&nbsp;pulled&nbsp;together&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;those&nbsp;for&nbsp;dissimilar&nbsp;images&nbsp;are&nbsp;pulled&nbsp;as&nbsp;far&nbsp;apart&nbsp;as&nbsp;possible.&nbsp;&nbsp;After&nbsp;such&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;mapping&nbsp;function&nbsp;is&nbsp;learned,&nbsp;you&nbsp;can&nbsp;take&nbsp;a&nbsp;query&nbsp;image&nbsp;(whose&nbsp;class&nbsp;label&nbsp;is&nbsp;not<br>
&nbsp;&nbsp;&nbsp;&nbsp;known),&nbsp;run&nbsp;it&nbsp;through&nbsp;the&nbsp;network&nbsp;to&nbsp;find&nbsp;its&nbsp;embedding&nbsp;vector,&nbsp;and,<br>
&nbsp;&nbsp;&nbsp;&nbsp;subsequently,&nbsp;assign&nbsp;to&nbsp;the&nbsp;query&nbsp;image&nbsp;the&nbsp;class&nbsp;label&nbsp;of&nbsp;the&nbsp;nearest<br>
&nbsp;&nbsp;&nbsp;&nbsp;training-image&nbsp;neighbor&nbsp;in&nbsp;the&nbsp;embedding&nbsp;space.&nbsp;&nbsp;As&nbsp;explained&nbsp;in&nbsp;my&nbsp;Metric<br>
&nbsp;&nbsp;&nbsp;&nbsp;Learning&nbsp;lecture&nbsp;slides&nbsp;in&nbsp;the&nbsp;Deep&nbsp;Learning&nbsp;class&nbsp;at&nbsp;Purdue,&nbsp;this&nbsp;approach&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;classification&nbsp;is&nbsp;likely&nbsp;to&nbsp;work&nbsp;better&nbsp;under&nbsp;data&nbsp;conditions&nbsp;when&nbsp;the&nbsp;more<br>
&nbsp;&nbsp;&nbsp;&nbsp;traditional&nbsp;neural&nbsp;network&nbsp;classifiers&nbsp;fail.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.3.0:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;I&nbsp;have&nbsp;made&nbsp;it&nbsp;a&nbsp;bit&nbsp;simpler&nbsp;for&nbsp;you&nbsp;to&nbsp;use&nbsp;DLStudio's&nbsp;transformer&nbsp;classes&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;your&nbsp;own&nbsp;scripts.&nbsp;&nbsp;This&nbsp;I&nbsp;did&nbsp;by&nbsp;eliminating&nbsp;'Transformers'&nbsp;as&nbsp;the&nbsp;parent&nbsp;class<br>
&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;TransformerFG&nbsp;and&nbsp;TransformerPreLN.&nbsp;&nbsp;Now,&nbsp;in&nbsp;your&nbsp;own&nbsp;scripts,&nbsp;you&nbsp;can&nbsp;have<br>
&nbsp;&nbsp;&nbsp;&nbsp;more&nbsp;direct&nbsp;access&nbsp;to&nbsp;these&nbsp;two&nbsp;classes&nbsp;that&nbsp;you&nbsp;need&nbsp;for&nbsp;transformer&nbsp;based<br>
&nbsp;&nbsp;&nbsp;&nbsp;learning.&nbsp;&nbsp;Your&nbsp;best&nbsp;guide&nbsp;to&nbsp;the&nbsp;syntax&nbsp;for&nbsp;calling&nbsp;TransformerFG&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;TransformerPreLN&nbsp;are&nbsp;the&nbsp;example&nbsp;scripts&nbsp;"seq2seq_with_transformerFG.py"&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;"seq2seq_with_transformerPreLN.py"&nbsp;in&nbsp;the&nbsp;ExamplesTransformers&nbsp;directory&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;distribution.&nbsp;&nbsp;Additional&nbsp;changes&nbsp;in&nbsp;Version&nbsp;2.3.0&nbsp;include&nbsp;general&nbsp;code&nbsp;clean-up<br>
&nbsp;&nbsp;&nbsp;&nbsp;by&nbsp;getting&nbsp;rid&nbsp;of&nbsp;variables&nbsp;no&nbsp;longer&nbsp;being&nbsp;used,&nbsp;harmonizing&nbsp;the&nbsp;names&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;constructor&nbsp;options,&nbsp;etc.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.2.8:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;In&nbsp;this&nbsp;version&nbsp;I&nbsp;have&nbsp;fixed&nbsp;a&nbsp;couple&nbsp;of&nbsp;errors&nbsp;that&nbsp;had&nbsp;crept&nbsp;into&nbsp;the&nbsp;previous<br>
&nbsp;&nbsp;&nbsp;&nbsp;version&nbsp;at&nbsp;the&nbsp;time&nbsp;of&nbsp;packaging&nbsp;that&nbsp;distribution.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.2.7:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;version&nbsp;provides&nbsp;you&nbsp;with&nbsp;the&nbsp;tools&nbsp;you&nbsp;need&nbsp;to&nbsp;cope&nbsp;with&nbsp;the&nbsp;frustrations<br>
&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;training&nbsp;a&nbsp;transformer&nbsp;based&nbsp;network.&nbsp;Such&nbsp;networks&nbsp;in&nbsp;general&nbsp;are&nbsp;difficult<br>
&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;train,&nbsp;in&nbsp;the&nbsp;sense&nbsp;that&nbsp;your&nbsp;per-epoch&nbsp;training&nbsp;time&nbsp;is&nbsp;likely&nbsp;to&nbsp;be&nbsp;much<br>
&nbsp;&nbsp;&nbsp;&nbsp;longer&nbsp;than&nbsp;what&nbsp;you&nbsp;are&nbsp;accustomed&nbsp;to,&nbsp;and&nbsp;it&nbsp;can&nbsp;take&nbsp;many,&nbsp;many&nbsp;more&nbsp;epochs&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;get&nbsp;the&nbsp;model&nbsp;to&nbsp;converge.&nbsp;&nbsp;In&nbsp;addition,&nbsp;you&nbsp;have&nbsp;the&nbsp;problem&nbsp;of&nbsp;stability&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;deal&nbsp;with.&nbsp;Stability&nbsp;means&nbsp;that&nbsp;with&nbsp;a&nbsp;wrong&nbsp;choice&nbsp;for&nbsp;the&nbsp;hyperparameters,&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;model&nbsp;that&nbsp;you&nbsp;are&nbsp;currently&nbsp;training&nbsp;could&nbsp;suddenly&nbsp;begin&nbsp;to&nbsp;diverge&nbsp;(which&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;something&nbsp;akin&nbsp;to&nbsp;mode&nbsp;collapse&nbsp;in&nbsp;training&nbsp;a&nbsp;GAN).&nbsp;If&nbsp;you&nbsp;have&nbsp;to&nbsp;wait&nbsp;until&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;end&nbsp;of&nbsp;training&nbsp;to&nbsp;see&nbsp;such&nbsp;failures,&nbsp;that&nbsp;can&nbsp;be&nbsp;very&nbsp;frustrating.&nbsp;&nbsp;To&nbsp;cope&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;these&nbsp;problems,&nbsp;this&nbsp;version&nbsp;of&nbsp;DLStudio&nbsp;automatically&nbsp;spits&nbsp;out&nbsp;a&nbsp;checkpoint&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;model&nbsp;every&nbsp;5&nbsp;epochs&nbsp;and&nbsp;also&nbsp;gives&nbsp;you&nbsp;the&nbsp;functions&nbsp;for&nbsp;evaluating&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;performance&nbsp;of&nbsp;the&nbsp;checkpoints.&nbsp;The&nbsp;performance&nbsp;check&nbsp;can&nbsp;be&nbsp;as&nbsp;simple&nbsp;as&nbsp;looking<br>
&nbsp;&nbsp;&nbsp;&nbsp;at&nbsp;the&nbsp;translated&nbsp;sentences&nbsp;vis-a-vis&nbsp;their&nbsp;targets&nbsp;for&nbsp;a&nbsp;random&nbsp;selection&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;sentence&nbsp;pairs&nbsp;from&nbsp;the&nbsp;data.&nbsp;&nbsp;When&nbsp;real&nbsp;learning&nbsp;is&nbsp;taking&nbsp;place,&nbsp;you&nbsp;will&nbsp;see<br>
&nbsp;&nbsp;&nbsp;&nbsp;longer&nbsp;and&nbsp;longer&nbsp;fragments&nbsp;of&nbsp;the&nbsp;translated&nbsp;sentences&nbsp;correspond&nbsp;to&nbsp;the&nbsp;target<br>
&nbsp;&nbsp;&nbsp;&nbsp;sentences.&nbsp;On&nbsp;the&nbsp;other&nbsp;hand,&nbsp;when&nbsp;you&nbsp;have&nbsp;model&nbsp;divergence,&nbsp;the&nbsp;translated<br>
&nbsp;&nbsp;&nbsp;&nbsp;sentences&nbsp;will&nbsp;appear&nbsp;to&nbsp;be&nbsp;gibberish.&nbsp;&nbsp;A&nbsp;future&nbsp;version&nbsp;of&nbsp;DLStudio&nbsp;will&nbsp;also<br>
&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;out&nbsp;the&nbsp;BLEU&nbsp;score&nbsp;for&nbsp;the&nbsp;checkpoints.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.2.5:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;version&nbsp;contains&nbsp;significantly&nbsp;improved&nbsp;documentation&nbsp;for&nbsp;DCGAN&nbsp;and&nbsp;WGAN&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;AdversarialLearning&nbsp;class&nbsp;of&nbsp;DLStudio.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.2.4:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;I&nbsp;have&nbsp;cleaned&nbsp;up&nbsp;the&nbsp;code&nbsp;in&nbsp;the&nbsp;new&nbsp;DIoULoss&nbsp;class&nbsp;that&nbsp;I&nbsp;added&nbsp;in&nbsp;the&nbsp;previous<br>
&nbsp;&nbsp;&nbsp;&nbsp;version.&nbsp;The&nbsp;script&nbsp;object_detection_and_localization_iou.py&nbsp;in&nbsp;the&nbsp;Examples<br>
&nbsp;&nbsp;&nbsp;&nbsp;directory&nbsp;of&nbsp;DLStudio&nbsp;is&nbsp;based&nbsp;on&nbsp;this&nbsp;loss&nbsp;function.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.2.3:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;inner&nbsp;class&nbsp;DetectAndLocalize&nbsp;of&nbsp;DLStudio&nbsp;now&nbsp;contains&nbsp;a&nbsp;custom&nbsp;loss&nbsp;function<br>
&nbsp;&nbsp;&nbsp;&nbsp;provided&nbsp;by&nbsp;the&nbsp;class&nbsp;DIoULoss&nbsp;that&nbsp;implements&nbsp;the&nbsp;more&nbsp;modern&nbsp;variants&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;basic&nbsp;IoU&nbsp;(Intersection&nbsp;over&nbsp;Union)&nbsp;loss&nbsp;function.&nbsp;&nbsp;These&nbsp;IoU&nbsp;variants&nbsp;are<br>
&nbsp;&nbsp;&nbsp;&nbsp;explained&nbsp;in&nbsp;the&nbsp;slides&nbsp;37-42&nbsp;of&nbsp;my&nbsp;Week&nbsp;7&nbsp;Lecture&nbsp;on&nbsp;"Object&nbsp;Detection&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;Localization."&nbsp;&nbsp;Your&nbsp;best&nbsp;entry&nbsp;point&nbsp;to&nbsp;become&nbsp;familiar&nbsp;with&nbsp;these&nbsp;loss<br>
&nbsp;&nbsp;&nbsp;&nbsp;functions&nbsp;is&nbsp;the&nbsp;script&nbsp;object_detection_and_localization_iou.py&nbsp;in&nbsp;the&nbsp;Examples<br>
&nbsp;&nbsp;&nbsp;&nbsp;directory&nbsp;of&nbsp;DLStudio.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.2.2:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;version&nbsp;of&nbsp;DLStudio&nbsp;presents&nbsp;my&nbsp;implementation&nbsp;of&nbsp;transformers&nbsp;in&nbsp;deep<br>
&nbsp;&nbsp;&nbsp;&nbsp;learning.&nbsp;You&nbsp;will&nbsp;find&nbsp;two&nbsp;transformer&nbsp;implementations&nbsp;in&nbsp;the&nbsp;Transformers<br>
&nbsp;&nbsp;&nbsp;&nbsp;module&nbsp;of&nbsp;the&nbsp;DLStudio&nbsp;platform&nbsp;in&nbsp;the&nbsp;distribution&nbsp;directory:&nbsp;TransformerFG&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;TransformerPreLN.&nbsp;&nbsp;"FG"&nbsp;in&nbsp;TransformerFG&nbsp;stands&nbsp;for&nbsp;"Transformer&nbsp;First<br>
&nbsp;&nbsp;&nbsp;&nbsp;Generation";&nbsp;it&nbsp;is&nbsp;my&nbsp;implementation&nbsp;of&nbsp;the&nbsp;architecture&nbsp;presented&nbsp;originally&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;seminal&nbsp;paper&nbsp;"Attention&nbsp;is&nbsp;All&nbsp;You&nbsp;Need"&nbsp;by&nbsp;Vaswani&nbsp;et&nbsp;el.&nbsp;&nbsp;And&nbsp;the&nbsp;second,<br>
&nbsp;&nbsp;&nbsp;&nbsp;TransformerPreLN&nbsp;("PreLN"&nbsp;stands&nbsp;for&nbsp;"Pre&nbsp;Layer&nbsp;Norm")&nbsp;is&nbsp;a&nbsp;small&nbsp;but&nbsp;important<br>
&nbsp;&nbsp;&nbsp;&nbsp;modification&nbsp;of&nbsp;the&nbsp;original&nbsp;idea&nbsp;that&nbsp;is&nbsp;based&nbsp;on&nbsp;the&nbsp;paper&nbsp;"On&nbsp;Layer<br>
&nbsp;&nbsp;&nbsp;&nbsp;Normalization&nbsp;in&nbsp;the&nbsp;Transformer&nbsp;Architecture"&nbsp;by&nbsp;Xiong&nbsp;et&nbsp;al.&nbsp;&nbsp;I&nbsp;could&nbsp;have<br>
&nbsp;&nbsp;&nbsp;&nbsp;easily&nbsp;combined&nbsp;the&nbsp;two&nbsp;implementations&nbsp;with&nbsp;a&nbsp;small&nbsp;number&nbsp;of&nbsp;conditional<br>
&nbsp;&nbsp;&nbsp;&nbsp;statements&nbsp;to&nbsp;account&nbsp;for&nbsp;the&nbsp;differences,&nbsp;however&nbsp;I&nbsp;have&nbsp;chosen&nbsp;to&nbsp;keep&nbsp;them<br>
&nbsp;&nbsp;&nbsp;&nbsp;separate&nbsp;in&nbsp;order&nbsp;to&nbsp;make&nbsp;it&nbsp;easier&nbsp;for&nbsp;the&nbsp;two&nbsp;to&nbsp;evolve&nbsp;separately&nbsp;and&nbsp;to&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;used&nbsp;differently&nbsp;for&nbsp;educational&nbsp;purposes.<br>
&nbsp;<br>
&nbsp;&nbsp;Versions&nbsp;2.1.7&nbsp;through&nbsp;2.2.1:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;These&nbsp;version&nbsp;numbers&nbsp;are&nbsp;for&nbsp;the&nbsp;stepping-stones&nbsp;in&nbsp;my&nbsp;journey&nbsp;into&nbsp;the&nbsp;world&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;transformers&nbsp;---&nbsp;my&nbsp;experiments&nbsp;with&nbsp;how&nbsp;to&nbsp;best&nbsp;implement&nbsp;the&nbsp;different<br>
&nbsp;&nbsp;&nbsp;&nbsp;components&nbsp;of&nbsp;a&nbsp;transformer&nbsp;for&nbsp;educational&nbsp;purposes.&nbsp;&nbsp;As&nbsp;things&nbsp;stand,&nbsp;these<br>
&nbsp;&nbsp;&nbsp;&nbsp;versions&nbsp;contain&nbsp;features&nbsp;that&nbsp;did&nbsp;not&nbsp;make&nbsp;into&nbsp;the&nbsp;public&nbsp;release&nbsp;version&nbsp;2.2.2<br>
&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;account&nbsp;of&nbsp;inadequate&nbsp;testing.&nbsp;&nbsp;I&nbsp;may&nbsp;include&nbsp;those&nbsp;features&nbsp;in&nbsp;versions&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;DLStudio&nbsp;after&nbsp;2.2.2.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.1.6:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;All&nbsp;the&nbsp;changes&nbsp;are&nbsp;confined&nbsp;to&nbsp;the&nbsp;DataPrediction&nbsp;module&nbsp;in&nbsp;the&nbsp;DLStudio<br>
&nbsp;&nbsp;&nbsp;&nbsp;platform.&nbsp;&nbsp;After&nbsp;posting&nbsp;the&nbsp;previous&nbsp;version,&nbsp;I&nbsp;noticed&nbsp;that&nbsp;the&nbsp;quality&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;code&nbsp;in&nbsp;DataPrediction&nbsp;was&nbsp;not&nbsp;up&nbsp;to&nbsp;par.&nbsp;&nbsp;The&nbsp;new&nbsp;version&nbsp;presents&nbsp;a&nbsp;cleaned-up<br>
&nbsp;&nbsp;&nbsp;&nbsp;version&nbsp;of&nbsp;the&nbsp;DataPrediction&nbsp;class.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.1.5:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;DLStudio&nbsp;has&nbsp;now&nbsp;been&nbsp;equipped&nbsp;with&nbsp;a&nbsp;new&nbsp;module&nbsp;named&nbsp;DataPrediction&nbsp;whose&nbsp;focus<br>
&nbsp;&nbsp;&nbsp;&nbsp;is&nbsp;solely&nbsp;on&nbsp;solving&nbsp;data&nbsp;prediction&nbsp;problems&nbsp;for&nbsp;time-series&nbsp;data.&nbsp;&nbsp;A<br>
&nbsp;&nbsp;&nbsp;&nbsp;time-series&nbsp;consists&nbsp;of&nbsp;a&nbsp;sequence&nbsp;of&nbsp;observations&nbsp;recorded&nbsp;at&nbsp;regular&nbsp;intervals.<br>
&nbsp;&nbsp;&nbsp;&nbsp;These&nbsp;could,&nbsp;for&nbsp;example,&nbsp;be&nbsp;the&nbsp;price&nbsp;of&nbsp;a&nbsp;stock&nbsp;share&nbsp;recorded&nbsp;every&nbsp;hour;&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;hourly&nbsp;recordings&nbsp;of&nbsp;electrical&nbsp;load&nbsp;at&nbsp;your&nbsp;local&nbsp;power&nbsp;utility&nbsp;company;&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;mean&nbsp;average&nbsp;temperature&nbsp;recorded&nbsp;on&nbsp;an&nbsp;annual&nbsp;basis;&nbsp;and&nbsp;so&nbsp;on.&nbsp;&nbsp;We&nbsp;want&nbsp;to&nbsp;use<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;past&nbsp;observations&nbsp;to&nbsp;predict&nbsp;the&nbsp;value&nbsp;of&nbsp;the&nbsp;next&nbsp;one.&nbsp;&nbsp;While&nbsp;data<br>
&nbsp;&nbsp;&nbsp;&nbsp;prediction&nbsp;has&nbsp;much&nbsp;in&nbsp;common&nbsp;with&nbsp;other&nbsp;forms&nbsp;of&nbsp;sequence&nbsp;based&nbsp;learning,&nbsp;it<br>
&nbsp;&nbsp;&nbsp;&nbsp;presents&nbsp;certain&nbsp;unique&nbsp;challenges&nbsp;of&nbsp;its&nbsp;own&nbsp;and&nbsp;those&nbsp;are&nbsp;with&nbsp;respect&nbsp;to&nbsp;(1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;Data&nbsp;Normalization;&nbsp;(2)&nbsp;Input&nbsp;Data&nbsp;Chunking;&nbsp;and&nbsp;(3)&nbsp;Multi-dimensional&nbsp;encoding<br>
&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;the&nbsp;"datetime"&nbsp;associated&nbsp;with&nbsp;each&nbsp;observation&nbsp;in&nbsp;the&nbsp;time-series.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.1.3:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Some&nbsp;users&nbsp;of&nbsp;DLStudio&nbsp;have&nbsp;reported&nbsp;that&nbsp;when&nbsp;they&nbsp;run&nbsp;the&nbsp;WGAN&nbsp;code&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;adversarial&nbsp;learning,&nbsp;the&nbsp;dataloader&nbsp;sometimes&nbsp;hangs&nbsp;in&nbsp;the&nbsp;middle&nbsp;of&nbsp;a&nbsp;training<br>
&nbsp;&nbsp;&nbsp;&nbsp;run.&nbsp;&nbsp;(A&nbsp;WGAN&nbsp;training&nbsp;session&nbsp;may&nbsp;involve&nbsp;as&nbsp;many&nbsp;as&nbsp;500&nbsp;epochs.)&nbsp;&nbsp;In&nbsp;trying&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;reproduce&nbsp;this&nbsp;issue,&nbsp;I&nbsp;discovered&nbsp;that&nbsp;the&nbsp;training&nbsp;loops&nbsp;always&nbsp;ran&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;completion&nbsp;if&nbsp;you&nbsp;set&nbsp;the&nbsp;number&nbsp;of&nbsp;workers&nbsp;in&nbsp;the&nbsp;dataloader&nbsp;to&nbsp;0.&nbsp;&nbsp;Version<br>
&nbsp;&nbsp;&nbsp;&nbsp;2.1.3&nbsp;makes&nbsp;it&nbsp;easier&nbsp;for&nbsp;you&nbsp;to&nbsp;specify&nbsp;the&nbsp;number&nbsp;of&nbsp;workers&nbsp;in&nbsp;your&nbsp;own<br>
&nbsp;&nbsp;&nbsp;&nbsp;scripts&nbsp;that&nbsp;call&nbsp;on&nbsp;the&nbsp;WGAN&nbsp;functionality&nbsp;in&nbsp;the&nbsp;AdversarialLearning&nbsp;class.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.1.2:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;adversarial&nbsp;learning&nbsp;part&nbsp;of&nbsp;DLStudio&nbsp;now&nbsp;includes&nbsp;a&nbsp;WGAN&nbsp;implementation&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;uses&nbsp;Gradient&nbsp;Penalty&nbsp;for&nbsp;the&nbsp;learning&nbsp;required&nbsp;by&nbsp;the&nbsp;Critic.&nbsp;&nbsp;All&nbsp;the&nbsp;changes<br>
&nbsp;&nbsp;&nbsp;&nbsp;made&nbsp;are&nbsp;in&nbsp;the&nbsp;AdversarialLearning&nbsp;class&nbsp;at&nbsp;the&nbsp;top&nbsp;level&nbsp;of&nbsp;the&nbsp;module.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.1.1:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;In&nbsp;order&nbsp;to&nbsp;make&nbsp;it&nbsp;easier&nbsp;to&nbsp;navigate&nbsp;through&nbsp;the&nbsp;large&nbsp;code&nbsp;base&nbsp;of&nbsp;the&nbsp;module,<br>
&nbsp;&nbsp;&nbsp;&nbsp;I&nbsp;am&nbsp;adopting&nbsp;the&nbsp;convention&nbsp;that&nbsp;"Network"&nbsp;in&nbsp;the&nbsp;name&nbsp;of&nbsp;a&nbsp;class&nbsp;be&nbsp;reserved<br>
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;only&nbsp;those&nbsp;cases&nbsp;when&nbsp;a&nbsp;class&nbsp;actually&nbsp;implements&nbsp;a&nbsp;network.&nbsp;&nbsp;This&nbsp;convention<br>
&nbsp;&nbsp;&nbsp;&nbsp;requires&nbsp;that&nbsp;the&nbsp;name&nbsp;of&nbsp;an&nbsp;encapsulating&nbsp;class&nbsp;meant&nbsp;for&nbsp;teaching/learning&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;certain&nbsp;aspect&nbsp;of&nbsp;deep&nbsp;learning&nbsp;not&nbsp;contain&nbsp;"Network"&nbsp;in&nbsp;it.&nbsp;&nbsp;Therefore,&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;Version&nbsp;2.1.1,&nbsp;I&nbsp;have&nbsp;changed&nbsp;the&nbsp;names&nbsp;of&nbsp;the&nbsp;top-level&nbsp;classes<br>
&nbsp;&nbsp;&nbsp;&nbsp;AdversarialNetworks&nbsp;and&nbsp;Seq2SeqNetworks&nbsp;to&nbsp;AdversarialLearning&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;Seq2SeqLearning,&nbsp;respectively.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.1.0:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;I&nbsp;have&nbsp;reorganized&nbsp;the&nbsp;code&nbsp;base&nbsp;a&nbsp;bit&nbsp;to&nbsp;make&nbsp;it&nbsp;easier&nbsp;for&nbsp;DLStudio&nbsp;to&nbsp;grow&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;future.&nbsp;&nbsp;This&nbsp;I&nbsp;did&nbsp;by&nbsp;moving&nbsp;the&nbsp;sequence-to-sequence&nbsp;learning&nbsp;(seq2seq)<br>
&nbsp;&nbsp;&nbsp;&nbsp;code&nbsp;to&nbsp;a&nbsp;separate&nbsp;module&nbsp;of&nbsp;the&nbsp;DLStudio&nbsp;platform.&nbsp;&nbsp;The&nbsp;name&nbsp;of&nbsp;the&nbsp;new&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;is&nbsp;Seq2SeqLearning&nbsp;and&nbsp;it&nbsp;resides&nbsp;at&nbsp;the&nbsp;top&nbsp;level&nbsp;of&nbsp;the&nbsp;distribution.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.0.9:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;With&nbsp;this&nbsp;version,&nbsp;DLStudio&nbsp;comes&nbsp;with&nbsp;educational&nbsp;material&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;sequence-to-sequence&nbsp;learning&nbsp;(seq2seq).&nbsp;To&nbsp;that&nbsp;end,&nbsp;I&nbsp;have&nbsp;included&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;following&nbsp;two&nbsp;new&nbsp;classes&nbsp;in&nbsp;DLStudio:&nbsp;(1)&nbsp;Seq2SeqWithLearnableEmbeddings&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;seq2seq&nbsp;with&nbsp;learnable&nbsp;embeddings;&nbsp;and&nbsp;(2)&nbsp;Seq2SeqWithPretrainedEmbeddings&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;doing&nbsp;the&nbsp;same&nbsp;with&nbsp;pre-trained&nbsp;embeddings.&nbsp;Although&nbsp;I&nbsp;have&nbsp;used&nbsp;word2vec&nbsp;for&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;case&nbsp;of&nbsp;pre-trained&nbsp;embeddings,&nbsp;you&nbsp;would&nbsp;be&nbsp;able&nbsp;to&nbsp;run&nbsp;the&nbsp;code&nbsp;with&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;Fasttext&nbsp;embeddings&nbsp;also.&nbsp;&nbsp;Both&nbsp;seq2seq&nbsp;implementations&nbsp;include&nbsp;the&nbsp;attention<br>
&nbsp;&nbsp;&nbsp;&nbsp;mechanism&nbsp;based&nbsp;on&nbsp;my&nbsp;understanding&nbsp;of&nbsp;the&nbsp;original&nbsp;paper&nbsp;on&nbsp;the&nbsp;subject&nbsp;by<br>
&nbsp;&nbsp;&nbsp;&nbsp;Bahdanau,&nbsp;Cho,&nbsp;and&nbsp;Bengio.&nbsp;You&nbsp;will&nbsp;find&nbsp;this&nbsp;code&nbsp;in&nbsp;a&nbsp;class&nbsp;named<br>
&nbsp;&nbsp;&nbsp;&nbsp;Attention_BCB.&nbsp;&nbsp;For&nbsp;the&nbsp;sake&nbsp;of&nbsp;comparison,&nbsp;I&nbsp;have&nbsp;also&nbsp;included&nbsp;an<br>
&nbsp;&nbsp;&nbsp;&nbsp;implementation&nbsp;of&nbsp;the&nbsp;the&nbsp;attention&nbsp;mechanism&nbsp;used&nbsp;in&nbsp;the&nbsp;very&nbsp;popular&nbsp;NLP<br>
&nbsp;&nbsp;&nbsp;&nbsp;tutorial&nbsp;by&nbsp;Sean&nbsp;Robertson.&nbsp;&nbsp;You&nbsp;will&nbsp;find&nbsp;that&nbsp;code&nbsp;in&nbsp;a&nbsp;class&nbsp;named<br>
&nbsp;&nbsp;&nbsp;&nbsp;Attention_SR.&nbsp;To&nbsp;switch&nbsp;between&nbsp;these&nbsp;two&nbsp;attention&nbsp;mechanisms,&nbsp;all&nbsp;you&nbsp;have&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;do&nbsp;is&nbsp;to&nbsp;comment-out&nbsp;and&nbsp;uncomment&nbsp;a&nbsp;couple&nbsp;of&nbsp;lines&nbsp;in&nbsp;the&nbsp;DecoderRNN&nbsp;code.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.0.8:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;version&nbsp;pulls&nbsp;into&nbsp;DLStudio&nbsp;a&nbsp;very&nbsp;important&nbsp;idea&nbsp;in&nbsp;text&nbsp;processing&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;language&nbsp;modeling&nbsp;---&nbsp;word&nbsp;embeddings.&nbsp;&nbsp;That&nbsp;is,&nbsp;representing&nbsp;words&nbsp;by<br>
&nbsp;&nbsp;&nbsp;&nbsp;fixed-sized&nbsp;numerical&nbsp;vectors&nbsp;that&nbsp;are&nbsp;learned&nbsp;on&nbsp;the&nbsp;basis&nbsp;of&nbsp;their&nbsp;contextual<br>
&nbsp;&nbsp;&nbsp;&nbsp;similarities&nbsp;(meaning&nbsp;that&nbsp;if&nbsp;two&nbsp;words&nbsp;occur&nbsp;frequently&nbsp;in&nbsp;each&nbsp;other's&nbsp;context,<br>
&nbsp;&nbsp;&nbsp;&nbsp;they&nbsp;should&nbsp;have&nbsp;similar&nbsp;numerical&nbsp;representations).&nbsp;&nbsp;Use&nbsp;of&nbsp;word&nbsp;embeddings&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;demonstrated&nbsp;in&nbsp;DLStudio&nbsp;through&nbsp;an&nbsp;inner&nbsp;class&nbsp;named<br>
&nbsp;&nbsp;&nbsp;&nbsp;TextClassificationWithEmbeddings.&nbsp;&nbsp;Using&nbsp;pre-trained&nbsp;word2vec&nbsp;embeddings,&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;new&nbsp;inner&nbsp;class&nbsp;can&nbsp;be&nbsp;used&nbsp;for&nbsp;experimenting&nbsp;with&nbsp;text&nbsp;classification,&nbsp;sentiment<br>
&nbsp;&nbsp;&nbsp;&nbsp;analysis,&nbsp;etc.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.0.7:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Made&nbsp;incremental&nbsp;improvements&nbsp;to&nbsp;the&nbsp;visualization&nbsp;of&nbsp;intermediate&nbsp;results&nbsp;during<br>
&nbsp;&nbsp;&nbsp;&nbsp;training.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.0.6:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;is&nbsp;a&nbsp;result&nbsp;of&nbsp;further&nbsp;clean-up&nbsp;of&nbsp;the&nbsp;code&nbsp;base&nbsp;in&nbsp;DLStudio.&nbsp;&nbsp;The&nbsp;basic<br>
&nbsp;&nbsp;&nbsp;&nbsp;functionality&nbsp;provided&nbsp;by&nbsp;the&nbsp;module&nbsp;has&nbsp;not&nbsp;changed.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.0.5:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;version&nbsp;has&nbsp;a&nbsp;bug-fix&nbsp;for&nbsp;the&nbsp;training&nbsp;loop&nbsp;used&nbsp;for&nbsp;demonstrating&nbsp;the&nbsp;power<br>
&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;skip&nbsp;connections.&nbsp;&nbsp;I&nbsp;have&nbsp;also&nbsp;cleaned&nbsp;up&nbsp;how&nbsp;the&nbsp;intermediate&nbsp;results<br>
&nbsp;&nbsp;&nbsp;&nbsp;produced&nbsp;during&nbsp;training&nbsp;are&nbsp;displayed&nbsp;in&nbsp;your&nbsp;terminal&nbsp;window.&nbsp;&nbsp;In&nbsp;addition,&nbsp;I<br>
&nbsp;&nbsp;&nbsp;&nbsp;deleted&nbsp;the&nbsp;part&nbsp;of&nbsp;DLStudio&nbsp;that&nbsp;dealt&nbsp;with&nbsp;Autograd&nbsp;customization&nbsp;since&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;material&nbsp;is&nbsp;now&nbsp;in&nbsp;my&nbsp;ComputationalGraphPrimer&nbsp;module.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.0.4:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;version&nbsp;mostly&nbsp;changes&nbsp;the&nbsp;HTML&nbsp;formatting&nbsp;of&nbsp;this&nbsp;documentation&nbsp;page.&nbsp;&nbsp;The<br>
&nbsp;&nbsp;&nbsp;&nbsp;code&nbsp;has&nbsp;not&nbsp;changed.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;2.0.3:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;I&nbsp;have&nbsp;been&nbsp;experimenting&nbsp;with&nbsp;how&nbsp;to&nbsp;best&nbsp;incorporate&nbsp;adversarial&nbsp;learning&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;DLStudio&nbsp;platform.&nbsp;That's&nbsp;what&nbsp;accounts&nbsp;for&nbsp;the&nbsp;jump&nbsp;from&nbsp;the&nbsp;previous&nbsp;public<br>
&nbsp;&nbsp;&nbsp;&nbsp;release&nbsp;version&nbsp;1.1.4&nbsp;to&nbsp;new&nbsp;version&nbsp;2.0.3.&nbsp;&nbsp;The&nbsp;latest&nbsp;version&nbsp;comes&nbsp;with&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;separate&nbsp;class&nbsp;named&nbsp;AdversarialLearning&nbsp;for&nbsp;experimenting&nbsp;with&nbsp;different&nbsp;types<br>
&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;such&nbsp;networks&nbsp;for&nbsp;learning&nbsp;data&nbsp;models&nbsp;with&nbsp;adversarial&nbsp;learning&nbsp;and,<br>
&nbsp;&nbsp;&nbsp;&nbsp;subsequently,&nbsp;generating&nbsp;new&nbsp;instances&nbsp;of&nbsp;the&nbsp;data&nbsp;from&nbsp;the&nbsp;learned&nbsp;models.&nbsp;The<br>
&nbsp;&nbsp;&nbsp;&nbsp;AdversarialLearning&nbsp;class&nbsp;includes&nbsp;two&nbsp;Discriminator-Generator&nbsp;(DG)&nbsp;pairs&nbsp;and&nbsp;one<br>
&nbsp;&nbsp;&nbsp;&nbsp;Critic-Generator&nbsp;(CG)&nbsp;pair.&nbsp;Of&nbsp;the&nbsp;two&nbsp;DG&nbsp;pairs,&nbsp;the&nbsp;first&nbsp;is&nbsp;based&nbsp;on&nbsp;the&nbsp;logic<br>
&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;DCGAN,&nbsp;and&nbsp;the&nbsp;second&nbsp;a&nbsp;small&nbsp;modification&nbsp;of&nbsp;the&nbsp;first.&nbsp;&nbsp;The&nbsp;CG&nbsp;pair&nbsp;is&nbsp;based<br>
&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;the&nbsp;logic&nbsp;of&nbsp;Wasserstein&nbsp;GAN.&nbsp;&nbsp;This&nbsp;version&nbsp;of&nbsp;the&nbsp;module&nbsp;also&nbsp;comes&nbsp;with&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;new&nbsp;examples&nbsp;directory,&nbsp;ExamplesAdversarialLearning,&nbsp;that&nbsp;contains&nbsp;example<br>
&nbsp;&nbsp;&nbsp;&nbsp;scripts&nbsp;that&nbsp;show&nbsp;how&nbsp;you&nbsp;can&nbsp;call&nbsp;the&nbsp;different&nbsp;DG&nbsp;and&nbsp;CG&nbsp;pairs&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;AdversarialLearning&nbsp;class.&nbsp;&nbsp;Also&nbsp;included&nbsp;is&nbsp;a&nbsp;new&nbsp;dataset&nbsp;I&nbsp;have&nbsp;created,<br>
&nbsp;&nbsp;&nbsp;&nbsp;PurdueShapes5GAN-20000,&nbsp;that&nbsp;contains&nbsp;20,000&nbsp;images&nbsp;of&nbsp;size&nbsp;64x64&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;experimenting&nbsp;with&nbsp;the&nbsp;GANs&nbsp;in&nbsp;this&nbsp;module.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;1.1.4:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;version&nbsp;has&nbsp;a&nbsp;new&nbsp;design&nbsp;for&nbsp;the&nbsp;text&nbsp;classification&nbsp;class&nbsp;TEXTnetOrder2.<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;has&nbsp;entailed&nbsp;new&nbsp;scripts&nbsp;for&nbsp;training&nbsp;and&nbsp;testing&nbsp;when&nbsp;using&nbsp;the&nbsp;new&nbsp;version<br>
&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;that&nbsp;class.&nbsp;Also&nbsp;includes&nbsp;a&nbsp;fix&nbsp;for&nbsp;a&nbsp;bug&nbsp;discovered&nbsp;in&nbsp;Version&nbsp;1.1.3<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;1.1.3:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;only&nbsp;change&nbsp;made&nbsp;in&nbsp;this&nbsp;version&nbsp;is&nbsp;to&nbsp;the&nbsp;class&nbsp;GRUnet&nbsp;that&nbsp;is&nbsp;used&nbsp;for&nbsp;text<br>
&nbsp;&nbsp;&nbsp;&nbsp;classification.&nbsp;&nbsp;In&nbsp;the&nbsp;new&nbsp;version,&nbsp;the&nbsp;final&nbsp;output&nbsp;of&nbsp;this&nbsp;network&nbsp;is&nbsp;based&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;LogSoftmax&nbsp;activation.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;1.1.2:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;version&nbsp;adds&nbsp;code&nbsp;to&nbsp;the&nbsp;module&nbsp;for&nbsp;experimenting&nbsp;with&nbsp;recurrent&nbsp;neural<br>
&nbsp;&nbsp;&nbsp;&nbsp;networks&nbsp;(RNN)&nbsp;for&nbsp;classifying&nbsp;variable-length&nbsp;text&nbsp;input.&nbsp;With&nbsp;an&nbsp;RNN,&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;variable-length&nbsp;text&nbsp;input&nbsp;can&nbsp;be&nbsp;characterized&nbsp;with&nbsp;a&nbsp;hidden&nbsp;state&nbsp;vector&nbsp;of&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;fixed&nbsp;size.&nbsp;&nbsp;The&nbsp;text&nbsp;processing&nbsp;capabilities&nbsp;of&nbsp;the&nbsp;module&nbsp;allow&nbsp;you&nbsp;to&nbsp;compare<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;results&nbsp;that&nbsp;you&nbsp;may&nbsp;obtain&nbsp;with&nbsp;and&nbsp;without&nbsp;using&nbsp;a&nbsp;GRU.&nbsp;For&nbsp;such<br>
&nbsp;&nbsp;&nbsp;&nbsp;experiments,&nbsp;this&nbsp;version&nbsp;also&nbsp;comes&nbsp;with&nbsp;a&nbsp;text&nbsp;dataset&nbsp;based&nbsp;on&nbsp;an&nbsp;old&nbsp;archive<br>
&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;product&nbsp;reviews&nbsp;made&nbsp;available&nbsp;by&nbsp;Amazon.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;1.1.1:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;version&nbsp;fixes&nbsp;the&nbsp;buggy&nbsp;behavior&nbsp;of&nbsp;the&nbsp;module&nbsp;when&nbsp;using&nbsp;the&nbsp;'depth'<br>
&nbsp;&nbsp;&nbsp;&nbsp;parameter&nbsp;to&nbsp;change&nbsp;the&nbsp;size&nbsp;of&nbsp;a&nbsp;network.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;1.1.0:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;main&nbsp;reason&nbsp;for&nbsp;this&nbsp;version&nbsp;was&nbsp;my&nbsp;observation&nbsp;that&nbsp;when&nbsp;the&nbsp;training&nbsp;data<br>
&nbsp;&nbsp;&nbsp;&nbsp;is&nbsp;intentionally&nbsp;corrupted&nbsp;with&nbsp;a&nbsp;high&nbsp;level&nbsp;of&nbsp;noise,&nbsp;it&nbsp;is&nbsp;possible&nbsp;for&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;output&nbsp;of&nbsp;regression&nbsp;to&nbsp;be&nbsp;a&nbsp;NaN&nbsp;(Not&nbsp;a&nbsp;Number).&nbsp;&nbsp;In&nbsp;my&nbsp;testing&nbsp;at&nbsp;noise&nbsp;levels<br>
&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;20%,&nbsp;50%,&nbsp;and&nbsp;80%,&nbsp;while&nbsp;you&nbsp;do&nbsp;not&nbsp;see&nbsp;this&nbsp;problem&nbsp;when&nbsp;the&nbsp;noise&nbsp;level&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;20%,&nbsp;it&nbsp;definitely&nbsp;becomes&nbsp;a&nbsp;problem&nbsp;when&nbsp;the&nbsp;noise&nbsp;level&nbsp;is&nbsp;at&nbsp;50%.&nbsp;&nbsp;To&nbsp;deal<br>
&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;this&nbsp;issue,&nbsp;this&nbsp;version&nbsp;includes&nbsp;the&nbsp;test&nbsp;'torch.isnan()'&nbsp;in&nbsp;the&nbsp;training<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;testing&nbsp;code&nbsp;for&nbsp;object&nbsp;detection.&nbsp;&nbsp;This&nbsp;version&nbsp;of&nbsp;the&nbsp;module&nbsp;also&nbsp;provides<br>
&nbsp;&nbsp;&nbsp;&nbsp;additional&nbsp;datasets&nbsp;with&nbsp;noise&nbsp;corrupted&nbsp;images&nbsp;with&nbsp;different&nbsp;levels&nbsp;of&nbsp;noise.<br>
&nbsp;&nbsp;&nbsp;&nbsp;However,&nbsp;since&nbsp;the&nbsp;total&nbsp;size&nbsp;of&nbsp;the&nbsp;datasets&nbsp;now&nbsp;exceeds&nbsp;the&nbsp;file-size&nbsp;limit&nbsp;at<br>
&nbsp;&nbsp;&nbsp;&nbsp;'<a href="https://pypi.org">https://pypi.org</a>',&nbsp;you'll&nbsp;need&nbsp;to&nbsp;download&nbsp;them&nbsp;separately&nbsp;from&nbsp;the&nbsp;link<br>
&nbsp;&nbsp;&nbsp;&nbsp;provided&nbsp;in&nbsp;the&nbsp;main&nbsp;documentation&nbsp;page.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;1.0.9:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;With&nbsp;this&nbsp;version,&nbsp;you&nbsp;can&nbsp;now&nbsp;use&nbsp;DLStudio&nbsp;for&nbsp;experiments&nbsp;in&nbsp;semantic<br>
&nbsp;&nbsp;&nbsp;&nbsp;segmentation&nbsp;of&nbsp;images.&nbsp;&nbsp;The&nbsp;code&nbsp;added&nbsp;to&nbsp;the&nbsp;module&nbsp;is&nbsp;in&nbsp;a&nbsp;new&nbsp;inner&nbsp;class<br>
&nbsp;&nbsp;&nbsp;&nbsp;that,&nbsp;as&nbsp;you&nbsp;might&nbsp;guess,&nbsp;is&nbsp;named&nbsp;SemanticSegmentation.&nbsp;&nbsp;The&nbsp;workhorse&nbsp;of&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;inner&nbsp;class&nbsp;is&nbsp;a&nbsp;new&nbsp;implementation&nbsp;of&nbsp;the&nbsp;famous&nbsp;Unet&nbsp;that&nbsp;I&nbsp;have&nbsp;named&nbsp;mUNet<br>
&nbsp;&nbsp;&nbsp;&nbsp;---&nbsp;the&nbsp;prefix&nbsp;"m"&nbsp;stands&nbsp;for&nbsp;"multi"&nbsp;for&nbsp;the&nbsp;ability&nbsp;of&nbsp;the&nbsp;network&nbsp;to&nbsp;segment<br>
&nbsp;&nbsp;&nbsp;&nbsp;out&nbsp;multiple&nbsp;objects&nbsp;simultaneously.&nbsp;&nbsp;This&nbsp;version&nbsp;of&nbsp;DLStudio&nbsp;also&nbsp;comes&nbsp;with&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;new&nbsp;dataset,&nbsp;PurdueShapes5MultiObject,&nbsp;for&nbsp;experimenting&nbsp;with&nbsp;mUNet.&nbsp;&nbsp;Each&nbsp;image<br>
&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;this&nbsp;dataset&nbsp;contains&nbsp;a&nbsp;random&nbsp;number&nbsp;of&nbsp;selections&nbsp;from&nbsp;five&nbsp;different&nbsp;shapes<br>
&nbsp;&nbsp;&nbsp;&nbsp;---&nbsp;rectangle,&nbsp;triangle,&nbsp;disk,&nbsp;oval,&nbsp;and&nbsp;star&nbsp;---&nbsp;that&nbsp;are&nbsp;randomly&nbsp;scaled,<br>
&nbsp;&nbsp;&nbsp;&nbsp;oriented,&nbsp;and&nbsp;located&nbsp;in&nbsp;each&nbsp;image.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;1.0.7:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;main&nbsp;reason&nbsp;for&nbsp;creating&nbsp;this&nbsp;version&nbsp;of&nbsp;DLStudio&nbsp;is&nbsp;to&nbsp;be&nbsp;able&nbsp;to&nbsp;use&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;module&nbsp;for&nbsp;illustrating&nbsp;how&nbsp;to&nbsp;simultaneously&nbsp;carry&nbsp;out&nbsp;classification&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;regression&nbsp;(C&amp;R)&nbsp;with&nbsp;the&nbsp;same&nbsp;convolutional&nbsp;network.&nbsp;&nbsp;The&nbsp;specific&nbsp;C&amp;R&nbsp;problem<br>
&nbsp;&nbsp;&nbsp;&nbsp;that&nbsp;is&nbsp;solved&nbsp;in&nbsp;this&nbsp;version&nbsp;is&nbsp;the&nbsp;problem&nbsp;of&nbsp;object&nbsp;detection&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;localization.&nbsp;You&nbsp;want&nbsp;a&nbsp;CNN&nbsp;to&nbsp;categorize&nbsp;the&nbsp;object&nbsp;in&nbsp;an&nbsp;image&nbsp;and,&nbsp;at&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;same&nbsp;time,&nbsp;estimate&nbsp;the&nbsp;bounding-box&nbsp;for&nbsp;the&nbsp;detected&nbsp;object.&nbsp;Estimating&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;bounding-box&nbsp;is&nbsp;referred&nbsp;to&nbsp;as&nbsp;regression.&nbsp;&nbsp;All&nbsp;of&nbsp;the&nbsp;code&nbsp;related&nbsp;to&nbsp;object<br>
&nbsp;&nbsp;&nbsp;&nbsp;detection&nbsp;and&nbsp;localization&nbsp;is&nbsp;in&nbsp;the&nbsp;inner&nbsp;class&nbsp;DetectAndLocalize&nbsp;of&nbsp;the&nbsp;main<br>
&nbsp;&nbsp;&nbsp;&nbsp;module&nbsp;file.&nbsp;&nbsp;Training&nbsp;a&nbsp;CNN&nbsp;to&nbsp;solve&nbsp;the&nbsp;detection&nbsp;and&nbsp;localization&nbsp;problem<br>
&nbsp;&nbsp;&nbsp;&nbsp;requires&nbsp;a&nbsp;dataset&nbsp;that,&nbsp;in&nbsp;addition&nbsp;to&nbsp;the&nbsp;class&nbsp;labels&nbsp;for&nbsp;the&nbsp;objects,&nbsp;also<br>
&nbsp;&nbsp;&nbsp;&nbsp;provides&nbsp;bounding-box&nbsp;annotations&nbsp;for&nbsp;the&nbsp;objects.&nbsp;&nbsp;Towards&nbsp;that&nbsp;end,&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;version&nbsp;also&nbsp;comes&nbsp;with&nbsp;a&nbsp;new&nbsp;dataset&nbsp;called&nbsp;PurdueShapes5.&nbsp;&nbsp;Another&nbsp;new&nbsp;inner<br>
&nbsp;&nbsp;&nbsp;&nbsp;class,&nbsp;CustomDataLoading,&nbsp;that&nbsp;is&nbsp;also&nbsp;included&nbsp;in&nbsp;Version&nbsp;1.0.7&nbsp;has&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;dataloader&nbsp;for&nbsp;the&nbsp;PurdueShapes5&nbsp;dataset.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;1.0.6:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;version&nbsp;has&nbsp;the&nbsp;bugfix&nbsp;for&nbsp;a&nbsp;bug&nbsp;in&nbsp;SkipBlock&nbsp;that&nbsp;was&nbsp;spotted&nbsp;by&nbsp;a&nbsp;student<br>
&nbsp;&nbsp;&nbsp;&nbsp;as&nbsp;I&nbsp;was&nbsp;demonstrating&nbsp;in&nbsp;class&nbsp;the&nbsp;concepts&nbsp;related&nbsp;to&nbsp;the&nbsp;use&nbsp;of&nbsp;skip<br>
&nbsp;&nbsp;&nbsp;&nbsp;connections&nbsp;in&nbsp;deep&nbsp;neural&nbsp;networks.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;1.0.5:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;version&nbsp;includes&nbsp;an&nbsp;inner&nbsp;class,&nbsp;BMEnet,&nbsp;for&nbsp;experimenting&nbsp;with&nbsp;skip<br>
&nbsp;&nbsp;&nbsp;&nbsp;connections&nbsp;to&nbsp;improve&nbsp;the&nbsp;performance&nbsp;of&nbsp;a&nbsp;deep&nbsp;network.&nbsp;&nbsp;The&nbsp;Examples<br>
&nbsp;&nbsp;&nbsp;&nbsp;subdirectory&nbsp;of&nbsp;the&nbsp;distribution&nbsp;includes&nbsp;a&nbsp;script,<br>
&nbsp;&nbsp;&nbsp;&nbsp;playing_with_skip_connections.py,&nbsp;that&nbsp;demonstrates&nbsp;how&nbsp;you&nbsp;can&nbsp;experiment&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;skip&nbsp;connections&nbsp;in&nbsp;a&nbsp;network.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;1.0.4:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;I&nbsp;have&nbsp;added&nbsp;one&nbsp;more&nbsp;inner&nbsp;class,&nbsp;AutogradCustomization,&nbsp;to&nbsp;the&nbsp;module&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;illustrates&nbsp;how&nbsp;to&nbsp;extend&nbsp;Autograd&nbsp;if&nbsp;you&nbsp;want&nbsp;to&nbsp;endow&nbsp;it&nbsp;with&nbsp;additional<br>
&nbsp;&nbsp;&nbsp;&nbsp;functionality.&nbsp;And,&nbsp;most&nbsp;importantly,&nbsp;this&nbsp;version&nbsp;fixes&nbsp;an&nbsp;important&nbsp;bug&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;caused&nbsp;wrong&nbsp;information&nbsp;to&nbsp;be&nbsp;written&nbsp;out&nbsp;to&nbsp;the&nbsp;disk&nbsp;when&nbsp;you&nbsp;tried&nbsp;to&nbsp;save&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;learned&nbsp;model&nbsp;at&nbsp;the&nbsp;end&nbsp;of&nbsp;a&nbsp;training&nbsp;session.&nbsp;I&nbsp;have&nbsp;also&nbsp;cleaned&nbsp;up&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;comment&nbsp;blocks&nbsp;in&nbsp;the&nbsp;implementation&nbsp;code.<br>
&nbsp;<br>
&nbsp;&nbsp;Version&nbsp;1.0.3:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;is&nbsp;the&nbsp;first&nbsp;public&nbsp;release&nbsp;version&nbsp;of&nbsp;this&nbsp;module.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:red; font-size:150%"><strong><a id="101">INTRODUCTION</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;DLStudio&nbsp;is&nbsp;an&nbsp;integrated&nbsp;software&nbsp;platform&nbsp;for&nbsp;teaching&nbsp;(and&nbsp;learning)&nbsp;a&nbsp;wide<br>
&nbsp;&nbsp;&nbsp;&nbsp;range&nbsp;of&nbsp;basic&nbsp;architectural&nbsp;features&nbsp;of&nbsp;deep-learning&nbsp;neural&nbsp;networks.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;To&nbsp;get&nbsp;the&nbsp;most&nbsp;educational&nbsp;value&nbsp;out&nbsp;of&nbsp;DLStudio,&nbsp;please&nbsp;see&nbsp;the&nbsp;slides&nbsp;for&nbsp;my<br>
&nbsp;&nbsp;&nbsp;&nbsp;lectures&nbsp;at&nbsp;Purdue's&nbsp;Deep&nbsp;Learning&nbsp;class.&nbsp;&nbsp;Most&nbsp;of&nbsp;the&nbsp;learning&nbsp;you&nbsp;do&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;DLStudio&nbsp;is&nbsp;through&nbsp;the&nbsp;scripts&nbsp;in&nbsp;the&nbsp;various&nbsp;Example&nbsp;directories&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;distribution.&nbsp;&nbsp;To&nbsp;get&nbsp;access&nbsp;to&nbsp;these&nbsp;Example&nbsp;directories,&nbsp;please&nbsp;do&nbsp;NOT&nbsp;do&nbsp;"sudo<br>
&nbsp;&nbsp;&nbsp;&nbsp;pip&nbsp;install"&nbsp;on&nbsp;this&nbsp;module&nbsp;since&nbsp;that&nbsp;only&nbsp;gives&nbsp;you&nbsp;the&nbsp;main&nbsp;module&nbsp;files.&nbsp;&nbsp;You<br>
&nbsp;&nbsp;&nbsp;&nbsp;would&nbsp;need&nbsp;to&nbsp;install&nbsp;it&nbsp;from&nbsp;the&nbsp;tar&nbsp;archive&nbsp;according&nbsp;to&nbsp;the&nbsp;installation<br>
&nbsp;&nbsp;&nbsp;&nbsp;instructions&nbsp;in&nbsp;this&nbsp;documentation&nbsp;page.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;As&nbsp;to&nbsp;why&nbsp;you&nbsp;may&nbsp;find&nbsp;DLStudio&nbsp;useful&nbsp;for&nbsp;your&nbsp;learning,&nbsp;note&nbsp;that&nbsp;most<br>
&nbsp;&nbsp;&nbsp;&nbsp;instructors&nbsp;who&nbsp;teach&nbsp;deep&nbsp;learning&nbsp;ask&nbsp;their&nbsp;students&nbsp;to&nbsp;download&nbsp;the&nbsp;so-called<br>
&nbsp;&nbsp;&nbsp;&nbsp;famous&nbsp;networks&nbsp;from,&nbsp;say,&nbsp;GitHub&nbsp;and&nbsp;become&nbsp;familiar&nbsp;with&nbsp;them&nbsp;by&nbsp;running&nbsp;them<br>
&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;the&nbsp;datasets&nbsp;used&nbsp;by&nbsp;the&nbsp;authors&nbsp;of&nbsp;those&nbsp;networks.&nbsp;&nbsp;This&nbsp;approach&nbsp;is&nbsp;akin&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;teaching&nbsp;automobile&nbsp;engineering&nbsp;by&nbsp;asking&nbsp;the&nbsp;students&nbsp;to&nbsp;take&nbsp;the&nbsp;high-powered<br>
&nbsp;&nbsp;&nbsp;&nbsp;cars&nbsp;of&nbsp;the&nbsp;day&nbsp;out&nbsp;for&nbsp;a&nbsp;test&nbsp;drive.&nbsp;&nbsp;In&nbsp;my&nbsp;opinion,&nbsp;this&nbsp;rather&nbsp;commonly&nbsp;used<br>
&nbsp;&nbsp;&nbsp;&nbsp;approach&nbsp;does&nbsp;not&nbsp;work&nbsp;for&nbsp;instilling&nbsp;in&nbsp;the&nbsp;students&nbsp;a&nbsp;deep&nbsp;understanding&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;issues&nbsp;related&nbsp;to&nbsp;network&nbsp;architectures.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Most&nbsp;instructors&nbsp;who&nbsp;teach&nbsp;deep&nbsp;learning&nbsp;ask&nbsp;their&nbsp;students&nbsp;to&nbsp;download&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;so-called&nbsp;famous&nbsp;networks&nbsp;from,&nbsp;say,&nbsp;GitHub&nbsp;and&nbsp;become&nbsp;familiar&nbsp;with&nbsp;them&nbsp;by<br>
&nbsp;&nbsp;&nbsp;&nbsp;running&nbsp;them&nbsp;on&nbsp;the&nbsp;datasets&nbsp;used&nbsp;by&nbsp;the&nbsp;authors&nbsp;of&nbsp;those&nbsp;networks.&nbsp;&nbsp;This<br>
&nbsp;&nbsp;&nbsp;&nbsp;approach&nbsp;is&nbsp;akin&nbsp;to&nbsp;teaching&nbsp;automobile&nbsp;engineering&nbsp;by&nbsp;asking&nbsp;the&nbsp;students&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;take&nbsp;the&nbsp;high-powered&nbsp;cars&nbsp;of&nbsp;the&nbsp;day&nbsp;out&nbsp;for&nbsp;a&nbsp;test&nbsp;drive.&nbsp;&nbsp;In&nbsp;my&nbsp;opinion,&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;rather&nbsp;commonly&nbsp;used&nbsp;approach&nbsp;does&nbsp;not&nbsp;work&nbsp;for&nbsp;instilling&nbsp;in&nbsp;the&nbsp;students&nbsp;a&nbsp;deep<br>
&nbsp;&nbsp;&nbsp;&nbsp;understanding&nbsp;of&nbsp;the&nbsp;issues&nbsp;related&nbsp;to&nbsp;network&nbsp;architectures.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;On&nbsp;the&nbsp;other&nbsp;hand,&nbsp;DLStudio&nbsp;offers&nbsp;its&nbsp;own&nbsp;implementations&nbsp;for&nbsp;a&nbsp;variety&nbsp;of&nbsp;key<br>
&nbsp;&nbsp;&nbsp;&nbsp;features&nbsp;of&nbsp;neural&nbsp;network&nbsp;architectures.&nbsp;&nbsp;These&nbsp;implementations,&nbsp;along&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;their&nbsp;explanations&nbsp;through&nbsp;detailed&nbsp;slide&nbsp;presentations&nbsp;at&nbsp;our&nbsp;Deep&nbsp;Learning<br>
&nbsp;&nbsp;&nbsp;&nbsp;class&nbsp;website&nbsp;at&nbsp;Purdue,&nbsp;result&nbsp;in&nbsp;an&nbsp;educational&nbsp;framework&nbsp;that&nbsp;is&nbsp;much&nbsp;more<br>
&nbsp;&nbsp;&nbsp;&nbsp;efficient&nbsp;in&nbsp;what&nbsp;it&nbsp;can&nbsp;deliver&nbsp;within&nbsp;the&nbsp;time&nbsp;constraints&nbsp;of&nbsp;a&nbsp;single<br>
&nbsp;&nbsp;&nbsp;&nbsp;semester.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;DLStudio&nbsp;facilitates&nbsp;learning&nbsp;through&nbsp;a&nbsp;combination&nbsp;of&nbsp;inner&nbsp;classes&nbsp;of&nbsp;the&nbsp;main<br>
&nbsp;&nbsp;&nbsp;&nbsp;module&nbsp;class&nbsp;---&nbsp;called&nbsp;DLStudio&nbsp;naturally&nbsp;---&nbsp;and&nbsp;several&nbsp;additional&nbsp;modules&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;overall&nbsp;platform.&nbsp;&nbsp;These&nbsp;modules&nbsp;deal&nbsp;with&nbsp;Adversarial&nbsp;Learning,&nbsp;Metric<br>
&nbsp;&nbsp;&nbsp;&nbsp;Learning,&nbsp;Variational&nbsp;Autoencoding,&nbsp;Diffusion,&nbsp;Data&nbsp;Prediction,&nbsp;Text&nbsp;Analytics,<br>
&nbsp;&nbsp;&nbsp;&nbsp;Transformer&nbsp;based&nbsp;learning,&nbsp;etc.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;For&nbsp;the&nbsp;most&nbsp;part,&nbsp;the&nbsp;common&nbsp;code&nbsp;that&nbsp;you'd&nbsp;need&nbsp;in&nbsp;different&nbsp;scenarios&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;using&nbsp;neural&nbsp;networks&nbsp;has&nbsp;been&nbsp;placed&nbsp;inside&nbsp;the&nbsp;definition&nbsp;of&nbsp;the&nbsp;main&nbsp;DLStudio<br>
&nbsp;&nbsp;&nbsp;&nbsp;class&nbsp;in&nbsp;a&nbsp;file&nbsp;named&nbsp;DLStudio.py&nbsp;in&nbsp;the&nbsp;distribution.&nbsp;&nbsp;That&nbsp;makes&nbsp;more&nbsp;compact<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;definition&nbsp;of&nbsp;the&nbsp;other&nbsp;inner&nbsp;classes&nbsp;within&nbsp;DLStudio.&nbsp;And,&nbsp;to&nbsp;a&nbsp;certain<br>
&nbsp;&nbsp;&nbsp;&nbsp;extent,&nbsp;that&nbsp;also&nbsp;results&nbsp;in&nbsp;a&nbsp;bit&nbsp;more&nbsp;compact&nbsp;code&nbsp;in&nbsp;the&nbsp;different&nbsp;modules&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;DLStudio&nbsp;platform.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:blue; font-size:100%"><strong>&nbsp;&nbsp;<a id="102">    SKIP CONNECTIONS</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;You&nbsp;can&nbsp;use&nbsp;DLStudio's&nbsp;inner&nbsp;class&nbsp;BMEnet&nbsp;to&nbsp;experiment&nbsp;with&nbsp;connection&nbsp;skipping<br>
&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;a&nbsp;deep&nbsp;network.&nbsp;Connection&nbsp;skipping&nbsp;means&nbsp;to&nbsp;provide&nbsp;shortcuts&nbsp;in&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;computational&nbsp;graph&nbsp;around&nbsp;the&nbsp;commonly&nbsp;used&nbsp;network&nbsp;components&nbsp;like<br>
&nbsp;&nbsp;&nbsp;&nbsp;convolutional&nbsp;and&nbsp;other&nbsp;types&nbsp;of&nbsp;layers.&nbsp;&nbsp;In&nbsp;the&nbsp;absence&nbsp;of&nbsp;such&nbsp;shortcuts,&nbsp;deep<br>
&nbsp;&nbsp;&nbsp;&nbsp;networks&nbsp;suffer&nbsp;from&nbsp;the&nbsp;problem&nbsp;of&nbsp;vanishing&nbsp;gradients&nbsp;that&nbsp;degrades&nbsp;their<br>
&nbsp;&nbsp;&nbsp;&nbsp;performance.&nbsp;&nbsp;Vanishing&nbsp;gradients&nbsp;means&nbsp;that&nbsp;the&nbsp;gradients&nbsp;of&nbsp;the&nbsp;loss&nbsp;calculated<br>
&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;the&nbsp;early&nbsp;layers&nbsp;of&nbsp;a&nbsp;network&nbsp;become&nbsp;increasingly&nbsp;muted&nbsp;as&nbsp;the&nbsp;network&nbsp;becomes<br>
&nbsp;&nbsp;&nbsp;&nbsp;deeper.&nbsp;&nbsp;An&nbsp;important&nbsp;mitigation&nbsp;strategy&nbsp;for&nbsp;addressing&nbsp;this&nbsp;problem&nbsp;consists&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;creating&nbsp;a&nbsp;CNN&nbsp;using&nbsp;blocks&nbsp;with&nbsp;skip&nbsp;connections.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;As&nbsp;shown&nbsp;in&nbsp;the&nbsp;script&nbsp;playing_with_skip_connections.py&nbsp;in&nbsp;the&nbsp;Examples&nbsp;directory<br>
&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;the&nbsp;distribution,&nbsp;you&nbsp;can&nbsp;easily&nbsp;create&nbsp;a&nbsp;CNN&nbsp;with&nbsp;arbitrary&nbsp;depth&nbsp;just&nbsp;by<br>
&nbsp;&nbsp;&nbsp;&nbsp;using&nbsp;the&nbsp;constructor&nbsp;option&nbsp;"depth"&nbsp;for&nbsp;BMEnet.&nbsp;The&nbsp;basic&nbsp;block&nbsp;of&nbsp;the&nbsp;network<br>
&nbsp;&nbsp;&nbsp;&nbsp;constructed&nbsp;in&nbsp;this&nbsp;manner&nbsp;is&nbsp;called&nbsp;SkipBlock&nbsp;which,&nbsp;very&nbsp;much&nbsp;like&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;BasicBlock&nbsp;in&nbsp;ResNet-18,&nbsp;has&nbsp;a&nbsp;couple&nbsp;of&nbsp;convolutional&nbsp;layers&nbsp;whose&nbsp;output&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;combined&nbsp;with&nbsp;the&nbsp;input&nbsp;to&nbsp;the&nbsp;block.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Note&nbsp;that&nbsp;the&nbsp;value&nbsp;given&nbsp;to&nbsp;the&nbsp;"depth"&nbsp;constructor&nbsp;option&nbsp;for&nbsp;the&nbsp;BMEnet&nbsp;class<br>
&nbsp;&nbsp;&nbsp;&nbsp;does&nbsp;NOT&nbsp;translate&nbsp;directly&nbsp;into&nbsp;the&nbsp;actual&nbsp;depth&nbsp;of&nbsp;the&nbsp;CNN.&nbsp;[Again,&nbsp;see&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;script&nbsp;playing_with_skip_connections.py&nbsp;in&nbsp;the&nbsp;Examples&nbsp;directory&nbsp;for&nbsp;how&nbsp;to&nbsp;use<br>
&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;option.]&nbsp;The&nbsp;value&nbsp;of&nbsp;"depth"&nbsp;is&nbsp;translated&nbsp;into&nbsp;how&nbsp;many&nbsp;instances&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;SkipBlock&nbsp;to&nbsp;use&nbsp;for&nbsp;constructing&nbsp;the&nbsp;CNN.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;you&nbsp;want&nbsp;to&nbsp;use&nbsp;DLStudio&nbsp;for&nbsp;learning&nbsp;how&nbsp;to&nbsp;create&nbsp;your&nbsp;own&nbsp;versions&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;SkipBlock-like&nbsp;shortcuts&nbsp;in&nbsp;a&nbsp;CNN,&nbsp;your&nbsp;starting&nbsp;point&nbsp;should&nbsp;be&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;script&nbsp;in&nbsp;the&nbsp;Examples&nbsp;directory&nbsp;of&nbsp;the&nbsp;distro:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;playing_with_skip_connections.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;script&nbsp;illustrates&nbsp;how&nbsp;to&nbsp;use&nbsp;the&nbsp;inner&nbsp;class&nbsp;BMEnet&nbsp;of&nbsp;the&nbsp;module&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;experimenting&nbsp;with&nbsp;skip&nbsp;connections&nbsp;in&nbsp;a&nbsp;CNN.&nbsp;As&nbsp;the&nbsp;script&nbsp;shows,&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;constructor&nbsp;of&nbsp;the&nbsp;BMEnet&nbsp;class&nbsp;comes&nbsp;with&nbsp;two&nbsp;options:&nbsp;skip_connections&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;depth.&nbsp;&nbsp;By&nbsp;turning&nbsp;the&nbsp;first&nbsp;on&nbsp;and&nbsp;off,&nbsp;you&nbsp;can&nbsp;directly&nbsp;illustrate&nbsp;in&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;classroom&nbsp;setting&nbsp;the&nbsp;improvement&nbsp;you&nbsp;can&nbsp;get&nbsp;with&nbsp;skip&nbsp;connections.&nbsp;&nbsp;And&nbsp;by<br>
&nbsp;&nbsp;&nbsp;&nbsp;giving&nbsp;an&nbsp;appropriate&nbsp;value&nbsp;to&nbsp;the&nbsp;"depth"&nbsp;option,&nbsp;you&nbsp;can&nbsp;show&nbsp;results&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;networks&nbsp;of&nbsp;different&nbsp;depths.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:blue; font-size:100%"><strong>&nbsp;&nbsp;<a id="103">    OBJECT DETECTION AND LOCALIZATION</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;code&nbsp;for&nbsp;how&nbsp;to&nbsp;solve&nbsp;the&nbsp;problem&nbsp;of&nbsp;object&nbsp;detection&nbsp;and&nbsp;localization&nbsp;with&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;CNN&nbsp;is&nbsp;in&nbsp;the&nbsp;inner&nbsp;classes&nbsp;DetectAndLocalize&nbsp;and&nbsp;CustomDataLoading.&nbsp;&nbsp;This&nbsp;code<br>
&nbsp;&nbsp;&nbsp;&nbsp;was&nbsp;developed&nbsp;for&nbsp;version&nbsp;1.0.7&nbsp;of&nbsp;the&nbsp;module.&nbsp;&nbsp;In&nbsp;general,&nbsp;object&nbsp;detection&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;localization&nbsp;problems&nbsp;are&nbsp;more&nbsp;challenging&nbsp;than&nbsp;pure&nbsp;classification&nbsp;problems<br>
&nbsp;&nbsp;&nbsp;&nbsp;because&nbsp;solving&nbsp;the&nbsp;localization&nbsp;part&nbsp;requires&nbsp;regression&nbsp;for&nbsp;the&nbsp;coordinates&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;bounding&nbsp;box&nbsp;that&nbsp;localize&nbsp;the&nbsp;object.&nbsp;&nbsp;If&nbsp;at&nbsp;all&nbsp;possible,&nbsp;you&nbsp;would&nbsp;want<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;same&nbsp;CNN&nbsp;to&nbsp;provide&nbsp;answers&nbsp;to&nbsp;both&nbsp;the&nbsp;classification&nbsp;and&nbsp;the&nbsp;regression<br>
&nbsp;&nbsp;&nbsp;&nbsp;questions&nbsp;and&nbsp;do&nbsp;so&nbsp;at&nbsp;the&nbsp;same&nbsp;time.&nbsp;&nbsp;This&nbsp;calls&nbsp;for&nbsp;a&nbsp;CNN&nbsp;to&nbsp;possess&nbsp;two<br>
&nbsp;&nbsp;&nbsp;&nbsp;different&nbsp;output&nbsp;layers,&nbsp;one&nbsp;for&nbsp;classification&nbsp;and&nbsp;the&nbsp;other&nbsp;for&nbsp;regression.&nbsp;&nbsp;A<br>
&nbsp;&nbsp;&nbsp;&nbsp;deep&nbsp;network&nbsp;that&nbsp;does&nbsp;exactly&nbsp;that&nbsp;is&nbsp;illustrated&nbsp;by&nbsp;the&nbsp;LOADnet&nbsp;classes&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;defined&nbsp;in&nbsp;the&nbsp;inner&nbsp;class&nbsp;DetectAndLocalize&nbsp;of&nbsp;the&nbsp;DLStudio&nbsp;platform.&nbsp;&nbsp;[By<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;way,&nbsp;the&nbsp;acronym&nbsp;"LOAD"&nbsp;in&nbsp;"LOADnet"&nbsp;stands&nbsp;for&nbsp;"LOcalization&nbsp;And<br>
&nbsp;&nbsp;&nbsp;&nbsp;Detection".]&nbsp;Although&nbsp;you&nbsp;will&nbsp;find&nbsp;three&nbsp;versions&nbsp;of&nbsp;the&nbsp;LOADnet&nbsp;class&nbsp;inside<br>
&nbsp;&nbsp;&nbsp;&nbsp;DetectAndLocalize,&nbsp;for&nbsp;now&nbsp;only&nbsp;pay&nbsp;attention&nbsp;to&nbsp;the&nbsp;LOADnet2&nbsp;class&nbsp;since&nbsp;that&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;one&nbsp;I&nbsp;have&nbsp;worked&nbsp;with&nbsp;the&nbsp;most&nbsp;for&nbsp;creating&nbsp;the&nbsp;1.0.7&nbsp;distribution.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;As&nbsp;you&nbsp;would&nbsp;expect,&nbsp;training&nbsp;a&nbsp;CNN&nbsp;for&nbsp;object&nbsp;detection&nbsp;and&nbsp;localization<br>
&nbsp;&nbsp;&nbsp;&nbsp;requires&nbsp;a&nbsp;dataset&nbsp;that,&nbsp;in&nbsp;addition&nbsp;to&nbsp;the&nbsp;class&nbsp;labels&nbsp;for&nbsp;the&nbsp;images,&nbsp;also<br>
&nbsp;&nbsp;&nbsp;&nbsp;provides&nbsp;bounding-box&nbsp;annotations&nbsp;for&nbsp;the&nbsp;objects&nbsp;in&nbsp;the&nbsp;images.&nbsp;Out&nbsp;of&nbsp;my&nbsp;great<br>
&nbsp;&nbsp;&nbsp;&nbsp;admiration&nbsp;for&nbsp;the&nbsp;CIFAR-10&nbsp;dataset&nbsp;as&nbsp;an&nbsp;educational&nbsp;tool&nbsp;for&nbsp;solving<br>
&nbsp;&nbsp;&nbsp;&nbsp;classification&nbsp;problems,&nbsp;I&nbsp;have&nbsp;created&nbsp;small-image-format&nbsp;training&nbsp;and&nbsp;testing<br>
&nbsp;&nbsp;&nbsp;&nbsp;datasets&nbsp;for&nbsp;illustrating&nbsp;the&nbsp;code&nbsp;devoted&nbsp;to&nbsp;object&nbsp;detection&nbsp;and&nbsp;localization<br>
&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;this&nbsp;module.&nbsp;&nbsp;The&nbsp;training&nbsp;dataset&nbsp;is&nbsp;named&nbsp;PurdueShapes5-10000-train.gz&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;it&nbsp;consists&nbsp;of&nbsp;10,000&nbsp;images,&nbsp;with&nbsp;each&nbsp;image&nbsp;of&nbsp;size&nbsp;32x32&nbsp;containing&nbsp;one&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;five&nbsp;possible&nbsp;shapes&nbsp;---&nbsp;rectangle,&nbsp;triangle,&nbsp;disk,&nbsp;oval,&nbsp;and&nbsp;star.&nbsp;The&nbsp;shape<br>
&nbsp;&nbsp;&nbsp;&nbsp;objects&nbsp;in&nbsp;the&nbsp;images&nbsp;are&nbsp;randomized&nbsp;with&nbsp;respect&nbsp;to&nbsp;size,&nbsp;orientation,&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;color.&nbsp;&nbsp;The&nbsp;testing&nbsp;dataset&nbsp;is&nbsp;named&nbsp;PurdueShapes5-1000-test.gz&nbsp;and&nbsp;it&nbsp;contains<br>
&nbsp;&nbsp;&nbsp;&nbsp;1000&nbsp;images&nbsp;generated&nbsp;by&nbsp;the&nbsp;same&nbsp;randomization&nbsp;process&nbsp;as&nbsp;used&nbsp;for&nbsp;the&nbsp;training<br>
&nbsp;&nbsp;&nbsp;&nbsp;dataset.&nbsp;&nbsp;You&nbsp;will&nbsp;find&nbsp;these&nbsp;datasets&nbsp;in&nbsp;the&nbsp;"data"&nbsp;subdirectory&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;"Examples"&nbsp;directory&nbsp;in&nbsp;the&nbsp;distribution.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Providing&nbsp;a&nbsp;new&nbsp;dataset&nbsp;for&nbsp;experiments&nbsp;with&nbsp;detection&nbsp;and&nbsp;localization&nbsp;meant<br>
&nbsp;&nbsp;&nbsp;&nbsp;that&nbsp;I&nbsp;also&nbsp;needed&nbsp;to&nbsp;supply&nbsp;a&nbsp;custom&nbsp;dataloader&nbsp;for&nbsp;the&nbsp;dataset.&nbsp;&nbsp;Toward&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;end,&nbsp;Version&nbsp;1.0.7&nbsp;also&nbsp;includes&nbsp;another&nbsp;inner&nbsp;class&nbsp;named&nbsp;CustomDataLoading<br>
&nbsp;&nbsp;&nbsp;&nbsp;where&nbsp;you&nbsp;will&nbsp;my&nbsp;implementation&nbsp;of&nbsp;the&nbsp;custom&nbsp;dataloader&nbsp;for&nbsp;the&nbsp;PurdueShapes5<br>
&nbsp;&nbsp;&nbsp;&nbsp;dataset.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;you&nbsp;want&nbsp;to&nbsp;use&nbsp;DLStudio&nbsp;for&nbsp;learning&nbsp;how&nbsp;to&nbsp;write&nbsp;your&nbsp;own&nbsp;PyTorch&nbsp;code&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;object&nbsp;detection&nbsp;and&nbsp;localization,&nbsp;your&nbsp;starting&nbsp;point&nbsp;should&nbsp;be&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;script&nbsp;in&nbsp;the&nbsp;Examples&nbsp;directory&nbsp;of&nbsp;the&nbsp;distro:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;object_detection_and_localization.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Execute&nbsp;the&nbsp;script&nbsp;and&nbsp;understand&nbsp;what&nbsp;functionality&nbsp;of&nbsp;the&nbsp;inner&nbsp;class<br>
&nbsp;&nbsp;&nbsp;&nbsp;DetectAndLocalize&nbsp;it&nbsp;invokes&nbsp;for&nbsp;object&nbsp;detection&nbsp;and&nbsp;localization.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:blue; font-size:100%"><strong>&nbsp;&nbsp;<a id="104">    NOISY OBJECT DETECTION AND LOCALIZATION</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;When&nbsp;the&nbsp;training&nbsp;data&nbsp;is&nbsp;intentionally&nbsp;corrupted&nbsp;with&nbsp;a&nbsp;high&nbsp;level&nbsp;of&nbsp;noise,&nbsp;it<br>
&nbsp;&nbsp;&nbsp;&nbsp;is&nbsp;possible&nbsp;for&nbsp;the&nbsp;output&nbsp;of&nbsp;regression&nbsp;to&nbsp;be&nbsp;a&nbsp;NaN&nbsp;(Not&nbsp;a&nbsp;Number).&nbsp;&nbsp;Here&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;what&nbsp;I&nbsp;observed&nbsp;when&nbsp;I&nbsp;tested&nbsp;the&nbsp;LOADnet2&nbsp;network&nbsp;at&nbsp;noise&nbsp;levels&nbsp;of&nbsp;20%,&nbsp;50%,<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;80%:&nbsp;At&nbsp;20%&nbsp;noise,&nbsp;both&nbsp;the&nbsp;labeling&nbsp;and&nbsp;the&nbsp;regression&nbsp;accuracies&nbsp;become<br>
&nbsp;&nbsp;&nbsp;&nbsp;worse&nbsp;compared&nbsp;to&nbsp;the&nbsp;noiseless&nbsp;case,&nbsp;but&nbsp;they&nbsp;would&nbsp;still&nbsp;be&nbsp;usable&nbsp;depending&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;application.&nbsp;&nbsp;For&nbsp;example,&nbsp;with&nbsp;two&nbsp;epochs&nbsp;of&nbsp;training,&nbsp;the&nbsp;overall<br>
&nbsp;&nbsp;&nbsp;&nbsp;classification&nbsp;accuracy&nbsp;decreases&nbsp;from&nbsp;91%&nbsp;to&nbsp;83%&nbsp;and&nbsp;the&nbsp;regression&nbsp;error<br>
&nbsp;&nbsp;&nbsp;&nbsp;increases&nbsp;from&nbsp;under&nbsp;a&nbsp;pixel&nbsp;(on&nbsp;the&nbsp;average)&nbsp;to&nbsp;around&nbsp;3&nbsp;pixels.&nbsp;&nbsp;However,&nbsp;when<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;level&nbsp;of&nbsp;noise&nbsp;is&nbsp;increased&nbsp;to&nbsp;50%,&nbsp;the&nbsp;regression&nbsp;output&nbsp;is&nbsp;often&nbsp;a&nbsp;NaN&nbsp;(Not<br>
&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;Number),&nbsp;as&nbsp;presented&nbsp;by&nbsp;'numpy.nan'&nbsp;or&nbsp;'torch.nan'.&nbsp;&nbsp;To&nbsp;deal&nbsp;with&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;problem,&nbsp;Version&nbsp;1.1.0&nbsp;of&nbsp;the&nbsp;DLStudio&nbsp;platform&nbsp;checks&nbsp;the&nbsp;output&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;bounding-box&nbsp;regression&nbsp;before&nbsp;drawing&nbsp;the&nbsp;rectangles&nbsp;on&nbsp;the&nbsp;images.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;you&nbsp;wish&nbsp;to&nbsp;experiment&nbsp;with&nbsp;detection&nbsp;and&nbsp;localization&nbsp;in&nbsp;the&nbsp;presence&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;noise,&nbsp;your&nbsp;starting&nbsp;point&nbsp;should&nbsp;be&nbsp;the&nbsp;script<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;noisy_object_detection_and_localization.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;the&nbsp;Examples&nbsp;directory&nbsp;of&nbsp;the&nbsp;distribution.&nbsp;&nbsp;Note&nbsp;that&nbsp;you&nbsp;would&nbsp;need&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;download&nbsp;the&nbsp;datasets&nbsp;for&nbsp;such&nbsp;experiments&nbsp;directly&nbsp;from&nbsp;the&nbsp;link&nbsp;provided&nbsp;near<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;top&nbsp;of&nbsp;this&nbsp;documentation&nbsp;page.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:blue; font-size:100%"><strong>&nbsp;&nbsp;<a id="105">    IoU REGRESSION FOR OBJECT DETECTION AND LOCALIZATION</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Starting&nbsp;with&nbsp;version&nbsp;2.2.3,&nbsp;DLStudio&nbsp;illustrates&nbsp;how&nbsp;you&nbsp;can&nbsp;use&nbsp;modern&nbsp;variants<br>
&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;the&nbsp;IoU&nbsp;(Intersection&nbsp;over&nbsp;Union)&nbsp;loss&nbsp;function&nbsp;for&nbsp;the&nbsp;regression&nbsp;needed&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;object&nbsp;localization.&nbsp;&nbsp;These&nbsp;loss&nbsp;functions&nbsp;are&nbsp;provided&nbsp;by&nbsp;the&nbsp;DIoULoss&nbsp;class<br>
&nbsp;&nbsp;&nbsp;&nbsp;that&nbsp;is&nbsp;a&nbsp;part&nbsp;of&nbsp;DLStudio's&nbsp;inner&nbsp;class&nbsp;DetectAndLocalize.&nbsp;If&nbsp;you&nbsp;wish&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;experiment&nbsp;with&nbsp;these&nbsp;loss&nbsp;functions,&nbsp;you&nbsp;best&nbsp;entry&nbsp;point&nbsp;would&nbsp;be&nbsp;the&nbsp;script<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;object_detection_and_localization_iou.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;the&nbsp;Examples&nbsp;directory&nbsp;of&nbsp;the&nbsp;distribution.&nbsp;&nbsp;This&nbsp;script&nbsp;uses&nbsp;the&nbsp;same<br>
&nbsp;&nbsp;&nbsp;&nbsp;PurdueShapes5-10000-train.gz&nbsp;and&nbsp;PurdueShapes5-1000-test.gz&nbsp;training&nbsp;and&nbsp;testing<br>
&nbsp;&nbsp;&nbsp;&nbsp;datasets&nbsp;as&nbsp;the&nbsp;object_detection_and_localization.py&nbsp;script&nbsp;mentioned&nbsp;earlier.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:blue; font-size:100%"><strong>&nbsp;&nbsp;<a id="106">    SEMANTIC SEGMENTATION</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;code&nbsp;for&nbsp;how&nbsp;to&nbsp;carry&nbsp;out&nbsp;semantic&nbsp;segmentation&nbsp;is&nbsp;in&nbsp;the&nbsp;inner&nbsp;class&nbsp;that&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;appropriately&nbsp;named&nbsp;SemanticSegmentation.&nbsp;&nbsp;At&nbsp;its&nbsp;simplest,&nbsp;the&nbsp;purpose&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;semantic&nbsp;segmentation&nbsp;is&nbsp;to&nbsp;assign&nbsp;correct&nbsp;labels&nbsp;to&nbsp;the&nbsp;different&nbsp;objects&nbsp;in&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;scene,&nbsp;while&nbsp;localizing&nbsp;them&nbsp;at&nbsp;the&nbsp;same&nbsp;time.&nbsp;&nbsp;At&nbsp;a&nbsp;more&nbsp;sophisticated&nbsp;level,&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;system&nbsp;that&nbsp;carries&nbsp;out&nbsp;semantic&nbsp;segmentation&nbsp;should&nbsp;also&nbsp;output&nbsp;a&nbsp;symbolic<br>
&nbsp;&nbsp;&nbsp;&nbsp;expression&nbsp;that&nbsp;reflects&nbsp;an&nbsp;understanding&nbsp;of&nbsp;the&nbsp;scene&nbsp;in&nbsp;the&nbsp;image&nbsp;that&nbsp;is&nbsp;based<br>
&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;the&nbsp;objects&nbsp;found&nbsp;in&nbsp;the&nbsp;image&nbsp;and&nbsp;their&nbsp;spatial&nbsp;relationships&nbsp;with&nbsp;one<br>
&nbsp;&nbsp;&nbsp;&nbsp;another.&nbsp;&nbsp;The&nbsp;code&nbsp;in&nbsp;the&nbsp;new&nbsp;inner&nbsp;class&nbsp;is&nbsp;based&nbsp;on&nbsp;only&nbsp;the&nbsp;simplest&nbsp;possible<br>
&nbsp;&nbsp;&nbsp;&nbsp;definition&nbsp;of&nbsp;what&nbsp;is&nbsp;meant&nbsp;by&nbsp;semantic&nbsp;segmentation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;convolutional&nbsp;network&nbsp;that&nbsp;carries&nbsp;out&nbsp;semantic&nbsp;segmentation&nbsp;DLStudio&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;named&nbsp;mUNet,&nbsp;where&nbsp;the&nbsp;letter&nbsp;"m"&nbsp;is&nbsp;short&nbsp;for&nbsp;"multi",&nbsp;which,&nbsp;in&nbsp;turn,&nbsp;stands<br>
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;the&nbsp;fact&nbsp;that&nbsp;mUNet&nbsp;is&nbsp;capable&nbsp;of&nbsp;segmenting&nbsp;out&nbsp;multiple&nbsp;object<br>
&nbsp;&nbsp;&nbsp;&nbsp;simultaneously&nbsp;from&nbsp;an&nbsp;image.&nbsp;&nbsp;The&nbsp;mUNet&nbsp;network&nbsp;is&nbsp;based&nbsp;on&nbsp;the&nbsp;now&nbsp;famous&nbsp;Unet<br>
&nbsp;&nbsp;&nbsp;&nbsp;network&nbsp;that&nbsp;was&nbsp;first&nbsp;proposed&nbsp;by&nbsp;Ronneberger,&nbsp;Fischer&nbsp;and&nbsp;Brox&nbsp;in&nbsp;the&nbsp;paper<br>
&nbsp;&nbsp;&nbsp;&nbsp;"U-Net:&nbsp;Convolutional&nbsp;Networks&nbsp;for&nbsp;Biomedical&nbsp;Image&nbsp;Segmentation".&nbsp;&nbsp;Their&nbsp;UNET<br>
&nbsp;&nbsp;&nbsp;&nbsp;extracts&nbsp;binary&nbsp;masks&nbsp;for&nbsp;the&nbsp;cell&nbsp;pixel&nbsp;blobs&nbsp;of&nbsp;interest&nbsp;in&nbsp;biomedical&nbsp;images.<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;output&nbsp;of&nbsp;UNET&nbsp;can&nbsp;therefore&nbsp;be&nbsp;treated&nbsp;as&nbsp;a&nbsp;pixel-wise&nbsp;binary&nbsp;classifier&nbsp;at<br>
&nbsp;&nbsp;&nbsp;&nbsp;each&nbsp;pixel&nbsp;position.&nbsp;&nbsp;The&nbsp;mUNet&nbsp;class,&nbsp;on&nbsp;the&nbsp;other&nbsp;hand,&nbsp;is&nbsp;intended&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;segmenting&nbsp;out&nbsp;multiple&nbsp;objects&nbsp;simultaneously&nbsp;form&nbsp;an&nbsp;image.&nbsp;[A&nbsp;weaker&nbsp;reason<br>
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;"m"&nbsp;in&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;class&nbsp;is&nbsp;that&nbsp;it&nbsp;uses&nbsp;skip&nbsp;connections&nbsp;in&nbsp;multiple<br>
&nbsp;&nbsp;&nbsp;&nbsp;ways&nbsp;---&nbsp;such&nbsp;connections&nbsp;are&nbsp;used&nbsp;not&nbsp;only&nbsp;across&nbsp;the&nbsp;two&nbsp;arms&nbsp;of&nbsp;the&nbsp;"U",&nbsp;but<br>
&nbsp;&nbsp;&nbsp;&nbsp;also&nbsp;also&nbsp;along&nbsp;the&nbsp;arms.&nbsp;&nbsp;The&nbsp;skip&nbsp;connections&nbsp;in&nbsp;the&nbsp;original&nbsp;Unet&nbsp;are&nbsp;only<br>
&nbsp;&nbsp;&nbsp;&nbsp;between&nbsp;the&nbsp;two&nbsp;arms&nbsp;of&nbsp;the&nbsp;U.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;mUNet&nbsp;works&nbsp;by&nbsp;assigning&nbsp;a&nbsp;separate&nbsp;channel&nbsp;in&nbsp;the&nbsp;output&nbsp;of&nbsp;the&nbsp;network&nbsp;to&nbsp;each<br>
&nbsp;&nbsp;&nbsp;&nbsp;different&nbsp;object&nbsp;type.&nbsp;&nbsp;After&nbsp;the&nbsp;network&nbsp;is&nbsp;trained,&nbsp;for&nbsp;a&nbsp;given&nbsp;input&nbsp;image,<br>
&nbsp;&nbsp;&nbsp;&nbsp;all&nbsp;you&nbsp;have&nbsp;to&nbsp;do&nbsp;is&nbsp;examine&nbsp;the&nbsp;different&nbsp;channels&nbsp;of&nbsp;the&nbsp;output&nbsp;for&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;presence&nbsp;or&nbsp;the&nbsp;absence&nbsp;of&nbsp;the&nbsp;objects&nbsp;corresponding&nbsp;to&nbsp;the&nbsp;channel&nbsp;index.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;version&nbsp;of&nbsp;DLStudio&nbsp;also&nbsp;comes&nbsp;with&nbsp;a&nbsp;new&nbsp;dataset,<br>
&nbsp;&nbsp;&nbsp;&nbsp;PurdueShapes5MultiObject,&nbsp;for&nbsp;experimenting&nbsp;with&nbsp;mUNet.&nbsp;&nbsp;Each&nbsp;image&nbsp;in&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;dataset&nbsp;contains&nbsp;a&nbsp;random&nbsp;number&nbsp;of&nbsp;selections&nbsp;from&nbsp;five&nbsp;different&nbsp;shapes,<br>
&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;the&nbsp;shapes&nbsp;being&nbsp;randomly&nbsp;scaled,&nbsp;oriented,&nbsp;and&nbsp;located&nbsp;in&nbsp;each&nbsp;image.<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;five&nbsp;different&nbsp;shapes&nbsp;are:&nbsp;rectangle,&nbsp;triangle,&nbsp;disk,&nbsp;oval,&nbsp;and&nbsp;star.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Your&nbsp;starting&nbsp;point&nbsp;for&nbsp;learning&nbsp;how&nbsp;to&nbsp;use&nbsp;the&nbsp;mUNet&nbsp;network&nbsp;for&nbsp;segmenting<br>
&nbsp;&nbsp;&nbsp;&nbsp;images&nbsp;should&nbsp;be&nbsp;the&nbsp;following&nbsp;script&nbsp;in&nbsp;the&nbsp;Examples&nbsp;directory&nbsp;of&nbsp;the&nbsp;distro:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;semantic_segmentation.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Execute&nbsp;the&nbsp;script&nbsp;and&nbsp;understand&nbsp;how&nbsp;it&nbsp;uses&nbsp;the&nbsp;functionality&nbsp;packed&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;inner&nbsp;class&nbsp;SemanticSegmentation&nbsp;for&nbsp;segmenting&nbsp;out&nbsp;the&nbsp;objects&nbsp;in&nbsp;an&nbsp;image.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:blue; font-size:100%"><strong>&nbsp;&nbsp;    VARIATIONAL AUTO-ENCODNG </strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Starting&nbsp;with&nbsp;Version&nbsp;2.5.1,&nbsp;you&nbsp;can&nbsp;experiment&nbsp;with&nbsp;generative&nbsp;autoencoding&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;DLStudio&nbsp;platform.&nbsp;&nbsp;The&nbsp;inner&nbsp;class&nbsp;of&nbsp;DLStudio&nbsp;that&nbsp;allows&nbsp;you&nbsp;to&nbsp;do&nbsp;that&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;called&nbsp;VAE&nbsp;(for&nbsp;Variational&nbsp;Auto-Encoding).&nbsp;&nbsp;Generative&nbsp;data&nbsp;modeling&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;variational&nbsp;autoencoding&nbsp;is&nbsp;based&nbsp;on&nbsp;the&nbsp;assumption&nbsp;that&nbsp;exists&nbsp;a&nbsp;relatively<br>
&nbsp;&nbsp;&nbsp;&nbsp;simple&nbsp;Latent&nbsp;Space&nbsp;that&nbsp;captures&nbsp;the&nbsp;essence&nbsp;of&nbsp;what's&nbsp;in&nbsp;the&nbsp;images&nbsp;you&nbsp;are<br>
&nbsp;&nbsp;&nbsp;&nbsp;interested&nbsp;in.&nbsp;&nbsp;That&nbsp;is,&nbsp;it&nbsp;is&nbsp;possible&nbsp;to&nbsp;map&nbsp;any&nbsp;given&nbsp;input&nbsp;image&nbsp;to&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;low-dimensional&nbsp;(relatively&nbsp;speaking)&nbsp;vector&nbsp;z&nbsp;that&nbsp;can&nbsp;be&nbsp;modeled&nbsp;with&nbsp;a&nbsp;simple<br>
&nbsp;&nbsp;&nbsp;&nbsp;probabilty&nbsp;distribution&nbsp;(which,&nbsp;ideally&nbsp;speaking,&nbsp;would&nbsp;be&nbsp;a&nbsp;zero-mean,<br>
&nbsp;&nbsp;&nbsp;&nbsp;unit-covariance&nbsp;Gaussian)&nbsp;that&nbsp;could&nbsp;subseqently&nbsp;be&nbsp;used&nbsp;to&nbsp;generate&nbsp;useful<br>
&nbsp;&nbsp;&nbsp;&nbsp;variants&nbsp;of&nbsp;the&nbsp;input&nbsp;image.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;great&nbsp;thing&nbsp;about&nbsp;VAE&nbsp;is&nbsp;that&nbsp;it&nbsp;allows&nbsp;you&nbsp;to&nbsp;carry&nbsp;out&nbsp;what's&nbsp;known&nbsp;as<br>
&nbsp;&nbsp;&nbsp;&nbsp;disentanglement&nbsp;learning&nbsp;in&nbsp;which&nbsp;the&nbsp;learned&nbsp;latent&nbsp;space&nbsp;captures&nbsp;the&nbsp;essence<br>
&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;each&nbsp;image&nbsp;in&nbsp;your&nbsp;training&nbsp;dataset&nbsp;and&nbsp;have&nbsp;the&nbsp;Decoder&nbsp;capture&nbsp;the&nbsp;rest.<br>
&nbsp;&nbsp;&nbsp;&nbsp;(The&nbsp;Encoder's&nbsp;job&nbsp;is&nbsp;to&nbsp;map&nbsp;the&nbsp;input&nbsp;image&nbsp;to&nbsp;its&nbsp;representation&nbsp;in&nbsp;the&nbsp;Latent<br>
&nbsp;&nbsp;&nbsp;&nbsp;Space.)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;I&nbsp;have&nbsp;implemented&nbsp;variational&nbsp;autoencoding&nbsp;in&nbsp;DLStudio&nbsp;through&nbsp;a&nbsp;base&nbsp;class<br>
&nbsp;&nbsp;&nbsp;&nbsp;Autoencoder&nbsp;and&nbsp;a&nbsp;derived&nbsp;class&nbsp;VAE.&nbsp;Both&nbsp;of&nbsp;these&nbsp;are&nbsp;inner&nbsp;classes&nbsp;of&nbsp;the&nbsp;main<br>
&nbsp;&nbsp;&nbsp;&nbsp;DLStudio&nbsp;class.&nbsp;(That&nbsp;is,&nbsp;both&nbsp;these&nbsp;classea&nbsp;are&nbsp;defined&nbsp;in&nbsp;the&nbsp;file<br>
&nbsp;&nbsp;&nbsp;&nbsp;"DLStudio.py"&nbsp;in&nbsp;the&nbsp;distribution.)&nbsp;It&nbsp;is&nbsp;the&nbsp;Encoder&nbsp;and&nbsp;the&nbsp;Decoder&nbsp;in&nbsp;the&nbsp;base<br>
&nbsp;&nbsp;&nbsp;&nbsp;class&nbsp;Autoencoder&nbsp;that&nbsp;does&nbsp;the&nbsp;bulk&nbsp;of&nbsp;computing&nbsp;in&nbsp;VAE.&nbsp;What&nbsp;the&nbsp;VAE&nbsp;class&nbsp;does<br>
&nbsp;&nbsp;&nbsp;&nbsp;specifically&nbsp;is&nbsp;to&nbsp;feed&nbsp;the&nbsp;output&nbsp;of&nbsp;Autoencoder's&nbsp;Encoder&nbsp;into&nbsp;two&nbsp;nn.Linear<br>
&nbsp;&nbsp;&nbsp;&nbsp;layers&nbsp;for&nbsp;the&nbsp;learning&nbsp;of&nbsp;the&nbsp;mean&nbsp;and&nbsp;the&nbsp;log-variance&nbsp;of&nbsp;the&nbsp;latent<br>
&nbsp;&nbsp;&nbsp;&nbsp;distribution&nbsp;for&nbsp;the&nbsp;training&nbsp;dataset.&nbsp;&nbsp;Regarding&nbsp;decoding,&nbsp;VAE's&nbsp;Decoder&nbsp;invokes<br>
&nbsp;&nbsp;&nbsp;&nbsp;what's&nbsp;known&nbsp;as&nbsp;the&nbsp;"reparameterization&nbsp;trick"&nbsp;for&nbsp;sampling&nbsp;the&nbsp;latent<br>
&nbsp;&nbsp;&nbsp;&nbsp;distribution&nbsp;to&nbsp;first&nbsp;construct&nbsp;a&nbsp;sample&nbsp;from&nbsp;the&nbsp;latent&nbsp;space,&nbsp;reshape&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;sample&nbsp;appropriately,&nbsp;and&nbsp;to&nbsp;then&nbsp;feed&nbsp;it&nbsp;into&nbsp;Autoencoder's&nbsp;Decoder.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;you&nbsp;want&nbsp;to&nbsp;experiment&nbsp;with&nbsp;autoencoding&nbsp;and&nbsp;variational&nbsp;autoencoding&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;DLStudio,&nbsp;your&nbsp;staring&nbsp;points&nbsp;should&nbsp;be&nbsp;the&nbsp;following&nbsp;three&nbsp;scripts&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;Examples&nbsp;directory&nbsp;of&nbsp;the&nbsp;distribution:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;run_autoencoder.py<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;run_vae.py<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;run_vae_for_image_generation.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;first&nbsp;script,&nbsp;run_autoencoder.py,&nbsp;is&nbsp;for&nbsp;experimenting&nbsp;with&nbsp;just&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;Autoencoder&nbsp;class&nbsp;by&nbsp;itself.&nbsp;&nbsp;For&nbsp;example,&nbsp;if&nbsp;you&nbsp;wanted&nbsp;to&nbsp;experiment&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;dimensionality&nbsp;reduction&nbsp;with&nbsp;an&nbsp;Autoencoder,&nbsp;all&nbsp;you&nbsp;would&nbsp;need&nbsp;to&nbsp;do&nbsp;would&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;change&nbsp;the&nbsp;last&nbsp;or&nbsp;the&nbsp;last&nbsp;couple&nbsp;of&nbsp;layers&nbsp;of&nbsp;the&nbsp;Decoder&nbsp;in&nbsp;the&nbsp;Autoencoder<br>
&nbsp;&nbsp;&nbsp;&nbsp;class&nbsp;and&nbsp;see&nbsp;for&nbsp;yourself&nbsp;the&nbsp;results&nbsp;by&nbsp;running&nbsp;this&nbsp;script.&nbsp;&nbsp;The&nbsp;second<br>
&nbsp;&nbsp;&nbsp;&nbsp;script,&nbsp;run_vae.py,&nbsp;is&nbsp;for&nbsp;experimenting&nbsp;with&nbsp;the&nbsp;variational&nbsp;autoencoding&nbsp;code<br>
&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;the&nbsp;VAE&nbsp;class.&nbsp;And&nbsp;the&nbsp;last&nbsp;script,&nbsp;run_vae_for_image_generation.py,&nbsp;is&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;experimenting&nbsp;with&nbsp;just&nbsp;the&nbsp;Decoder&nbsp;part&nbsp;of&nbsp;the&nbsp;VAE&nbsp;class.&nbsp;&nbsp;The&nbsp;VAE&nbsp;Decoder&nbsp;is&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;reality&nbsp;a&nbsp;Generator&nbsp;that&nbsp;samples&nbsp;a&nbsp;Gaussian&nbsp;probability&nbsp;distribution,&nbsp;as<br>
&nbsp;&nbsp;&nbsp;&nbsp;specified&nbsp;by&nbsp;its&nbsp;mean&nbsp;and&nbsp;covariance,&nbsp;and&nbsp;transforms&nbsp;the&nbsp;sample&nbsp;back&nbsp;into&nbsp;an<br>
&nbsp;&nbsp;&nbsp;&nbsp;image&nbsp;that,&nbsp;ideally&nbsp;speaking,&nbsp;should&nbsp;belong&nbsp;to&nbsp;the&nbsp;same&nbsp;distribution&nbsp;as&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;images&nbsp;used&nbsp;for&nbsp;training&nbsp;the&nbsp;VAE.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:blue; font-size:100%"><strong>&nbsp;&nbsp;    VAE WITH CODEBOOK LEARNING </strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;development&nbsp;of&nbsp;VQVAE&nbsp;brought&nbsp;us&nbsp;what's&nbsp;now&nbsp;known&nbsp;as&nbsp;the&nbsp;Codebook&nbsp;Learning&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;Deep&nbsp;Learning&nbsp;that&nbsp;eventually&nbsp;placed&nbsp;languages&nbsp;and&nbsp;images&nbsp;on&nbsp;an&nbsp;equal&nbsp;footing<br>
&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;regard&nbsp;to&nbsp;attention-based&nbsp;processing&nbsp;with&nbsp;transformers.&nbsp;&nbsp;The&nbsp;prefix&nbsp;"VQ"<br>
&nbsp;&nbsp;&nbsp;&nbsp;stands&nbsp;for&nbsp;"Vector&nbsp;Quantization"&nbsp;that&nbsp;replaces&nbsp;each&nbsp;vector&nbsp;produced&nbsp;by&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;Encoder&nbsp;(the&nbsp;values&nbsp;in&nbsp;the&nbsp;vector&nbsp;lie&nbsp;along&nbsp;the&nbsp;channel&nbsp;dimension&nbsp;at&nbsp;the&nbsp;output<br>
&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;an&nbsp;Encoder)&nbsp;by&nbsp;the&nbsp;closest&nbsp;similar&nbsp;vector&nbsp;in&nbsp;a&nbsp;learned&nbsp;codebook.&nbsp;&nbsp;This&nbsp;act<br>
&nbsp;&nbsp;&nbsp;&nbsp;suppresses&nbsp;noise&nbsp;and&nbsp;other&nbsp;irrelevant&nbsp;variations&nbsp;at&nbsp;the&nbsp;Encoder&nbsp;output.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;VQVAE&nbsp;does&nbsp;three&nbsp;things:&nbsp;(1)&nbsp;If&nbsp;the&nbsp;output&nbsp;of&nbsp;the&nbsp;Encoder&nbsp;is&nbsp;of&nbsp;shape&nbsp;NxNxC,&nbsp;it<br>
&nbsp;&nbsp;&nbsp;&nbsp;is&nbsp;re-imagined&nbsp;as&nbsp;representing&nbsp;the&nbsp;input&nbsp;image&nbsp;with&nbsp;N^2&nbsp;embedding&nbsp;vectors,&nbsp;each<br>
&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;dimensionality&nbsp;C.&nbsp;(2)&nbsp;You&nbsp;declare&nbsp;a&nbsp;Codebook&nbsp;consisting&nbsp;of&nbsp;K&nbsp;vectors,&nbsp;each&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;dimensionality,&nbsp;again,&nbsp;C.&nbsp;And,&nbsp;finally,&nbsp;(3)&nbsp;You&nbsp;replace&nbsp;each&nbsp;embedding&nbsp;vector&nbsp;at<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;output&nbsp;of&nbsp;the&nbsp;Encoder&nbsp;with&nbsp;its&nbsp;closest&nbsp;Codebook&nbsp;vector.&nbsp;Finally,&nbsp;you&nbsp;feed&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;"reconstituted"&nbsp;embedding&nbsp;vectors&nbsp;into&nbsp;the&nbsp;Decoder&nbsp;to&nbsp;recover&nbsp;an&nbsp;image&nbsp;(or&nbsp;its<br>
&nbsp;&nbsp;&nbsp;&nbsp;desired&nbsp;variants&nbsp;in&nbsp;a&nbsp;conditional&nbsp;implementation).<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;VQGAN&nbsp;goes&nbsp;one&nbsp;step&nbsp;further&nbsp;by&nbsp;showing&nbsp;that&nbsp;the&nbsp;Codebook&nbsp;vectors&nbsp;in&nbsp;a&nbsp;VQVAE&nbsp;can<br>
&nbsp;&nbsp;&nbsp;&nbsp;be&nbsp;treated&nbsp;like&nbsp;the&nbsp;embedding&nbsp;vectors&nbsp;associated&nbsp;with&nbsp;the&nbsp;tokens&nbsp;in&nbsp;transformer-<br>
&nbsp;&nbsp;&nbsp;&nbsp;based&nbsp;neural&nbsp;architectures&nbsp;for&nbsp;language&nbsp;processing.&nbsp;&nbsp;The&nbsp;token&nbsp;sequences&nbsp;for&nbsp;such<br>
&nbsp;&nbsp;&nbsp;&nbsp;transformer&nbsp;based&nbsp;processing&nbsp;would&nbsp;consist&nbsp;of&nbsp;the&nbsp;integer&nbsp;indices&nbsp;associated&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;Codebook&nbsp;vectors&nbsp;that&nbsp;an&nbsp;input&nbsp;image&nbsp;is&nbsp;mapped&nbsp;to.&nbsp;The&nbsp;original&nbsp;authors&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;VQGAN&nbsp;have&nbsp;demonstrated&nbsp;that&nbsp;such&nbsp;networks&nbsp;can&nbsp;generate&nbsp;virtual&nbsp;image&nbsp;variants&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;training&nbsp;images,&nbsp;with&nbsp;the&nbsp;virtual&nbsp;images&nbsp;being&nbsp;larger&nbsp;in&nbsp;size&nbsp;and&nbsp;of&nbsp;greater<br>
&nbsp;&nbsp;&nbsp;&nbsp;visual&nbsp;complexity&nbsp;than&nbsp;was&nbsp;possible&nbsp;before.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;To&nbsp;learn&nbsp;these&nbsp;ideas&nbsp;through&nbsp;DLStudio,&nbsp;please&nbsp;note&nbsp;the&nbsp;following&nbsp;scripts&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;Examples&nbsp;directory&nbsp;of&nbsp;the&nbsp;distribution:&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;run_vqvae.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;run_vqgan.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;run_vqgan_map_image_to_codebook.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;run_vqgan_transformer.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;script&nbsp;"run_vqvae.py"&nbsp;stands&nbsp;on&nbsp;its&nbsp;own&nbsp;and&nbsp;try&nbsp;to&nbsp;play&nbsp;with&nbsp;it&nbsp;in&nbsp;order&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;learn&nbsp;the&nbsp;basics&nbsp;of&nbsp;codebook&nbsp;learning.&nbsp;&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;following&nbsp;two&nbsp;scripts&nbsp;named&nbsp;above:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;run_vqgan.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;run_vqgan_transformer.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;two&nbsp;parts&nbsp;of&nbsp;what&nbsp;it&nbsp;takes&nbsp;to&nbsp;accomplish&nbsp;the&nbsp;following:&nbsp;autoregressive<br>
&nbsp;&nbsp;&nbsp;&nbsp;modeling&nbsp;of&nbsp;the&nbsp;images&nbsp;through&nbsp;the&nbsp;tokens&nbsp;that&nbsp;correspond&nbsp;to&nbsp;the&nbsp;vectors&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;learned&nbsp;Codebook.&nbsp;&nbsp;You&nbsp;first&nbsp;work&nbsp;with&nbsp;"run_vqgan.py"&nbsp;to&nbsp;train&nbsp;the&nbsp;Encoder,&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;Decoder,&nbsp;and&nbsp;the&nbsp;VectorQuantizer&nbsp;networks.&nbsp;&nbsp;Subsequently,&nbsp;using&nbsp;the&nbsp;components<br>
&nbsp;&nbsp;&nbsp;&nbsp;trained&nbsp;by&nbsp;"run_vqgan.py",&nbsp;you&nbsp;work&nbsp;with&nbsp;"run_vqgan_transformer.py"&nbsp;in&nbsp;order&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;train&nbsp;a&nbsp;transformer&nbsp;network&nbsp;for&nbsp;autoregressive&nbsp;modeling&nbsp;of&nbsp;the&nbsp;images.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;On&nbsp;the&nbsp;four&nbsp;scripts&nbsp;named&nbsp;above,&nbsp;that&nbsp;leaves<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;run_vqgan_map_image_to_codebook.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;is&nbsp;specifically&nbsp;for&nbsp;you&nbsp;to&nbsp;acquire&nbsp;deeper&nbsp;intuitions&nbsp;about&nbsp;what&nbsp;exactly&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;images&nbsp;is&nbsp;represented&nbsp;by&nbsp;the&nbsp;Codebook&nbsp;vectors.&nbsp;&nbsp;In&nbsp;order&nbsp;to&nbsp;play&nbsp;with&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;script,&nbsp;you&nbsp;MUST&nbsp;first&nbsp;run&nbsp;the&nbsp;script&nbsp;"run_vqgan.py"&nbsp;for&nbsp;training&nbsp;the&nbsp;VQGAN<br>
&nbsp;&nbsp;&nbsp;&nbsp;network.&nbsp;&nbsp;Subsequently,&nbsp;playing&nbsp;with&nbsp;"run_vqgan_map_image_to_codebook.py"&nbsp;will<br>
&nbsp;&nbsp;&nbsp;&nbsp;help&nbsp;you&nbsp;better&nbsp;understand&nbsp;the&nbsp;role&nbsp;played&nbsp;by&nbsp;the&nbsp;Codebook&nbsp;vectors&nbsp;in&nbsp;the&nbsp;output<br>
&nbsp;&nbsp;&nbsp;&nbsp;produced&nbsp;by&nbsp;the&nbsp;Decoder.&nbsp;&nbsp;For&nbsp;additional&nbsp;information&nbsp;related&nbsp;to&nbsp;this&nbsp;script,&nbsp;see<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;comment&nbsp;block&nbsp;at&nbsp;the&nbsp;top&nbsp;of&nbsp;the&nbsp;file&nbsp;in&nbsp;the&nbsp;Examples&nbsp;directory&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;distribution.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:blue; font-size:100%"><strong>&nbsp;&nbsp;<a id="107">    TEXT CLASSIFICATION</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Starting&nbsp;with&nbsp;Version&nbsp;1.1.2,&nbsp;the&nbsp;module&nbsp;includes&nbsp;an&nbsp;inner&nbsp;class<br>
&nbsp;&nbsp;&nbsp;&nbsp;TextClassification&nbsp;that&nbsp;allows&nbsp;you&nbsp;to&nbsp;do&nbsp;simple&nbsp;experiments&nbsp;with&nbsp;neural<br>
&nbsp;&nbsp;&nbsp;&nbsp;networks&nbsp;with&nbsp;feedback&nbsp;(that&nbsp;are&nbsp;also&nbsp;called&nbsp;Recurrent&nbsp;Neural&nbsp;Networks).&nbsp;&nbsp;With<br>
&nbsp;&nbsp;&nbsp;&nbsp;an&nbsp;RNN,&nbsp;textual&nbsp;data&nbsp;of&nbsp;arbitrary&nbsp;length&nbsp;can&nbsp;be&nbsp;characterized&nbsp;with&nbsp;a&nbsp;hidden<br>
&nbsp;&nbsp;&nbsp;&nbsp;state&nbsp;vector&nbsp;of&nbsp;a&nbsp;fixed&nbsp;size.&nbsp;&nbsp;To&nbsp;facilitate&nbsp;text&nbsp;based&nbsp;experiments,&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;module&nbsp;also&nbsp;comes&nbsp;with&nbsp;text&nbsp;datasets&nbsp;derived&nbsp;from&nbsp;an&nbsp;old&nbsp;Amazon&nbsp;archive&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;product&nbsp;reviews.&nbsp;&nbsp;Further&nbsp;information&nbsp;regarding&nbsp;the&nbsp;datasets&nbsp;is&nbsp;in&nbsp;the&nbsp;comment<br>
&nbsp;&nbsp;&nbsp;&nbsp;block&nbsp;associated&nbsp;with&nbsp;the&nbsp;class&nbsp;SentimentAnalysisDataset.&nbsp;If&nbsp;you&nbsp;want&nbsp;to&nbsp;use<br>
&nbsp;&nbsp;&nbsp;&nbsp;DLStudio&nbsp;for&nbsp;experimenting&nbsp;with&nbsp;text,&nbsp;your&nbsp;starting&nbsp;points&nbsp;should&nbsp;be&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;following&nbsp;three&nbsp;scripts&nbsp;in&nbsp;the&nbsp;Examples&nbsp;directory&nbsp;of&nbsp;the&nbsp;distribution:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text_classification_with_TEXTnet.py<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text_classification_with_TEXTnetOrder2.py<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text_classification_with_GRU.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;first&nbsp;of&nbsp;these&nbsp;is&nbsp;meant&nbsp;to&nbsp;be&nbsp;used&nbsp;with&nbsp;the&nbsp;TEXTnet&nbsp;network&nbsp;that&nbsp;does&nbsp;not<br>
&nbsp;&nbsp;&nbsp;&nbsp;include&nbsp;any&nbsp;protection&nbsp;against&nbsp;the&nbsp;vanishing&nbsp;gradients&nbsp;problem&nbsp;that&nbsp;a&nbsp;poorly<br>
&nbsp;&nbsp;&nbsp;&nbsp;designed&nbsp;RNN&nbsp;can&nbsp;suffer&nbsp;from.&nbsp;&nbsp;The&nbsp;second&nbsp;script&nbsp;mentioned&nbsp;above&nbsp;is&nbsp;based&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;TEXTnetOrder2&nbsp;network&nbsp;and&nbsp;it&nbsp;includes&nbsp;rudimentary&nbsp;protection,&nbsp;but&nbsp;not<br>
&nbsp;&nbsp;&nbsp;&nbsp;enough&nbsp;to&nbsp;suffice&nbsp;for&nbsp;any&nbsp;practical&nbsp;application.&nbsp;&nbsp;The&nbsp;purpose&nbsp;of&nbsp;TEXTnetOrder2<br>
&nbsp;&nbsp;&nbsp;&nbsp;is&nbsp;to&nbsp;serve&nbsp;as&nbsp;an&nbsp;educational&nbsp;stepping&nbsp;stone&nbsp;to&nbsp;a&nbsp;GRU&nbsp;(Gated&nbsp;Recurrent&nbsp;Unit)<br>
&nbsp;&nbsp;&nbsp;&nbsp;network&nbsp;that&nbsp;is&nbsp;used&nbsp;in&nbsp;the&nbsp;third&nbsp;script&nbsp;listed&nbsp;above.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Starting&nbsp;with&nbsp;Version&nbsp;2.0.8,&nbsp;the&nbsp;Examples&nbsp;directory&nbsp;of&nbsp;DLStudio&nbsp;also&nbsp;includes<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;following&nbsp;three&nbsp;scripts&nbsp;that&nbsp;use&nbsp;the&nbsp;same&nbsp;learning&nbsp;networks&nbsp;as&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;corresponding&nbsp;scripts&nbsp;mentioned&nbsp;above&nbsp;but&nbsp;with&nbsp;word&nbsp;representations&nbsp;based&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;word2vec&nbsp;embeddings:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text_classification_with_TEXTnet_word2vec.py<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text_classification_with_TEXTnetOrder2_word2vec.py<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text_classification_with_GRU_word2vec.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;pre-trained&nbsp;word2vec&nbsp;embeddings&nbsp;used&nbsp;in&nbsp;these&nbsp;scripts&nbsp;are&nbsp;accessed<br>
&nbsp;&nbsp;&nbsp;&nbsp;through&nbsp;the&nbsp;popular&nbsp;gensim&nbsp;library.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:blue; font-size:100%"><strong>&nbsp;&nbsp;<a id="108">    DATA MODELING WITH ADVERSARIAL LEARNING</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Starting&nbsp;with&nbsp;version&nbsp;2.0.3,&nbsp;DLStudio&nbsp;includes&nbsp;a&nbsp;separate&nbsp;module&nbsp;named<br>
&nbsp;&nbsp;&nbsp;&nbsp;AdversarialLearning&nbsp;for&nbsp;experimenting&nbsp;with&nbsp;different&nbsp;adversarial&nbsp;learning<br>
&nbsp;&nbsp;&nbsp;&nbsp;approaches&nbsp;for&nbsp;data&nbsp;modeling.&nbsp;&nbsp;Adversarial&nbsp;Learning&nbsp;consists&nbsp;of&nbsp;simultaneously<br>
&nbsp;&nbsp;&nbsp;&nbsp;training&nbsp;a&nbsp;Generator&nbsp;and&nbsp;a&nbsp;Discriminator&nbsp;(or,&nbsp;a&nbsp;Generator&nbsp;and&nbsp;a&nbsp;Critic)&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;goal&nbsp;of&nbsp;getting&nbsp;the&nbsp;Generator&nbsp;to&nbsp;produce&nbsp;from&nbsp;pure&nbsp;noise&nbsp;images&nbsp;that&nbsp;look<br>
&nbsp;&nbsp;&nbsp;&nbsp;like&nbsp;those&nbsp;in&nbsp;the&nbsp;training&nbsp;dataset.&nbsp;&nbsp;When&nbsp;Generator-Discriminator&nbsp;pairs&nbsp;are<br>
&nbsp;&nbsp;&nbsp;&nbsp;used,&nbsp;the&nbsp;Discriminator's&nbsp;job&nbsp;is&nbsp;to&nbsp;become&nbsp;an&nbsp;expert&nbsp;at&nbsp;recognizing&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;training&nbsp;images&nbsp;so&nbsp;it&nbsp;can&nbsp;let&nbsp;us&nbsp;know&nbsp;when&nbsp;the&nbsp;generator&nbsp;produces&nbsp;an&nbsp;image<br>
&nbsp;&nbsp;&nbsp;&nbsp;that&nbsp;does&nbsp;not&nbsp;look&nbsp;like&nbsp;what&nbsp;is&nbsp;in&nbsp;the&nbsp;training&nbsp;dataset.&nbsp;&nbsp;The&nbsp;output&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;Discriminator&nbsp;consists&nbsp;of&nbsp;the&nbsp;probability&nbsp;that&nbsp;the&nbsp;input&nbsp;to&nbsp;the&nbsp;discriminator<br>
&nbsp;&nbsp;&nbsp;&nbsp;is&nbsp;like&nbsp;one&nbsp;of&nbsp;the&nbsp;training&nbsp;images.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;On&nbsp;the&nbsp;other&nbsp;hand,&nbsp;when&nbsp;a&nbsp;Generator-Critic&nbsp;pair&nbsp;is&nbsp;used,&nbsp;the&nbsp;Critic's&nbsp;job&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;become&nbsp;adept&nbsp;at&nbsp;estimating&nbsp;the&nbsp;distance&nbsp;between&nbsp;the&nbsp;distribution&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;corresponds&nbsp;to&nbsp;the&nbsp;training&nbsp;dataset&nbsp;and&nbsp;the&nbsp;distribution&nbsp;that&nbsp;has&nbsp;been&nbsp;learned<br>
&nbsp;&nbsp;&nbsp;&nbsp;by&nbsp;the&nbsp;Generator&nbsp;so&nbsp;far.&nbsp;&nbsp;If&nbsp;the&nbsp;distance&nbsp;between&nbsp;the&nbsp;distributions&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;differentiable&nbsp;with&nbsp;respect&nbsp;to&nbsp;the&nbsp;weights&nbsp;in&nbsp;the&nbsp;networks,&nbsp;one&nbsp;can&nbsp;backprop<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;distance&nbsp;and&nbsp;update&nbsp;the&nbsp;weights&nbsp;in&nbsp;an&nbsp;iterative&nbsp;training&nbsp;loop.&nbsp;&nbsp;This&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;roughly&nbsp;the&nbsp;idea&nbsp;of&nbsp;the&nbsp;Wasserstein&nbsp;GAN&nbsp;that&nbsp;is&nbsp;incorporated&nbsp;as&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;Critic-Generator&nbsp;pair&nbsp;CG1&nbsp;in&nbsp;the&nbsp;AdversarialLearning&nbsp;class.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;AdversarialLearning&nbsp;class&nbsp;includes&nbsp;two&nbsp;kinds&nbsp;of&nbsp;adversarial&nbsp;networks&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;data&nbsp;modeling:&nbsp;DCGAN&nbsp;and&nbsp;WGAN.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;DCGAN&nbsp;is&nbsp;short&nbsp;for&nbsp;"Deep&nbsp;Convolutional&nbsp;Generative&nbsp;Adversarial&nbsp;Network",&nbsp;owes<br>
&nbsp;&nbsp;&nbsp;&nbsp;its&nbsp;origins&nbsp;to&nbsp;the&nbsp;paper&nbsp;"Unsupervised&nbsp;Representation&nbsp;Learning&nbsp;with&nbsp;Deep<br>
&nbsp;&nbsp;&nbsp;&nbsp;Convolutional&nbsp;Generative&nbsp;Adversarial&nbsp;Networks"&nbsp;by&nbsp;Radford&nbsp;et&nbsp;al.&nbsp;&nbsp;DCGAN&nbsp;was<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;first&nbsp;fully&nbsp;convolutional&nbsp;network&nbsp;for&nbsp;GANs&nbsp;(Generative&nbsp;Adversarial<br>
&nbsp;&nbsp;&nbsp;&nbsp;Network).&nbsp;CNN's&nbsp;typically&nbsp;have&nbsp;a&nbsp;fully-connected&nbsp;layer&nbsp;(an&nbsp;instance&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;nn.Linear)&nbsp;at&nbsp;the&nbsp;topmost&nbsp;level.&nbsp;&nbsp;For&nbsp;the&nbsp;topmost&nbsp;layer&nbsp;in&nbsp;the&nbsp;Generator<br>
&nbsp;&nbsp;&nbsp;&nbsp;network,&nbsp;DCGAN&nbsp;uses&nbsp;another&nbsp;convolution&nbsp;layer&nbsp;that&nbsp;produces&nbsp;the&nbsp;final&nbsp;output<br>
&nbsp;&nbsp;&nbsp;&nbsp;image.&nbsp;&nbsp;And&nbsp;for&nbsp;the&nbsp;topmost&nbsp;layer&nbsp;of&nbsp;the&nbsp;Discriminator,&nbsp;DCGAN&nbsp;flattens&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;output&nbsp;and&nbsp;feeds&nbsp;that&nbsp;into&nbsp;a&nbsp;sigmoid&nbsp;function&nbsp;for&nbsp;producing&nbsp;scalar&nbsp;value.<br>
&nbsp;&nbsp;&nbsp;&nbsp;Additionally,&nbsp;DCGAN&nbsp;also&nbsp;gets&nbsp;rid&nbsp;of&nbsp;max-pooling&nbsp;for&nbsp;downsampling&nbsp;and&nbsp;instead<br>
&nbsp;&nbsp;&nbsp;&nbsp;uses&nbsp;convolutions&nbsp;with&nbsp;strides.&nbsp;&nbsp;Yet&nbsp;another&nbsp;feature&nbsp;of&nbsp;a&nbsp;DCGAN&nbsp;is&nbsp;the&nbsp;use&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;batch&nbsp;normalization&nbsp;in&nbsp;all&nbsp;layers,&nbsp;except&nbsp;in&nbsp;the&nbsp;output&nbsp;layer&nbsp;of&nbsp;the&nbsp;Generator<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;the&nbsp;input&nbsp;layer&nbsp;of&nbsp;the&nbsp;Discriminator.&nbsp;&nbsp;As&nbsp;the&nbsp;authors&nbsp;of&nbsp;DCGAN&nbsp;stated,<br>
&nbsp;&nbsp;&nbsp;&nbsp;while,&nbsp;in&nbsp;general,&nbsp;batch&nbsp;normalization&nbsp;stabilizes&nbsp;learning&nbsp;by&nbsp;normalizing&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;input&nbsp;to&nbsp;each&nbsp;layer&nbsp;to&nbsp;have&nbsp;zero&nbsp;mean&nbsp;and&nbsp;unit&nbsp;variance,&nbsp;applying&nbsp;BN&nbsp;at&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;output&nbsp;results&nbsp;in&nbsp;sample&nbsp;oscillation&nbsp;and&nbsp;model&nbsp;instability.&nbsp;&nbsp;I&nbsp;have&nbsp;also<br>
&nbsp;&nbsp;&nbsp;&nbsp;retained&nbsp;in&nbsp;the&nbsp;DCGAN&nbsp;code&nbsp;the&nbsp;leaky&nbsp;ReLU&nbsp;activation&nbsp;recommended&nbsp;by&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;authors&nbsp;for&nbsp;the&nbsp;Discriminator.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;other&nbsp;adversarial&nbsp;learning&nbsp;framework&nbsp;incorporated&nbsp;in&nbsp;AdversarialLearning<br>
&nbsp;&nbsp;&nbsp;&nbsp;is&nbsp;based&nbsp;on&nbsp;WGAN,&nbsp;which&nbsp;stands&nbsp;for&nbsp;Wasserstein&nbsp;GAN.&nbsp;&nbsp;This&nbsp;GAN&nbsp;was&nbsp;proposed&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;paper&nbsp;"Wasserstein&nbsp;GAN"&nbsp;by&nbsp;Arjovsky,&nbsp;Chintala,&nbsp;and&nbsp;Bottou.&nbsp;&nbsp;WGANs&nbsp;is&nbsp;based<br>
&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;estimating&nbsp;the&nbsp;Wasserstein&nbsp;distance&nbsp;between&nbsp;the&nbsp;distribution&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;corresponds&nbsp;to&nbsp;the&nbsp;training&nbsp;images&nbsp;and&nbsp;the&nbsp;distribution&nbsp;that&nbsp;has&nbsp;been&nbsp;learned<br>
&nbsp;&nbsp;&nbsp;&nbsp;so&nbsp;far&nbsp;by&nbsp;the&nbsp;Generator.&nbsp;&nbsp;The&nbsp;authors&nbsp;of&nbsp;WGAN&nbsp;have&nbsp;shown&nbsp;that&nbsp;minimizing&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;distance&nbsp;in&nbsp;an&nbsp;iterative&nbsp;learning&nbsp;framework&nbsp;also&nbsp;involves&nbsp;solving&nbsp;a&nbsp;minimax<br>
&nbsp;&nbsp;&nbsp;&nbsp;problem&nbsp;involving&nbsp;a&nbsp;Critic&nbsp;and&nbsp;a&nbsp;Generator.&nbsp;The&nbsp;Critic's&nbsp;job&nbsp;is&nbsp;to&nbsp;become&nbsp;an<br>
&nbsp;&nbsp;&nbsp;&nbsp;expert&nbsp;at&nbsp;recognizing&nbsp;the&nbsp;training&nbsp;data&nbsp;while,&nbsp;at&nbsp;the&nbsp;same&nbsp;time,&nbsp;distrusting<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;output&nbsp;of&nbsp;the&nbsp;Generator.&nbsp;Unlike&nbsp;the&nbsp;Discriminator&nbsp;of&nbsp;a&nbsp;GAN,&nbsp;the&nbsp;Critic<br>
&nbsp;&nbsp;&nbsp;&nbsp;merely&nbsp;seeks&nbsp;to&nbsp;estimate&nbsp;the&nbsp;Wasserstein&nbsp;distance&nbsp;between&nbsp;the&nbsp;true<br>
&nbsp;&nbsp;&nbsp;&nbsp;distribution&nbsp;associated&nbsp;with&nbsp;the&nbsp;training&nbsp;data&nbsp;and&nbsp;the&nbsp;distribution&nbsp;being<br>
&nbsp;&nbsp;&nbsp;&nbsp;learned&nbsp;by&nbsp;the&nbsp;Generator.&nbsp;&nbsp;As&nbsp;the&nbsp;Generator&nbsp;parameters&nbsp;are&nbsp;kept&nbsp;fixed,&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;Critic&nbsp;seems&nbsp;to&nbsp;update&nbsp;its&nbsp;parameters&nbsp;that&nbsp;maximize&nbsp;the&nbsp;Wasserstein&nbsp;distance<br>
&nbsp;&nbsp;&nbsp;&nbsp;between&nbsp;the&nbsp;true&nbsp;and&nbsp;the&nbsp;fake&nbsp;distributions.&nbsp;Subsequently,&nbsp;as&nbsp;the&nbsp;Critic<br>
&nbsp;&nbsp;&nbsp;&nbsp;parameters&nbsp;are&nbsp;kept&nbsp;fixed,&nbsp;the&nbsp;Generator&nbsp;updates&nbsp;its&nbsp;learnable&nbsp;parameters&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;an&nbsp;attempt&nbsp;to&nbsp;minimize&nbsp;the&nbsp;same&nbsp;distance.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Estimation&nbsp;of&nbsp;the&nbsp;Wasserstein&nbsp;distance&nbsp;in&nbsp;the&nbsp;above&nbsp;logic&nbsp;requires&nbsp;for&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;Critic&nbsp;to&nbsp;learn&nbsp;a&nbsp;1-Lipschitz&nbsp;function.&nbsp;DLStudio&nbsp;implements&nbsp;the&nbsp;following&nbsp;two<br>
&nbsp;&nbsp;&nbsp;&nbsp;strategies&nbsp;for&nbsp;this&nbsp;learning:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;--&nbsp;&nbsp;Clipping&nbsp;the&nbsp;values&nbsp;of&nbsp;the&nbsp;learnable&nbsp;parameters&nbsp;of&nbsp;the&nbsp;Critic&nbsp;network<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;a&nbsp;user-specified&nbsp;interval;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;--&nbsp;&nbsp;Penalizing&nbsp;the&nbsp;gradient&nbsp;of&nbsp;the&nbsp;norm&nbsp;of&nbsp;the&nbsp;Critic&nbsp;with&nbsp;respect&nbsp;to&nbsp;its<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;input.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;first&nbsp;of&nbsp;these&nbsp;is&nbsp;implemented&nbsp;in&nbsp;the&nbsp;function&nbsp;"run_gan_code()"&nbsp;in&nbsp;the&nbsp;file<br>
&nbsp;&nbsp;&nbsp;&nbsp;AdversarialLearning.py&nbsp;and&nbsp;the&nbsp;second&nbsp;in&nbsp;the&nbsp;function<br>
&nbsp;&nbsp;&nbsp;&nbsp;"run_wgan_with_gp_code()"&nbsp;in&nbsp;the&nbsp;same&nbsp;file.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;you&nbsp;wish&nbsp;to&nbsp;use&nbsp;the&nbsp;DLStudio&nbsp;platform&nbsp;to&nbsp;learn&nbsp;about&nbsp;data&nbsp;modeling&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;adversarial&nbsp;learning,&nbsp;your&nbsp;entry&nbsp;points&nbsp;should&nbsp;be&nbsp;the&nbsp;following&nbsp;scripts&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;ExamplesAdversarialLearning&nbsp;directory&nbsp;of&nbsp;the&nbsp;distro:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;&nbsp;dcgan_DG1.py&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;&nbsp;dcgan_DG2.py&nbsp;&nbsp;&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.&nbsp;&nbsp;wgan_CG1.py&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.&nbsp;&nbsp;wgan_with_gp_CG2.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;first&nbsp;script&nbsp;demonstrates&nbsp;the&nbsp;DCGAN&nbsp;logic&nbsp;on&nbsp;the&nbsp;PurdueShapes5GAN&nbsp;dataset.<br>
&nbsp;&nbsp;&nbsp;&nbsp;In&nbsp;order&nbsp;to&nbsp;show&nbsp;the&nbsp;sensitivity&nbsp;of&nbsp;the&nbsp;basic&nbsp;DCGAN&nbsp;logic&nbsp;to&nbsp;any&nbsp;variations&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;network&nbsp;or&nbsp;the&nbsp;weight&nbsp;initializations,&nbsp;the&nbsp;second&nbsp;script&nbsp;introduces&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;small&nbsp;change&nbsp;in&nbsp;the&nbsp;network.&nbsp;&nbsp;The&nbsp;third&nbsp;script&nbsp;is&nbsp;a&nbsp;demonstration&nbsp;of&nbsp;using&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;Wasserstein&nbsp;distance&nbsp;for&nbsp;data&nbsp;modeling&nbsp;through&nbsp;adversarial&nbsp;learning.&nbsp;The<br>
&nbsp;&nbsp;&nbsp;&nbsp;fourth&nbsp;script&nbsp;includes&nbsp;a&nbsp;gradient&nbsp;penalty&nbsp;in&nbsp;the&nbsp;critic&nbsp;logic&nbsp;called&nbsp;on&nbsp;by&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;third&nbsp;script.&nbsp;&nbsp;The&nbsp;results&nbsp;produced&nbsp;by&nbsp;these&nbsp;scripts&nbsp;(for&nbsp;the&nbsp;constructor<br>
&nbsp;&nbsp;&nbsp;&nbsp;options&nbsp;shown&nbsp;in&nbsp;the&nbsp;scripts)&nbsp;are&nbsp;included&nbsp;in&nbsp;a&nbsp;subdirectory&nbsp;named<br>
&nbsp;&nbsp;&nbsp;&nbsp;RVLCloud_based_results.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:blue; font-size:100%"><strong>&nbsp;&nbsp;<a id="109">    DATA MODELING WITH DIFFUSION</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Starting&nbsp;with&nbsp;Version&nbsp;2.4.2,&nbsp;DLStudio&nbsp;includes&nbsp;a&nbsp;new&nbsp;module&nbsp;named<br>
&nbsp;&nbsp;&nbsp;&nbsp;GenerativeDiffusion&nbsp;for&nbsp;experimenting&nbsp;with&nbsp;what's&nbsp;known&nbsp;as&nbsp;"Denoising&nbsp;Diffusion".<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;Denoising&nbsp;Diffusion&nbsp;approach&nbsp;to&nbsp;data&nbsp;modeling&nbsp;is&nbsp;based&nbsp;on&nbsp;the&nbsp;interaction<br>
&nbsp;&nbsp;&nbsp;&nbsp;between&nbsp;two&nbsp;Markov&nbsp;Chains&nbsp;over&nbsp;T&nbsp;timesteps:&nbsp;A&nbsp;forward&nbsp;chain&nbsp;called&nbsp;the&nbsp;q-chain<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;a&nbsp;reverse&nbsp;chain&nbsp;called&nbsp;the&nbsp;p-chain.&nbsp;&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;At&nbsp;each&nbsp;timestep&nbsp;in&nbsp;the&nbsp;forward&nbsp;q-chain,&nbsp;the&nbsp;data&nbsp;coursing&nbsp;through&nbsp;the&nbsp;chain&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;subject&nbsp;to&nbsp;a&nbsp;Markov&nbsp;transition&nbsp;that&nbsp;injects&nbsp;a&nbsp;small&nbsp;amount&nbsp;of&nbsp;zero-mean&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;isotropic&nbsp;Gaussian&nbsp;noise&nbsp;into&nbsp;the&nbsp;data.&nbsp;The&nbsp;goal&nbsp;in&nbsp;the&nbsp;q-chain&nbsp;is&nbsp;to&nbsp;inject<br>
&nbsp;&nbsp;&nbsp;&nbsp;sufficient&nbsp;noise&nbsp;at&nbsp;each&nbsp;timestep&nbsp;so&nbsp;that,&nbsp;at&nbsp;the&nbsp;end&nbsp;of&nbsp;the&nbsp;T&nbsp;timesteps,&nbsp;one<br>
&nbsp;&nbsp;&nbsp;&nbsp;will&nbsp;end&nbsp;up&nbsp;with&nbsp;pure&nbsp;isotropic&nbsp;noise.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;On&nbsp;the&nbsp;other&nbsp;hand,&nbsp;the&nbsp;goal&nbsp;in&nbsp;the&nbsp;reverse&nbsp;p-chain,&nbsp;is&nbsp;to&nbsp;start&nbsp;with&nbsp;zero-mean<br>
&nbsp;&nbsp;&nbsp;&nbsp;isotropic&nbsp;noise,&nbsp;subject&nbsp;it&nbsp;to&nbsp;a&nbsp;denoising&nbsp;Markov&nbsp;transition&nbsp;that&nbsp;gets&nbsp;rid&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;bit&nbsp;of&nbsp;the&nbsp;noise&nbsp;in&nbsp;the&nbsp;input,&nbsp;do&nbsp;so&nbsp;at&nbsp;every&nbsp;timestep,&nbsp;until&nbsp;you&nbsp;have<br>
&nbsp;&nbsp;&nbsp;&nbsp;recovered&nbsp;a&nbsp;recognizable&nbsp;image&nbsp;at&nbsp;the&nbsp;end&nbsp;of&nbsp;the&nbsp;chain.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;While&nbsp;the&nbsp;amount&nbsp;of&nbsp;noise&nbsp;that&nbsp;is&nbsp;injected&nbsp;into&nbsp;the&nbsp;data&nbsp;at&nbsp;each&nbsp;transition&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;forward&nbsp;q-chain&nbsp;is&nbsp;set&nbsp;by&nbsp;the&nbsp;user,&nbsp;how&nbsp;much&nbsp;denoising&nbsp;to&nbsp;carry&nbsp;out&nbsp;at&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;corresponding&nbsp;transition&nbsp;in&nbsp;the&nbsp;reverse&nbsp;p-chain&nbsp;is&nbsp;determined&nbsp;by&nbsp;a&nbsp;neural<br>
&nbsp;&nbsp;&nbsp;&nbsp;network&nbsp;whose&nbsp;job&nbsp;is&nbsp;to&nbsp;estimate&nbsp;the&nbsp;amount&nbsp;of&nbsp;denoising&nbsp;that,&nbsp;in&nbsp;a&nbsp;sense,<br>
&nbsp;&nbsp;&nbsp;&nbsp;would&nbsp;be&nbsp;"exact"&nbsp;opposite&nbsp;of&nbsp;the&nbsp;extent&nbsp;of&nbsp;diffusion&nbsp;carried&nbsp;at&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;corresponding&nbsp;transition&nbsp;in&nbsp;the&nbsp;forward&nbsp;q-chain.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;computational&nbsp;scenario&nbsp;described&nbsp;above&nbsp;becomes&nbsp;particularly&nbsp;tractable&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;case&nbsp;when&nbsp;you&nbsp;use&nbsp;isotropic&nbsp;Gaussian&nbsp;noise&nbsp;for&nbsp;both&nbsp;diffusion&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;denoising.&nbsp;When&nbsp;the&nbsp;transition&nbsp;probability&nbsp;at&nbsp;each&nbsp;timestep&nbsp;is&nbsp;isotropic<br>
&nbsp;&nbsp;&nbsp;&nbsp;Gaussian&nbsp;in&nbsp;the&nbsp;forward&nbsp;q-chain,&nbsp;it&nbsp;is&nbsp;easy&nbsp;to&nbsp;show&nbsp;that&nbsp;one&nbsp;can&nbsp;combine&nbsp;an<br>
&nbsp;&nbsp;&nbsp;&nbsp;arbitrary&nbsp;number&nbsp;of&nbsp;timesteps&nbsp;and&nbsp;get&nbsp;to&nbsp;the&nbsp;target&nbsp;timestep&nbsp;in&nbsp;a&nbsp;single&nbsp;hop.<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;leads&nbsp;to&nbsp;a&nbsp;particularly&nbsp;efficient&nbsp;algorithm&nbsp;described&nbsp;below&nbsp;for&nbsp;training<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;denoising&nbsp;neural&nbsp;network&nbsp;whose&nbsp;job&nbsp;is&nbsp;merely&nbsp;to&nbsp;estimate&nbsp;the&nbsp;best<br>
&nbsp;&nbsp;&nbsp;&nbsp;denoising&nbsp;transitions&nbsp;at&nbsp;each&nbsp;timestep:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;---&nbsp;At&nbsp;each&nbsp;iteration&nbsp;of&nbsp;training&nbsp;the&nbsp;neural&nbsp;network,&nbsp;randomly&nbsp;choose&nbsp;a&nbsp;timestep<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;t&nbsp;from&nbsp;the&nbsp;range&nbsp;that&nbsp;consists&nbsp;of&nbsp;T&nbsp;timesteps.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;---&nbsp;Apply&nbsp;a&nbsp;single&nbsp;cumulative&nbsp;q-chain&nbsp;transition&nbsp;to&nbsp;the&nbsp;input&nbsp;training&nbsp;image<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;that&nbsp;would&nbsp;be&nbsp;equivalent&nbsp;to&nbsp;taking&nbsp;the&nbsp;input&nbsp;image&nbsp;through&nbsp;t&nbsp;consecutive<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;transitions&nbsp;in&nbsp;the&nbsp;q-chain.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;---&nbsp;For&nbsp;each&nbsp;q-chain&nbsp;transition&nbsp;to&nbsp;the&nbsp;timestep&nbsp;t,&nbsp;use&nbsp;the&nbsp;Bayes'&nbsp;Rule&nbsp;to&nbsp;estimate<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;posterior&nbsp;probability&nbsp;q(&nbsp;x_{t-1}&nbsp;|&nbsp;x_t,&nbsp;x_0&nbsp;)&nbsp;from&nbsp;the&nbsp;Markov&nbsp;transition<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;probability&nbsp;q(&nbsp;x_t&nbsp;|&nbsp;x0,&nbsp;x_{t-1}&nbsp;).<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;---&nbsp;Use&nbsp;the&nbsp;posterior&nbsp;probabilities&nbsp;mentioned&nbsp;above&nbsp;as&nbsp;the&nbsp;target&nbsp;for&nbsp;training<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;neural&nbsp;network&nbsp;whose&nbsp;job&nbsp;is&nbsp;to&nbsp;estimate&nbsp;the&nbsp;transition&nbsp;probability&nbsp;p(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x_{t-1}&nbsp;|&nbsp;x_t&nbsp;)&nbsp;in&nbsp;the&nbsp;reverse&nbsp;p-chain.&nbsp;&nbsp;The&nbsp;loss&nbsp;function&nbsp;for&nbsp;training<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;neural&nbsp;network&nbsp;could&nbsp;be&nbsp;the&nbsp;KL-Divergence&nbsp;between&nbsp;the&nbsp;posterior&nbsp;q(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x_{t-1}&nbsp;|&nbsp;x_t,&nbsp;x_0&nbsp;)&nbsp;and&nbsp;the&nbsp;predicted&nbsp;p(&nbsp;x_{t-1}&nbsp;|&nbsp;x_t&nbsp;).<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Another&nbsp;possibility&nbsp;for&nbsp;the&nbsp;loss&nbsp;would&nbsp;be&nbsp;the&nbsp;MSE&nbsp;error&nbsp;between&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;isotropic&nbsp;noise&nbsp;that&nbsp;was&nbsp;injected&nbsp;in&nbsp;the&nbsp;q-chain&nbsp;transition&nbsp;in&nbsp;question<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;the&nbsp;prediction&nbsp;of&nbsp;the&nbsp;same&nbsp;in&nbsp;the&nbsp;p-chain&nbsp;by&nbsp;using&nbsp;the&nbsp;posterior<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;estimates&nbsp;for&nbsp;the&nbsp;mean&nbsp;and&nbsp;the&nbsp;variance&nbsp;using&nbsp;the&nbsp;transition&nbsp;probability<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;p(&nbsp;x_{t-1}&nbsp;|&nbsp;x_t&nbsp;)&nbsp;predicted&nbsp;by&nbsp;the&nbsp;neural&nbsp;network.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Yet&nbsp;another&nbsp;possibility&nbsp;is&nbsp;to&nbsp;directly&nbsp;form&nbsp;an&nbsp;estimate&nbsp;for&nbsp;the&nbsp;input<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;image&nbsp;x_0&nbsp;using&nbsp;the&nbsp;above-mentioned&nbsp;posterior&nbsp;estimates&nbsp;for&nbsp;the&nbsp;mean&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;variance&nbsp;and&nbsp;then&nbsp;construct&nbsp;an&nbsp;MSE&nbsp;loss&nbsp;based&nbsp;on&nbsp;the&nbsp;difference<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;between&nbsp;the&nbsp;estimated&nbsp;x_0&nbsp;and&nbsp;its&nbsp;true&nbsp;value.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;As&nbsp;should&nbsp;be&nbsp;clear&nbsp;from&nbsp;the&nbsp;above&nbsp;description,&nbsp;the&nbsp;sole&nbsp;goal&nbsp;of&nbsp;training&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;neural&nbsp;network&nbsp;is&nbsp;to&nbsp;make&nbsp;it&nbsp;an&nbsp;expert&nbsp;at&nbsp;the&nbsp;prediction&nbsp;of&nbsp;the&nbsp;denoising<br>
&nbsp;&nbsp;&nbsp;&nbsp;transition&nbsp;probabilities&nbsp;p(&nbsp;x_{t-1}&nbsp;|&nbsp;x_t&nbsp;).&nbsp;&nbsp;Typically,&nbsp;you&nbsp;carry&nbsp;out&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;training&nbsp;in&nbsp;an&nbsp;infinite&nbsp;loop&nbsp;while&nbsp;spiting&nbsp;out&nbsp;the&nbsp;checkpoints&nbsp;every&nbsp;so&nbsp;often.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;When&nbsp;you&nbsp;are&nbsp;ready&nbsp;to&nbsp;see&nbsp;the&nbsp;image&nbsp;generation&nbsp;power&nbsp;of&nbsp;a&nbsp;checkpoint,&nbsp;you<br>
&nbsp;&nbsp;&nbsp;&nbsp;start&nbsp;with&nbsp;isotropic&nbsp;Gaussian&nbsp;noise&nbsp;as&nbsp;the&nbsp;input&nbsp;and&nbsp;take&nbsp;it&nbsp;through&nbsp;all&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;T&nbsp;timestep&nbsp;p-chain&nbsp;transitions&nbsp;that&nbsp;should&nbsp;lead&nbsp;to&nbsp;a&nbsp;recognizable&nbsp;image.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;ExamplesDiffusion&nbsp;directory&nbsp;of&nbsp;DLStudio&nbsp;contains&nbsp;the&nbsp;following&nbsp;files&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;you&nbsp;will&nbsp;find&nbsp;helpful&nbsp;for&nbsp;your&nbsp;experiments&nbsp;with&nbsp;diffusion:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.&nbsp;&nbsp;README<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;&nbsp;RunCodeForDiffusion.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;&nbsp;GenerateNewImageSamples.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.&nbsp;&nbsp;VisualizeSamples.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Any&nbsp;experiment&nbsp;with&nbsp;diffusion&nbsp;will&nbsp;involve&nbsp;all&nbsp;three&nbsp;scripts&nbsp;mentioned&nbsp;above.<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;script&nbsp;RunCodeForDiffusion.py&nbsp;is&nbsp;for&nbsp;training&nbsp;the&nbsp;neural&nbsp;network&nbsp;to&nbsp;become<br>
&nbsp;&nbsp;&nbsp;&nbsp;adept&nbsp;at&nbsp;learning&nbsp;the&nbsp;p-chain&nbsp;transition&nbsp;probabilities&nbsp;p(&nbsp;x_{t-1}&nbsp;|&nbsp;x_t&nbsp;).<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;script&nbsp;GenerateNewImageSamples.py&nbsp;is&nbsp;for&nbsp;generating&nbsp;the&nbsp;images&nbsp;using&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;learned&nbsp;model.&nbsp;&nbsp;This&nbsp;script&nbsp;deposits&nbsp;all&nbsp;the&nbsp;generated&nbsp;images&nbsp;in&nbsp;a&nbsp;numpy<br>
&nbsp;&nbsp;&nbsp;&nbsp;archive&nbsp;for&nbsp;ndarrays.&nbsp;&nbsp;The&nbsp;last&nbsp;script,&nbsp;VisualizeSamples.py,&nbsp;is&nbsp;for&nbsp;extracting<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;individual&nbsp;images&nbsp;from&nbsp;that&nbsp;archive.&nbsp;&nbsp;Please&nbsp;make&nbsp;sure&nbsp;that&nbsp;you&nbsp;have&nbsp;gone<br>
&nbsp;&nbsp;&nbsp;&nbsp;through&nbsp;the&nbsp;README&nbsp;mentioned&nbsp;above&nbsp;before&nbsp;starting&nbsp;your&nbsp;experiments&nbsp;with&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;diffusion&nbsp;part&nbsp;of&nbsp;DLStudio.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:blue; font-size:100%"><strong>&nbsp;&nbsp;<a id="110">    SEQUENCE-TO-SEQUENCE LEARNING WITH ATTENTION</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Sequence-to-sequence&nbsp;learning&nbsp;(seq2seq)&nbsp;is&nbsp;about&nbsp;predicting&nbsp;an&nbsp;outcome<br>
&nbsp;&nbsp;&nbsp;&nbsp;sequence&nbsp;from&nbsp;a&nbsp;causation&nbsp;sequence,&nbsp;or,&nbsp;said&nbsp;another&nbsp;way,&nbsp;a&nbsp;target&nbsp;sequence<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;a&nbsp;source&nbsp;sequence.&nbsp;&nbsp;Automatic&nbsp;machine&nbsp;translation&nbsp;is&nbsp;probably&nbsp;one&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;most&nbsp;popular&nbsp;applications&nbsp;of&nbsp;seq2seq.&nbsp;&nbsp;DLStudio&nbsp;uses&nbsp;English-to-Spanish<br>
&nbsp;&nbsp;&nbsp;&nbsp;translation&nbsp;to&nbsp;illustrate&nbsp;the&nbsp;programming&nbsp;idioms&nbsp;and&nbsp;the&nbsp;PyTorch&nbsp;structures<br>
&nbsp;&nbsp;&nbsp;&nbsp;you&nbsp;need&nbsp;for&nbsp;seq2seq.&nbsp;&nbsp;To&nbsp;that&nbsp;end,&nbsp;Version&nbsp;2.1.0&nbsp;of&nbsp;DLStudio&nbsp;includes&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;new&nbsp;module&nbsp;named&nbsp;Seq2SeqLearning&nbsp;that&nbsp;consists&nbsp;of&nbsp;the&nbsp;following&nbsp;two&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;demonstration&nbsp;classes:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;&nbsp;Seq2SeqWithLearnableEmbeddings<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;&nbsp;Seq2SeqWithPretrainedEmbeddings<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;As&nbsp;their&nbsp;names&nbsp;imply,&nbsp;the&nbsp;first&nbsp;is&nbsp;for&nbsp;seq2seq&nbsp;with&nbsp;learnable&nbsp;embeddings&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;second&nbsp;for&nbsp;seq2seq&nbsp;with&nbsp;pre-trained&nbsp;embeddings&nbsp;like&nbsp;word2vec&nbsp;or&nbsp;fasttext.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;As&nbsp;mentioned&nbsp;above,&nbsp;the&nbsp;specific&nbsp;example&nbsp;of&nbsp;seq2seq&nbsp;addressed&nbsp;in&nbsp;my<br>
&nbsp;&nbsp;&nbsp;&nbsp;implementation&nbsp;code&nbsp;is&nbsp;translation&nbsp;from&nbsp;English&nbsp;to&nbsp;Spanish.&nbsp;(I&nbsp;chose&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;example&nbsp;because&nbsp;learning&nbsp;and&nbsp;keeping&nbsp;up&nbsp;with&nbsp;Spanish&nbsp;is&nbsp;one&nbsp;of&nbsp;my&nbsp;hobbies.)<br>
&nbsp;&nbsp;&nbsp;&nbsp;In&nbsp;the&nbsp;Seq2SeqWithLearnableEmbeddings&nbsp;class,&nbsp;the&nbsp;learning&nbsp;framework&nbsp;learns&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;best&nbsp;embedding&nbsp;vectors&nbsp;to&nbsp;use&nbsp;for&nbsp;the&nbsp;two&nbsp;languages&nbsp;involved.&nbsp;On&nbsp;the&nbsp;other<br>
&nbsp;&nbsp;&nbsp;&nbsp;hand,&nbsp;in&nbsp;the&nbsp;Seq2SeqWithPretrainedEmbeddings&nbsp;class,&nbsp;I&nbsp;use&nbsp;the&nbsp;word2vec<br>
&nbsp;&nbsp;&nbsp;&nbsp;embeddings&nbsp;provided&nbsp;by&nbsp;Google&nbsp;for&nbsp;the&nbsp;source&nbsp;language.&nbsp;&nbsp;As&nbsp;to&nbsp;why&nbsp;I&nbsp;use&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;pre-training&nbsp;embeddings&nbsp;for&nbsp;just&nbsp;the&nbsp;source&nbsp;language&nbsp;is&nbsp;explained&nbsp;in&nbsp;the&nbsp;main<br>
&nbsp;&nbsp;&nbsp;&nbsp;comment&nbsp;doc&nbsp;associated&nbsp;with&nbsp;the&nbsp;class&nbsp;Seq2SeqWithPretrainedEmbeddings.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Any&nbsp;modern&nbsp;attempt&nbsp;at&nbsp;seq2seq&nbsp;must&nbsp;include&nbsp;attention.&nbsp;&nbsp;This&nbsp;is&nbsp;done&nbsp;by<br>
&nbsp;&nbsp;&nbsp;&nbsp;incorporating&nbsp;a&nbsp;separate&nbsp;Attention&nbsp;network&nbsp;in&nbsp;the&nbsp;Encoder-Decoder&nbsp;framework<br>
&nbsp;&nbsp;&nbsp;&nbsp;needed&nbsp;for&nbsp;seq2seq&nbsp;learning.&nbsp;&nbsp;The&nbsp;goal&nbsp;of&nbsp;the&nbsp;attention&nbsp;network&nbsp;is&nbsp;to&nbsp;modify<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;current&nbsp;hidden&nbsp;state&nbsp;in&nbsp;the&nbsp;decoder&nbsp;using&nbsp;the&nbsp;attention&nbsp;units&nbsp;produced<br>
&nbsp;&nbsp;&nbsp;&nbsp;previously&nbsp;by&nbsp;the&nbsp;encoder&nbsp;for&nbsp;the&nbsp;source&nbsp;language&nbsp;sentence.&nbsp;&nbsp;The&nbsp;main<br>
&nbsp;&nbsp;&nbsp;&nbsp;Attention&nbsp;model&nbsp;I&nbsp;have&nbsp;used&nbsp;is&nbsp;based&nbsp;on&nbsp;my&nbsp;understanding&nbsp;of&nbsp;the&nbsp;attention<br>
&nbsp;&nbsp;&nbsp;&nbsp;mechanism&nbsp;proposed&nbsp;by&nbsp;Bahdanau,&nbsp;Cho,&nbsp;and&nbsp;Bengio.&nbsp;You&nbsp;will&nbsp;see&nbsp;this&nbsp;attention<br>
&nbsp;&nbsp;&nbsp;&nbsp;code&nbsp;in&nbsp;a&nbsp;class&nbsp;named&nbsp;Attention_BCB&nbsp;in&nbsp;the&nbsp;seq2seq&nbsp;implementations&nbsp;named<br>
&nbsp;&nbsp;&nbsp;&nbsp;above.&nbsp;I&nbsp;have&nbsp;also&nbsp;provided&nbsp;another&nbsp;attention&nbsp;class&nbsp;named&nbsp;Attention_SR&nbsp;that&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;my&nbsp;implementation&nbsp;of&nbsp;the&nbsp;attention&nbsp;mechanism&nbsp;in&nbsp;the&nbsp;very&nbsp;popular&nbsp;NLP&nbsp;tutorial<br>
&nbsp;&nbsp;&nbsp;&nbsp;by&nbsp;Sean&nbsp;Robertson&nbsp;at&nbsp;the&nbsp;PyTorch&nbsp;website.&nbsp;&nbsp;The&nbsp;URLs&nbsp;to&nbsp;both&nbsp;these&nbsp;attention<br>
&nbsp;&nbsp;&nbsp;&nbsp;mechanisms&nbsp;are&nbsp;in&nbsp;my&nbsp;Week&nbsp;14&nbsp;lecture&nbsp;material&nbsp;on&nbsp;deep&nbsp;learning&nbsp;at&nbsp;Purdue.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;following&nbsp;two&nbsp;scripts&nbsp;in&nbsp;the&nbsp;ExamplesSeq2SeqLearning&nbsp;directory&nbsp;are&nbsp;your<br>
&nbsp;&nbsp;&nbsp;&nbsp;main&nbsp;entry&nbsp;points&nbsp;for&nbsp;experimenting&nbsp;with&nbsp;the&nbsp;seq2seq&nbsp;code&nbsp;in&nbsp;DLStudio:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;&nbsp;seq2seq_with_learnable_embeddings.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;&nbsp;seq2seq_with_pretrained_embeddings.py<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;With&nbsp;the&nbsp;first&nbsp;script,&nbsp;the&nbsp;overall&nbsp;network&nbsp;will&nbsp;learn&nbsp;on&nbsp;its&nbsp;own&nbsp;the&nbsp;best<br>
&nbsp;&nbsp;&nbsp;&nbsp;embeddings&nbsp;to&nbsp;use&nbsp;for&nbsp;representing&nbsp;the&nbsp;words&nbsp;in&nbsp;the&nbsp;two&nbsp;languages.&nbsp;&nbsp;And,&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;second&nbsp;script,&nbsp;the&nbsp;pre-trained&nbsp;word2vec&nbsp;embeddings&nbsp;from&nbsp;Google&nbsp;are&nbsp;used<br>
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;the&nbsp;source&nbsp;language&nbsp;while&nbsp;the&nbsp;system&nbsp;learns&nbsp;the&nbsp;embeddings&nbsp;for&nbsp;the&nbsp;target<br>
&nbsp;&nbsp;&nbsp;&nbsp;language.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:blue; font-size:100%"><strong>&nbsp;&nbsp;<a id="111">    DATA PREDICTION</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Let's&nbsp;say&nbsp;you&nbsp;have&nbsp;a&nbsp;sequence&nbsp;of&nbsp;observations&nbsp;recorded&nbsp;at&nbsp;regular&nbsp;intervals.<br>
&nbsp;&nbsp;&nbsp;&nbsp;These&nbsp;could,&nbsp;for&nbsp;example,&nbsp;be&nbsp;the&nbsp;price&nbsp;of&nbsp;a&nbsp;stock&nbsp;share&nbsp;recorded&nbsp;every&nbsp;hour;<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;hourly&nbsp;recordings&nbsp;of&nbsp;electrical&nbsp;load&nbsp;at&nbsp;your&nbsp;local&nbsp;power&nbsp;utility&nbsp;company;<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;mean&nbsp;average&nbsp;temperature&nbsp;recorded&nbsp;on&nbsp;an&nbsp;annual&nbsp;basis;&nbsp;and&nbsp;so&nbsp;on.&nbsp;&nbsp;We&nbsp;want<br>
&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;use&nbsp;the&nbsp;past&nbsp;observations&nbsp;to&nbsp;predict&nbsp;the&nbsp;value&nbsp;of&nbsp;the&nbsp;next&nbsp;one.&nbsp;&nbsp;Solving<br>
&nbsp;&nbsp;&nbsp;&nbsp;these&nbsp;types&nbsp;of&nbsp;problems&nbsp;is&nbsp;the&nbsp;focus&nbsp;of&nbsp;the&nbsp;DataPrediction&nbsp;module&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;DLStudio&nbsp;platform.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;As&nbsp;a&nbsp;problem,&nbsp;data&nbsp;prediction&nbsp;has&nbsp;much&nbsp;in&nbsp;common&nbsp;with&nbsp;text&nbsp;analytics&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;seq2seq&nbsp;processing,&nbsp;in&nbsp;the&nbsp;sense&nbsp;that&nbsp;the&nbsp;prediction&nbsp;at&nbsp;the&nbsp;next&nbsp;time&nbsp;instant<br>
&nbsp;&nbsp;&nbsp;&nbsp;must&nbsp;be&nbsp;based&nbsp;on&nbsp;the&nbsp;previous&nbsp;observations&nbsp;in&nbsp;a&nbsp;manner&nbsp;similar&nbsp;to&nbsp;what&nbsp;we&nbsp;do<br>
&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;text&nbsp;analytics&nbsp;where&nbsp;the&nbsp;next&nbsp;word&nbsp;is&nbsp;understood&nbsp;taking&nbsp;into&nbsp;account&nbsp;all<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;previous&nbsp;words&nbsp;in&nbsp;a&nbsp;sentence.&nbsp;&nbsp;However,&nbsp;there&nbsp;are&nbsp;three&nbsp;significant<br>
&nbsp;&nbsp;&nbsp;&nbsp;differences&nbsp;between&nbsp;purely&nbsp;numerical&nbsp;data&nbsp;prediction&nbsp;problems&nbsp;and&nbsp;text-based<br>
&nbsp;&nbsp;&nbsp;&nbsp;problems:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;1)&nbsp;Data&nbsp;Normalization:&nbsp;As&nbsp;you&nbsp;know&nbsp;by&nbsp;this&nbsp;time,&nbsp;neural&nbsp;networks&nbsp;require&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;your&nbsp;input&nbsp;data&nbsp;be&nbsp;normalized&nbsp;to&nbsp;the&nbsp;[0,1]&nbsp;interval,&nbsp;assuming&nbsp;it&nbsp;consists<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;non-negative&nbsp;numbers,&nbsp;or&nbsp;the&nbsp;[-1,1]&nbsp;interval&nbsp;otherwise.&nbsp;&nbsp;When&nbsp;solving&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sequential-data&nbsp;problem&nbsp;like&nbsp;text&nbsp;analytics,&nbsp;after&nbsp;you&nbsp;have&nbsp;normalized&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;input&nbsp;data&nbsp;(which&nbsp;is&nbsp;likely&nbsp;to&nbsp;consist&nbsp;of&nbsp;the&nbsp;numeric&nbsp;embeddings&nbsp;for&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;input&nbsp;words),&nbsp;you&nbsp;can&nbsp;forget&nbsp;about&nbsp;it.&nbsp;&nbsp;You&nbsp;don't&nbsp;have&nbsp;that&nbsp;luxury&nbsp;when<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;solving&nbsp;a&nbsp;data&nbsp;prediction&nbsp;problem.&nbsp;&nbsp;As&nbsp;you&nbsp;would&nbsp;expect,&nbsp;the&nbsp;next&nbsp;value<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;predicted&nbsp;by&nbsp;an&nbsp;algorithm&nbsp;must&nbsp;be&nbsp;at&nbsp;the&nbsp;same&nbsp;scale&nbsp;as&nbsp;the&nbsp;original&nbsp;input<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;data.&nbsp;&nbsp;This&nbsp;requires&nbsp;that&nbsp;the&nbsp;output&nbsp;of&nbsp;a&nbsp;neural-network-based&nbsp;prediction<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;algorithm&nbsp;must&nbsp;be&nbsp;"inverse&nbsp;normalized".&nbsp;&nbsp;And&nbsp;that,&nbsp;in&nbsp;turn,&nbsp;requires<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;remembering&nbsp;the&nbsp;normalization&nbsp;parameters&nbsp;used&nbsp;in&nbsp;each&nbsp;channel&nbsp;of&nbsp;the&nbsp;input<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;data.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;2)&nbsp;Input&nbsp;Data&nbsp;Chunking:&nbsp;The&nbsp;notion&nbsp;of&nbsp;a&nbsp;sentence&nbsp;that&nbsp;is&nbsp;important&nbsp;in&nbsp;text<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;analytics&nbsp;does&nbsp;not&nbsp;carry&nbsp;over&nbsp;to&nbsp;the&nbsp;data&nbsp;prediction&nbsp;problem.&nbsp;&nbsp;In&nbsp;general,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;you&nbsp;would&nbsp;want&nbsp;a&nbsp;prediction&nbsp;to&nbsp;be&nbsp;made&nbsp;using&nbsp;ALL&nbsp;of&nbsp;the&nbsp;past<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;observations.&nbsp;When&nbsp;the&nbsp;sequential&nbsp;data&nbsp;available&nbsp;for&nbsp;training&nbsp;a&nbsp;predictor<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;is&nbsp;arbitrarily&nbsp;long,&nbsp;as&nbsp;is&nbsp;the&nbsp;case&nbsp;with&nbsp;numerical&nbsp;data&nbsp;in&nbsp;general,&nbsp;you<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;would&nbsp;need&nbsp;to&nbsp;decide&nbsp;how&nbsp;to&nbsp;"chunk"&nbsp;the&nbsp;data&nbsp;---&nbsp;that&nbsp;is,&nbsp;how&nbsp;to&nbsp;extract<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sub-sequences&nbsp;from&nbsp;the&nbsp;data&nbsp;for&nbsp;the&nbsp;purpose&nbsp;of&nbsp;training&nbsp;a&nbsp;neural&nbsp;network.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;3)&nbsp;Datetime&nbsp;Conditioning:&nbsp;Time-series&nbsp;data&nbsp;typically&nbsp;includes&nbsp;a&nbsp;"datetime"<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stamp&nbsp;for&nbsp;each&nbsp;observation.&nbsp;&nbsp;Representing&nbsp;datetime&nbsp;as&nbsp;a&nbsp;one-dimensional<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ever-increasing&nbsp;time&nbsp;value&nbsp;does&nbsp;not&nbsp;work&nbsp;for&nbsp;data&nbsp;prediction&nbsp;if&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;observations&nbsp;depend&nbsp;on&nbsp;the&nbsp;time&nbsp;of&nbsp;the&nbsp;day,&nbsp;the&nbsp;day&nbsp;of&nbsp;the&nbsp;week,&nbsp;the&nbsp;season<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;the&nbsp;year,&nbsp;and&nbsp;other&nbsp;such&nbsp;temporal&nbsp;effects.&nbsp;&nbsp;Incorporating&nbsp;such&nbsp;effects<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;a&nbsp;prediction&nbsp;framework&nbsp;requires&nbsp;a&nbsp;multi-dimensional&nbsp;encoding&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;datetime&nbsp;values.&nbsp;&nbsp;See&nbsp;the&nbsp;doc&nbsp;page&nbsp;associated&nbsp;with&nbsp;the&nbsp;DataPrediction&nbsp;class<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;a&nbsp;longer&nbsp;explanation&nbsp;of&nbsp;this&nbsp;aspect&nbsp;of&nbsp;data&nbsp;prediction.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Now&nbsp;that&nbsp;you&nbsp;understand&nbsp;how&nbsp;the&nbsp;data&nbsp;prediction&nbsp;problem&nbsp;differs&nbsp;from,&nbsp;say,&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;problem&nbsp;of&nbsp;text&nbsp;analytics,&nbsp;it&nbsp;is&nbsp;time&nbsp;for&nbsp;me&nbsp;to&nbsp;state&nbsp;my&nbsp;main&nbsp;goal&nbsp;in&nbsp;defining<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;DataPrediction&nbsp;module&nbsp;in&nbsp;the&nbsp;DLStudio&nbsp;platform.&nbsp;&nbsp;I&nbsp;actually&nbsp;have&nbsp;two<br>
&nbsp;&nbsp;&nbsp;&nbsp;goals:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(a)&nbsp;To&nbsp;deepen&nbsp;your&nbsp;understanding&nbsp;of&nbsp;a&nbsp;GRU.&nbsp;&nbsp;At&nbsp;this&nbsp;point,&nbsp;your&nbsp;understanding<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;a&nbsp;GRU&nbsp;is&nbsp;likely&nbsp;to&nbsp;be&nbsp;based&nbsp;on&nbsp;calling&nbsp;PyTorch's&nbsp;GRU&nbsp;in&nbsp;your&nbsp;own&nbsp;code.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Using&nbsp;a&nbsp;pre-programmed&nbsp;implementation&nbsp;for&nbsp;a&nbsp;GRU&nbsp;makes&nbsp;life&nbsp;easy&nbsp;and&nbsp;you<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;also&nbsp;get&nbsp;a&nbsp;piece&nbsp;of&nbsp;highly&nbsp;optimized&nbsp;code&nbsp;that&nbsp;you&nbsp;can&nbsp;just&nbsp;call&nbsp;in&nbsp;your<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;own&nbsp;code.&nbsp;&nbsp;However,&nbsp;with&nbsp;a&nbsp;pre-programmed&nbsp;GRU,&nbsp;you&nbsp;are&nbsp;unlikely&nbsp;to&nbsp;get<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;insights&nbsp;into&nbsp;how&nbsp;such&nbsp;an&nbsp;RNN&nbsp;is&nbsp;actually&nbsp;implemented.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(b)&nbsp;To&nbsp;demonstrate&nbsp;how&nbsp;you&nbsp;can&nbsp;use&nbsp;a&nbsp;Recurrent&nbsp;Neural&nbsp;Network&nbsp;(RNN)&nbsp;for&nbsp;data<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prediction&nbsp;taking&nbsp;into&nbsp;account&nbsp;the&nbsp;data&nbsp;normalization,&nbsp;chunking,&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;datetime&nbsp;conditioning&nbsp;issues&nbsp;mentioned&nbsp;earlier.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;To&nbsp;address&nbsp;the&nbsp;first&nbsp;goal&nbsp;above,&nbsp;the&nbsp;DataPrediction&nbsp;class&nbsp;presented&nbsp;in&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;file&nbsp;is&nbsp;based&nbsp;on&nbsp;my&nbsp;pmGRU&nbsp;(Poor&nbsp;Man's&nbsp;GRU).&nbsp;&nbsp;This&nbsp;GRU&nbsp;is&nbsp;my&nbsp;implementation&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;"Minimal&nbsp;Gated&nbsp;Unit"&nbsp;GRU&nbsp;variant&nbsp;that&nbsp;was&nbsp;first&nbsp;presented&nbsp;by&nbsp;Joel&nbsp;Heck&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;Fathi&nbsp;Salem&nbsp;in&nbsp;their&nbsp;paper&nbsp;"Simplified&nbsp;Minimal&nbsp;Gated&nbsp;Unit&nbsp;Variations&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;Recurrent&nbsp;Neural&nbsp;Networks".&nbsp;&nbsp;Its&nbsp;hallmark&nbsp;is&nbsp;that&nbsp;it&nbsp;combines&nbsp;the&nbsp;Update&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;Reset&nbsp;gates&nbsp;of&nbsp;a&nbsp;regular&nbsp;GRU&nbsp;into&nbsp;a&nbsp;single&nbsp;gate&nbsp;called&nbsp;the&nbsp;Forget&nbsp;Gate.<br>
&nbsp;&nbsp;&nbsp;&nbsp;You&nbsp;could&nbsp;say&nbsp;that&nbsp;pmGRU&nbsp;is&nbsp;a&nbsp;lightweight&nbsp;version&nbsp;of&nbsp;a&nbsp;regular&nbsp;GRU&nbsp;and&nbsp;its&nbsp;use<br>
&nbsp;&nbsp;&nbsp;&nbsp;may&nbsp;therefore&nbsp;lead&nbsp;to&nbsp;a&nbsp;slight&nbsp;loss&nbsp;of&nbsp;accuracy&nbsp;in&nbsp;the&nbsp;predictions.&nbsp;&nbsp;You&nbsp;will<br>
&nbsp;&nbsp;&nbsp;&nbsp;find&nbsp;it&nbsp;educational&nbsp;to&nbsp;compare&nbsp;the&nbsp;performance&nbsp;you&nbsp;get&nbsp;with&nbsp;my&nbsp;pmGRU-based<br>
&nbsp;&nbsp;&nbsp;&nbsp;implementation&nbsp;with&nbsp;an&nbsp;implementation&nbsp;that&nbsp;uses&nbsp;PyTorch's&nbsp;GRU&nbsp;for&nbsp;the&nbsp;same<br>
&nbsp;&nbsp;&nbsp;&nbsp;dataset.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Your&nbsp;main&nbsp;entry&nbsp;point&nbsp;for&nbsp;experimenting&nbsp;with&nbsp;the&nbsp;DataPrediction&nbsp;module&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;following&nbsp;script&nbsp;in&nbsp;the&nbsp;ExamplesDataPrediction&nbsp;directory&nbsp;of&nbsp;the&nbsp;DLStudio<br>
&nbsp;&nbsp;&nbsp;&nbsp;distribution:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;power_load_prediction_with_pmGRU.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Before&nbsp;you&nbsp;can&nbsp;run&nbsp;this&nbsp;script,&nbsp;you&nbsp;would&nbsp;need&nbsp;to&nbsp;download&nbsp;the&nbsp;training<br>
&nbsp;&nbsp;&nbsp;&nbsp;dataset&nbsp;used&nbsp;in&nbsp;this&nbsp;example.&nbsp;&nbsp;See&nbsp;the&nbsp;"For&nbsp;Data&nbsp;Prediction"&nbsp;part&nbsp;of&nbsp;the&nbsp;"The<br>
&nbsp;&nbsp;&nbsp;&nbsp;Datasets&nbsp;Included"&nbsp;section&nbsp;of&nbsp;the&nbsp;doc&nbsp;page&nbsp;for&nbsp;that.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:blue; font-size:100%"><strong>&nbsp;&nbsp;<a id="112">    TRANSFORMERS</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;For&nbsp;Seq2SeqLearning&nbsp;learning,&nbsp;the&nbsp;goal&nbsp;of&nbsp;a&nbsp;Transformer&nbsp;based&nbsp;implementation<br>
&nbsp;&nbsp;&nbsp;&nbsp;is&nbsp;the&nbsp;same&nbsp;as&nbsp;described&nbsp;earlier&nbsp;in&nbsp;this&nbsp;Introduction&nbsp;except&nbsp;that&nbsp;now&nbsp;you<br>
&nbsp;&nbsp;&nbsp;&nbsp;completely&nbsp;forgo&nbsp;recurrence.&nbsp;That&nbsp;is,&nbsp;you&nbsp;only&nbsp;use&nbsp;the&nbsp;mechanism&nbsp;of&nbsp;attention<br>
&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;translate&nbsp;sentences&nbsp;from&nbsp;a&nbsp;source&nbsp;language&nbsp;into&nbsp;sentences&nbsp;in&nbsp;the&nbsp;target<br>
&nbsp;&nbsp;&nbsp;&nbsp;language.&nbsp;For&nbsp;such&nbsp;applications,&nbsp;you&nbsp;need&nbsp;two&nbsp;forms&nbsp;of&nbsp;attention:<br>
&nbsp;&nbsp;&nbsp;&nbsp;self-attention&nbsp;and&nbsp;cross-attention.&nbsp;&nbsp;Self-attention&nbsp;refers&nbsp;to&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;intra-sentence&nbsp;relationships&nbsp;between&nbsp;the&nbsp;words&nbsp;and&nbsp;cross-attention&nbsp;refers&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;inter-sentence&nbsp;relationships&nbsp;between&nbsp;the&nbsp;words&nbsp;in&nbsp;a&nbsp;pair&nbsp;of&nbsp;sentences,&nbsp;one<br>
&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;the&nbsp;source&nbsp;language&nbsp;and&nbsp;the&nbsp;other&nbsp;in&nbsp;the&nbsp;target&nbsp;language.&nbsp;I&nbsp;have&nbsp;explained<br>
&nbsp;&nbsp;&nbsp;&nbsp;these&nbsp;concepts&nbsp;in&nbsp;great&nbsp;detail&nbsp;in&nbsp;the&nbsp;doc&nbsp;sections&nbsp;of&nbsp;the&nbsp;inner&nbsp;classes&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;Transformers&nbsp;class.&nbsp;&nbsp;In&nbsp;particular,&nbsp;I&nbsp;have&nbsp;explained&nbsp;the&nbsp;concept&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;"dot-product"&nbsp;attention&nbsp;in&nbsp;which&nbsp;each&nbsp;word&nbsp;puts&nbsp;out&nbsp;three&nbsp;things:&nbsp;a&nbsp;Query<br>
&nbsp;&nbsp;&nbsp;&nbsp;Vector&nbsp;Q,&nbsp;a&nbsp;Key&nbsp;Vector&nbsp;K,&nbsp;and&nbsp;a&nbsp;Value&nbsp;Vector&nbsp;V.&nbsp;By&nbsp;taking&nbsp;the&nbsp;dot-product&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;Query&nbsp;Vector&nbsp;Q&nbsp;of&nbsp;a&nbsp;word&nbsp;with&nbsp;the&nbsp;Key&nbsp;Vector&nbsp;K&nbsp;for&nbsp;all&nbsp;the&nbsp;words&nbsp;in&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;sentence,&nbsp;the&nbsp;neural&nbsp;network&nbsp;gets&nbsp;a&nbsp;measure&nbsp;of&nbsp;the&nbsp;extent&nbsp;to&nbsp;which&nbsp;each&nbsp;word<br>
&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;a&nbsp;sentence&nbsp;is&nbsp;important&nbsp;to&nbsp;every&nbsp;other&nbsp;word.&nbsp;&nbsp;These&nbsp;dot-product&nbsp;values&nbsp;are<br>
&nbsp;&nbsp;&nbsp;&nbsp;then&nbsp;used&nbsp;as&nbsp;weights&nbsp;on&nbsp;the&nbsp;Value&nbsp;Vectors,&nbsp;V,&nbsp;for&nbsp;the&nbsp;individual&nbsp;words.&nbsp;&nbsp;Cross<br>
&nbsp;&nbsp;&nbsp;&nbsp;attention&nbsp;works&nbsp;in&nbsp;a&nbsp;similar&nbsp;manner,&nbsp;except&nbsp;that&nbsp;now&nbsp;you&nbsp;take&nbsp;the&nbsp;dot-products<br>
&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;the&nbsp;Q&nbsp;vectors&nbsp;in&nbsp;the&nbsp;target-language&nbsp;sentence&nbsp;with&nbsp;the&nbsp;K&nbsp;vectors&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;corresponding&nbsp;source-language&nbsp;sentence&nbsp;for&nbsp;producing&nbsp;the&nbsp;weight&nbsp;vectors&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;tell&nbsp;us&nbsp;how&nbsp;to&nbsp;weight&nbsp;the&nbsp;source-language&nbsp;Value&nbsp;Vectors&nbsp;vis-a-vis&nbsp;the&nbsp;words&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;target&nbsp;language.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;In&nbsp;addition&nbsp;to&nbsp;their&nbsp;use&nbsp;in&nbsp;Seq2SeqLearning&nbsp;learning,&nbsp;transformers&nbsp;are&nbsp;now<br>
&nbsp;&nbsp;&nbsp;&nbsp;also&nbsp;used&nbsp;widely&nbsp;in&nbsp;computer&nbsp;vision&nbsp;applications.&nbsp;As&nbsp;a&nbsp;nod&nbsp;to&nbsp;their&nbsp;adoption<br>
&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;the&nbsp;learning&nbsp;required&nbsp;for&nbsp;solving&nbsp;CV&nbsp;problems,&nbsp;I&nbsp;have&nbsp;created&nbsp;a&nbsp;new&nbsp;class<br>
&nbsp;&nbsp;&nbsp;&nbsp;named&nbsp;visTransformer&nbsp;in&nbsp;the&nbsp;Transformers&nbsp;module&nbsp;of&nbsp;DLStudio.&nbsp;&nbsp;The&nbsp;transformer<br>
&nbsp;&nbsp;&nbsp;&nbsp;part&nbsp;of&nbsp;the&nbsp;logic&nbsp;in&nbsp;a&nbsp;visTransformer&nbsp;is&nbsp;identical&nbsp;to&nbsp;what&nbsp;it&nbsp;is&nbsp;in&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;transformer&nbsp;class&nbsp;for&nbsp;Seq2SeqLearning&nbsp;learning.&nbsp;&nbsp;That&nbsp;logic&nbsp;kicks&nbsp;in&nbsp;after&nbsp;you<br>
&nbsp;&nbsp;&nbsp;&nbsp;have&nbsp;divided&nbsp;an&nbsp;image&nbsp;into&nbsp;patches&nbsp;and&nbsp;you&nbsp;represent&nbsp;each&nbsp;patch&nbsp;by&nbsp;an<br>
&nbsp;&nbsp;&nbsp;&nbsp;embedding&nbsp;vector&nbsp;---&nbsp;in&nbsp;exactly&nbsp;the&nbsp;same&nbsp;as&nbsp;when&nbsp;you&nbsp;represent&nbsp;a&nbsp;word&nbsp;or&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;token&nbsp;in&nbsp;a&nbsp;sentence&nbsp;by&nbsp;an&nbsp;embedding&nbsp;vector.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;You&nbsp;will&nbsp;see&nbsp;three&nbsp;different&nbsp;implementations&nbsp;of&nbsp;the&nbsp;transformer&nbsp;architecture&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;Transformers&nbsp;module&nbsp;of&nbsp;the&nbsp;DLStudio&nbsp;platform:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TransformerFG<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;TransformerPreLN<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;visTransformer<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;"FG"&nbsp;suffix&nbsp;TransformerFG&nbsp;stands&nbsp;for&nbsp;"First&nbsp;Generation";&nbsp;the&nbsp;"PreLN"<br>
&nbsp;&nbsp;&nbsp;&nbsp;suffix&nbsp;in&nbsp;TransformerPreLN&nbsp;for&nbsp;"Pre&nbsp;LayerNorm";&nbsp;and,&nbsp;finally,&nbsp;the&nbsp;name<br>
&nbsp;&nbsp;&nbsp;&nbsp;visTransformer&nbsp;stands&nbsp;for&nbsp;"Vision&nbsp;Transformer."&nbsp;&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;TransformerFG&nbsp;is&nbsp;my&nbsp;implementation&nbsp;of&nbsp;the&nbsp;transformer&nbsp;architecture&nbsp;proposed&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;famous&nbsp;paper&nbsp;by&nbsp;Vaswani&nbsp;et&nbsp;al.&nbsp;&nbsp;and&nbsp;TransformerPreLN&nbsp;my&nbsp;implementation&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;same&nbsp;architecture&nbsp;but&nbsp;with&nbsp;the&nbsp;modification&nbsp;suggested&nbsp;by&nbsp;Xiong&nbsp;et&nbsp;al.&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;more&nbsp;stable&nbsp;learning.&nbsp;&nbsp;Since,&nbsp;the&nbsp;modification&nbsp;is&nbsp;small&nbsp;from&nbsp;an&nbsp;architectural<br>
&nbsp;&nbsp;&nbsp;&nbsp;standpoint,&nbsp;I&nbsp;could&nbsp;have&nbsp;combined&nbsp;both&nbsp;transformer&nbsp;types&nbsp;in&nbsp;the&nbsp;same<br>
&nbsp;&nbsp;&nbsp;&nbsp;implementation&nbsp;with&nbsp;some&nbsp;conditional&nbsp;logic&nbsp;to&nbsp;account&nbsp;for&nbsp;the&nbsp;differences.<br>
&nbsp;&nbsp;&nbsp;&nbsp;However,&nbsp;I&nbsp;have&nbsp;chosen&nbsp;to&nbsp;keep&nbsp;them&nbsp;separate&nbsp;mostly&nbsp;for&nbsp;educational&nbsp;purposes.<br>
&nbsp;&nbsp;&nbsp;&nbsp;Further&nbsp;details&nbsp;on&nbsp;these&nbsp;implementations&nbsp;are&nbsp;in&nbsp;the&nbsp;documentation&nbsp;blocks&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;Transformers&nbsp;module.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;visTransformer&nbsp;implementation&nbsp;is&nbsp;based&nbsp;on&nbsp;the&nbsp;paper&nbsp;"An&nbsp;Image&nbsp;is&nbsp;Worth<br>
&nbsp;&nbsp;&nbsp;&nbsp;16x16&nbsp;Words:&nbsp;Transformers&nbsp;for&nbsp;Image&nbsp;Recognition&nbsp;at&nbsp;Scale''&nbsp;by&nbsp;Dosovitskiy&nbsp;et<br>
&nbsp;&nbsp;&nbsp;&nbsp;al.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;you&nbsp;want&nbsp;to&nbsp;use&nbsp;my&nbsp;code&nbsp;for&nbsp;learning&nbsp;the&nbsp;main&nbsp;ideas&nbsp;related&nbsp;to&nbsp;how&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;create&nbsp;purely&nbsp;attention&nbsp;based&nbsp;networks,&nbsp;your&nbsp;starting&nbsp;point&nbsp;for&nbsp;that&nbsp;should&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;following&nbsp;scripts&nbsp;in&nbsp;the&nbsp;ExamplesTransformers&nbsp;directory&nbsp;of&nbsp;the&nbsp;DLStudio<br>
&nbsp;&nbsp;&nbsp;&nbsp;distribution:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;seq2seq_with_transformerFG.py<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;seq2seq_with_transformerPreLN.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;These&nbsp;scripts&nbsp;uses&nbsp;the&nbsp;following&nbsp;English-Spanish&nbsp;sentence-pairs&nbsp;dataset<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;en_es_xformer_8_90000.tar.gz<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;that&nbsp;contains&nbsp;90,000&nbsp;pairs&nbsp;of&nbsp;English-Spanish&nbsp;sentences&nbsp;with&nbsp;the&nbsp;maximum<br>
&nbsp;&nbsp;&nbsp;&nbsp;number&nbsp;of&nbsp;words&nbsp;in&nbsp;each&nbsp;sentence&nbsp;limited&nbsp;to&nbsp;8&nbsp;words.&nbsp;&nbsp;For&nbsp;processing&nbsp;by&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;attention&nbsp;networks,&nbsp;each&nbsp;sentence&nbsp;is&nbsp;enclosed&nbsp;in&nbsp;&lt;SOS&gt;&nbsp;and&nbsp;&lt;EOS&gt;&nbsp;tokens,&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;former&nbsp;standing&nbsp;for&nbsp;"Start&nbsp;of&nbsp;Sentence"&nbsp;and&nbsp;the&nbsp;latter&nbsp;for&nbsp;"End&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;Sentence".<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;And&nbsp;if&nbsp;you&nbsp;wish&nbsp;to&nbsp;use&nbsp;visTransformer&nbsp;for&nbsp;solving&nbsp;image&nbsp;recognition&nbsp;problems<br>
&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;a&nbsp;transformer&nbsp;based&nbsp;implementation,&nbsp;your&nbsp;starting&nbsp;point&nbsp;should&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;following&nbsp;scripts&nbsp;in&nbsp;the&nbsp;same&nbsp;ExamplesTransformers&nbsp;directory&nbsp;that&nbsp;was<br>
&nbsp;&nbsp;&nbsp;&nbsp;mentioned&nbsp;above:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;image_recog_with_visTransformer.py<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;test_checkpoint_for_visTransformer.py&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Both&nbsp;these&nbsp;script&nbsp;use&nbsp;the&nbsp;CIFAR10&nbsp;dataset&nbsp;for&nbsp;demonstrating&nbsp;image&nbsp;recognition.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:blue; font-size:100%"><strong>&nbsp;&nbsp;<a id="113">    METRIC LEARNING</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;main&nbsp;idea&nbsp;of&nbsp;metric&nbsp;learning&nbsp;is&nbsp;to&nbsp;learn&nbsp;a&nbsp;mapping&nbsp;from&nbsp;the&nbsp;images&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;their&nbsp;embedding&nbsp;vector&nbsp;representations&nbsp;in&nbsp;such&nbsp;a&nbsp;way&nbsp;that&nbsp;the&nbsp;embeddings&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;what&nbsp;are&nbsp;supposed&nbsp;to&nbsp;be&nbsp;similar&nbsp;images&nbsp;are&nbsp;pulled&nbsp;together&nbsp;and&nbsp;those&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;dissimilar&nbsp;images&nbsp;are&nbsp;pulled&nbsp;as&nbsp;far&nbsp;apart&nbsp;as&nbsp;possible.&nbsp;&nbsp;After&nbsp;such&nbsp;a&nbsp;mapping<br>
&nbsp;&nbsp;&nbsp;&nbsp;function&nbsp;is&nbsp;learned,&nbsp;you&nbsp;can&nbsp;take&nbsp;a&nbsp;query&nbsp;image&nbsp;(whose&nbsp;class&nbsp;label&nbsp;is&nbsp;not<br>
&nbsp;&nbsp;&nbsp;&nbsp;known),&nbsp;run&nbsp;it&nbsp;through&nbsp;the&nbsp;network&nbsp;to&nbsp;find&nbsp;its&nbsp;embedding&nbsp;vector,&nbsp;and,<br>
&nbsp;&nbsp;&nbsp;&nbsp;subsequently,&nbsp;assign&nbsp;to&nbsp;the&nbsp;query&nbsp;images&nbsp;the&nbsp;class&nbsp;label&nbsp;of&nbsp;the&nbsp;nearest<br>
&nbsp;&nbsp;&nbsp;&nbsp;training-image&nbsp;neighbor&nbsp;in&nbsp;the&nbsp;embedding&nbsp;space.&nbsp;&nbsp;As&nbsp;explained&nbsp;in&nbsp;my&nbsp;Metric<br>
&nbsp;&nbsp;&nbsp;&nbsp;Learning&nbsp;lecture&nbsp;in&nbsp;the&nbsp;Deep&nbsp;Learning&nbsp;class&nbsp;at&nbsp;Purdue,&nbsp;this&nbsp;approach&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;classification&nbsp;is&nbsp;likely&nbsp;to&nbsp;work&nbsp;under&nbsp;data&nbsp;circumstances&nbsp;when&nbsp;the&nbsp;more&nbsp;neural<br>
&nbsp;&nbsp;&nbsp;&nbsp;network&nbsp;classifiers&nbsp;fail.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Two&nbsp;commonly&nbsp;used&nbsp;loss&nbsp;functions&nbsp;for&nbsp;metric&nbsp;learning&nbsp;are&nbsp;Pairwise&nbsp;Contrastive<br>
&nbsp;&nbsp;&nbsp;&nbsp;Loss&nbsp;and&nbsp;Triplet&nbsp;Loss.&nbsp;&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Pairwise&nbsp;Contrastive&nbsp;Loss&nbsp;is&nbsp;based&nbsp;on&nbsp;extracting&nbsp;all&nbsp;the&nbsp;Positive&nbsp;and&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;Negative&nbsp;Pairs&nbsp;of&nbsp;images&nbsp;form&nbsp;a&nbsp;batch.&nbsp;&nbsp;For&nbsp;a&nbsp;Positive&nbsp;Pair,&nbsp;both&nbsp;the&nbsp;images<br>
&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;the&nbsp;pair&nbsp;must&nbsp;have&nbsp;the&nbsp;same&nbsp;label,&nbsp;and,&nbsp;for&nbsp;a&nbsp;Negative&nbsp;Pair,&nbsp;the&nbsp;two&nbsp;labels<br>
&nbsp;&nbsp;&nbsp;&nbsp;must&nbsp;be&nbsp;different.&nbsp;&nbsp;A&nbsp;minimization&nbsp;of&nbsp;the&nbsp;Pairwise&nbsp;Contrastive&nbsp;Loss&nbsp;should<br>
&nbsp;&nbsp;&nbsp;&nbsp;decrease&nbsp;the&nbsp;distance&nbsp;between&nbsp;the&nbsp;embedding&nbsp;vectors&nbsp;for&nbsp;a&nbsp;Positive&nbsp;Pair&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;increase&nbsp;the&nbsp;distance&nbsp;between&nbsp;the&nbsp;embedding&nbsp;vectors&nbsp;for&nbsp;a&nbsp;Negative&nbsp;Pair.&nbsp;&nbsp;If<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;two&nbsp;embeddings&nbsp;in&nbsp;a&nbsp;Negative&nbsp;Pair&nbsp;are&nbsp;already&nbsp;well&nbsp;separated,&nbsp;there&nbsp;would<br>
&nbsp;&nbsp;&nbsp;&nbsp;be&nbsp;no&nbsp;point&nbsp;to&nbsp;have&nbsp;them&nbsp;contribute&nbsp;to&nbsp;the&nbsp;loss&nbsp;calculation.&nbsp;&nbsp;This&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;accomplished&nbsp;by&nbsp;incorporating&nbsp;the&nbsp;notion&nbsp;of&nbsp;a&nbsp;margin.&nbsp;&nbsp;The&nbsp;idea&nbsp;is&nbsp;that&nbsp;we<br>
&nbsp;&nbsp;&nbsp;&nbsp;want&nbsp;to&nbsp;increase&nbsp;the&nbsp;distance&nbsp;between&nbsp;the&nbsp;two&nbsp;embeddings&nbsp;in&nbsp;a&nbsp;Negative&nbsp;Pair&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;value&nbsp;specified&nbsp;by&nbsp;the&nbsp;Margin&nbsp;and&nbsp;no&nbsp;more.&nbsp;Should&nbsp;it&nbsp;be&nbsp;the&nbsp;case&nbsp;that&nbsp;two<br>
&nbsp;&nbsp;&nbsp;&nbsp;such&nbsp;embeddings&nbsp;are&nbsp;already&nbsp;separated&nbsp;by&nbsp;a&nbsp;distance&nbsp;greater&nbsp;than&nbsp;the&nbsp;Margin,<br>
&nbsp;&nbsp;&nbsp;&nbsp;we&nbsp;do&nbsp;not&nbsp;include&nbsp;such&nbsp;pairs&nbsp;in&nbsp;the&nbsp;loss&nbsp;calculation.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Triplet&nbsp;Loss,&nbsp;on&nbsp;the&nbsp;other&nbsp;hand,&nbsp;starts&nbsp;with&nbsp;the&nbsp;notion&nbsp;of&nbsp;triplets&nbsp;(i,j,k)&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;indices&nbsp;for&nbsp;triplets&nbsp;of&nbsp;images&nbsp;in&nbsp;a&nbsp;batch,&nbsp;with&nbsp;no&nbsp;two&nbsp;of&nbsp;the&nbsp;indices<br>
&nbsp;&nbsp;&nbsp;&nbsp;being&nbsp;the&nbsp;same.&nbsp;The&nbsp;image&nbsp;indexed&nbsp;by&nbsp;i&nbsp;is&nbsp;considered&nbsp;to&nbsp;be&nbsp;the&nbsp;Anchor&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;triplet,&nbsp;the&nbsp;image&nbsp;indexed&nbsp;by&nbsp;j&nbsp;as&nbsp;Positive,&nbsp;and&nbsp;the&nbsp;one&nbsp;by&nbsp;k&nbsp;as&nbsp;the&nbsp;Negative.<br>
&nbsp;&nbsp;&nbsp;&nbsp;We&nbsp;also&nbsp;refer&nbsp;to&nbsp;such&nbsp;triplets&nbsp;with&nbsp;the&nbsp;notation&nbsp;(Anchor,&nbsp;Pos,&nbsp;Neg).&nbsp;&nbsp;We&nbsp;again<br>
&nbsp;&nbsp;&nbsp;&nbsp;need&nbsp;the&nbsp;notion&nbsp;of&nbsp;a&nbsp;Margin.&nbsp;When&nbsp;we&nbsp;form&nbsp;the&nbsp;(Anchor,&nbsp;Pos,&nbsp;Neg)&nbsp;triplets&nbsp;from<br>
&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;batch,&nbsp;we&nbsp;focus&nbsp;on&nbsp;only&nbsp;those&nbsp;Neg&nbsp;images&nbsp;that&nbsp;are&nbsp;further&nbsp;away&nbsp;from&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;Anchor&nbsp;than&nbsp;the&nbsp;Pos&nbsp;image,&nbsp;but&nbsp;no&nbsp;farther&nbsp;than&nbsp;the&nbsp;(Anchor,Pos)&nbsp;distance&nbsp;plus<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;Margin.&nbsp;&nbsp;Including&nbsp;the&nbsp;Negative&nbsp;images&nbsp;that&nbsp;are&nbsp;closer&nbsp;than&nbsp;the&nbsp;(Anchor,<br>
&nbsp;&nbsp;&nbsp;&nbsp;Pos)&nbsp;distance&nbsp;can&nbsp;make&nbsp;the&nbsp;learning&nbsp;unstable&nbsp;and&nbsp;including&nbsp;the&nbsp;Negatives&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;farther&nbsp;than&nbsp;the&nbsp;"(Anchor,Pos)&nbsp;plus&nbsp;the&nbsp;Margin"&nbsp;distance&nbsp;is&nbsp;likely&nbsp;to&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;wasteful.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Forming&nbsp;set&nbsp;of&nbsp;Positive&nbsp;and&nbsp;Negative&nbsp;Pairs&nbsp;for&nbsp;the&nbsp;Pairwise&nbsp;Contrastive&nbsp;Loss<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;forming&nbsp;sets&nbsp;of&nbsp;Triplets&nbsp;for&nbsp;the&nbsp;Triplet&nbsp;Loss&nbsp;is&nbsp;referred&nbsp;to&nbsp;as&nbsp;Mining&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;batch.&nbsp;&nbsp;This&nbsp;allows&nbsp;us&nbsp;to&nbsp;talk&nbsp;about&nbsp;concepts&nbsp;like&nbsp;"negative-hard&nbsp;mining",<br>
&nbsp;&nbsp;&nbsp;&nbsp;"negative&nbsp;semi-hard&nbsp;mining",&nbsp;etc.,&nbsp;that&nbsp;depend&nbsp;on&nbsp;the&nbsp;relative&nbsp;distances<br>
&nbsp;&nbsp;&nbsp;&nbsp;between&nbsp;the&nbsp;images&nbsp;in&nbsp;the&nbsp;Negative&nbsp;Pairs&nbsp;and&nbsp;the&nbsp;distance&nbsp;of&nbsp;a&nbsp;negative<br>
&nbsp;&nbsp;&nbsp;&nbsp;vis-a-vis&nbsp;those&nbsp;in&nbsp;a&nbsp;Positive&nbsp;Pair.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;you&nbsp;wish&nbsp;to&nbsp;use&nbsp;this&nbsp;module&nbsp;to&nbsp;learn&nbsp;about&nbsp;metric&nbsp;learning,&nbsp;your&nbsp;entry<br>
&nbsp;&nbsp;&nbsp;&nbsp;points&nbsp;should&nbsp;be&nbsp;the&nbsp;following&nbsp;scripts&nbsp;in&nbsp;the&nbsp;ExamplesMetricLearning&nbsp;directory<br>
&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;the&nbsp;distro:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;&nbsp;example_for_pairwise_contrastive_loss.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;&nbsp;example_for_triplet_loss.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;As&nbsp;the&nbsp;names&nbsp;imply,&nbsp;the&nbsp;first&nbsp;script&nbsp;demonstrates&nbsp;using&nbsp;the&nbsp;Pairwise<br>
&nbsp;&nbsp;&nbsp;&nbsp;Contrastive&nbsp;Loss&nbsp;for&nbsp;metric&nbsp;learning&nbsp;and&nbsp;the&nbsp;second&nbsp;script&nbsp;using&nbsp;the&nbsp;Triplet<br>
&nbsp;&nbsp;&nbsp;&nbsp;Loss&nbsp;for&nbsp;doing&nbsp;the&nbsp;same.&nbsp;&nbsp;Both&nbsp;scripts&nbsp;can&nbsp;work&nbsp;with&nbsp;either&nbsp;the&nbsp;pre-trained<br>
&nbsp;&nbsp;&nbsp;&nbsp;ResNet-50&nbsp;trunk&nbsp;model&nbsp;or&nbsp;the&nbsp;homebrewed&nbsp;network&nbsp;supplied&nbsp;with&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;MetricLearning&nbsp;module.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:red; font-size:150%"><strong><a id="114">INSTALLATION</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;DLStudio&nbsp;class&nbsp;was&nbsp;packaged&nbsp;using&nbsp;setuptools.&nbsp;&nbsp;For&nbsp;installation,&nbsp;execute<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;following&nbsp;command&nbsp;in&nbsp;the&nbsp;source&nbsp;directory&nbsp;(this&nbsp;is&nbsp;the&nbsp;directory&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;contains&nbsp;the&nbsp;setup.py&nbsp;file&nbsp;after&nbsp;you&nbsp;have&nbsp;downloaded&nbsp;and&nbsp;uncompressed&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;package):<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sudo&nbsp;python3&nbsp;setup.py&nbsp;install<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;On&nbsp;Linux&nbsp;distributions,&nbsp;this&nbsp;will&nbsp;install&nbsp;the&nbsp;module&nbsp;file&nbsp;at&nbsp;a&nbsp;location&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;looks&nbsp;like<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;/usr/local/lib/python3.10/dist-packages/<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;you&nbsp;do&nbsp;not&nbsp;have&nbsp;root&nbsp;access,&nbsp;you&nbsp;have&nbsp;the&nbsp;option&nbsp;of&nbsp;working&nbsp;directly&nbsp;off<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;directory&nbsp;in&nbsp;which&nbsp;you&nbsp;downloaded&nbsp;the&nbsp;software&nbsp;by&nbsp;simply&nbsp;placing&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;following&nbsp;statements&nbsp;at&nbsp;the&nbsp;top&nbsp;of&nbsp;your&nbsp;scripts&nbsp;that&nbsp;use&nbsp;the&nbsp;DLStudio&nbsp;class:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;import&nbsp;sys<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sys.path.append(&nbsp;"pathname_to_DLStudio_directory"&nbsp;)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;To&nbsp;uninstall&nbsp;DLStudio,&nbsp;simply&nbsp;delete&nbsp;the&nbsp;source&nbsp;code&nbsp;directory,&nbsp;locate&nbsp;where<br>
&nbsp;&nbsp;&nbsp;&nbsp;DLStudio&nbsp;was&nbsp;installed&nbsp;with&nbsp;"locate&nbsp;DLStudio"&nbsp;and&nbsp;delete&nbsp;those&nbsp;files.&nbsp;&nbsp;As<br>
&nbsp;&nbsp;&nbsp;&nbsp;mentioned&nbsp;above,&nbsp;the&nbsp;full&nbsp;pathname&nbsp;to&nbsp;the&nbsp;installed&nbsp;version&nbsp;is&nbsp;likely&nbsp;to&nbsp;look<br>
&nbsp;&nbsp;&nbsp;&nbsp;like&nbsp;/usr/local/lib/python3.10/dist-packages/DLStudio*<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;you&nbsp;want&nbsp;to&nbsp;carry&nbsp;out&nbsp;a&nbsp;non-standard&nbsp;install&nbsp;of&nbsp;DLStudio,&nbsp;look&nbsp;up&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;on-line&nbsp;information&nbsp;on&nbsp;Disutils&nbsp;by&nbsp;pointing&nbsp;your&nbsp;browser&nbsp;to<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://docs.python.org/dist/dist.html">http://docs.python.org/dist/dist.html</a><br>
&nbsp;<br>
<span style="color:red; font-size:150%"><strong><a id="115">USAGE</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;you&nbsp;want&nbsp;to&nbsp;specify&nbsp;a&nbsp;network&nbsp;with&nbsp;just&nbsp;a&nbsp;configuration&nbsp;string,&nbsp;your&nbsp;usage<br>
&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;the&nbsp;module&nbsp;is&nbsp;going&nbsp;to&nbsp;look&nbsp;like:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;DLStudio&nbsp;import&nbsp;*<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;convo_layers_config&nbsp;=&nbsp;"1x[128,3,3,1]-MaxPool(2)&nbsp;1x[16,5,5,1]-MaxPool(2)"<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fc_layers_config&nbsp;=&nbsp;[-1,1024,10]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dls&nbsp;=&nbsp;DLStudio(&nbsp;&nbsp;&nbsp;dataroot&nbsp;=&nbsp;"/home/kak/ImageDatasets/CIFAR-10/",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;image_size&nbsp;=&nbsp;[32,32],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;convo_layers_config&nbsp;=&nbsp;convo_layers_config,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fc_layers_config&nbsp;=&nbsp;fc_layers_config,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;path_saved_model&nbsp;=&nbsp;"./saved_model",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;momentum&nbsp;=&nbsp;0.9,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;learning_rate&nbsp;=&nbsp;1e-3,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;epochs&nbsp;=&nbsp;2,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;batch_size&nbsp;=&nbsp;4,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;classes&nbsp;=&nbsp;('plane','car','bird','cat','deer',<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'dog','frog','horse','ship','truck'),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;use_gpu&nbsp;=&nbsp;True,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;debug_train&nbsp;=&nbsp;0,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;debug_test&nbsp;=&nbsp;1,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;configs_for_all_convo_layers&nbsp;=&nbsp;dls.parse_config_string_for_convo_layers()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;convo_layers&nbsp;=&nbsp;dls.build_convo_layers2(&nbsp;configs_for_all_convo_layers&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fc_layers&nbsp;=&nbsp;dls.build_fc_layers()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;model&nbsp;=&nbsp;dls.Net(convo_layers,&nbsp;fc_layers)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dls.show_network_summary(model)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dls.load_cifar_10_dataset()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dls.run_code_for_training(model)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dls.run_code_for_testing(model)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;or,&nbsp;if&nbsp;you&nbsp;would&nbsp;rather&nbsp;experiment&nbsp;with&nbsp;a&nbsp;drop-in&nbsp;network,&nbsp;your&nbsp;usage&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;module&nbsp;is&nbsp;going&nbsp;to&nbsp;look&nbsp;something&nbsp;like:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dls&nbsp;=&nbsp;DLStudio(&nbsp;&nbsp;&nbsp;dataroot&nbsp;=&nbsp;"/home/kak/ImageDatasets/CIFAR-10/",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;image_size&nbsp;=&nbsp;[32,32],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;path_saved_model&nbsp;=&nbsp;"./saved_model",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;momentum&nbsp;=&nbsp;0.9,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;learning_rate&nbsp;=&nbsp;1e-3,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;epochs&nbsp;=&nbsp;2,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;batch_size&nbsp;=&nbsp;4,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;classes&nbsp;=&nbsp;('plane','car','bird','cat','deer',<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'dog','frog','horse','ship','truck'),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;use_gpu&nbsp;=&nbsp;True,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;debug_train&nbsp;=&nbsp;0,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;debug_test&nbsp;=&nbsp;1,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;exp_seq&nbsp;=&nbsp;DLStudio.ExperimentsWithSequential(&nbsp;dl_studio&nbsp;=&nbsp;dls&nbsp;)&nbsp;&nbsp;&nbsp;##&nbsp;for&nbsp;your&nbsp;drop-in&nbsp;network<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;exp_seq.load_cifar_10_dataset_with_augmentation()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;model&nbsp;=&nbsp;exp_seq.Net()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dls.show_network_summary(model)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;exp_seq.run_code_for_training(model)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;exp_seq.run_code_for_testing(model)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;assumes&nbsp;that&nbsp;you&nbsp;copy-and-pasted&nbsp;the&nbsp;network&nbsp;you&nbsp;want&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;experiment&nbsp;with&nbsp;in&nbsp;a&nbsp;class&nbsp;like&nbsp;ExperimentsWithSequential&nbsp;that&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;included&nbsp;in&nbsp;the&nbsp;module.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:red; font-size:150%"><strong><a id="116">CONSTRUCTOR PARAMETERS</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;batch_size:&nbsp;&nbsp;Carries&nbsp;the&nbsp;usual&nbsp;meaning&nbsp;in&nbsp;the&nbsp;neural&nbsp;network&nbsp;context.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;classes:&nbsp;&nbsp;A&nbsp;list&nbsp;of&nbsp;the&nbsp;symbolic&nbsp;names&nbsp;for&nbsp;the&nbsp;classes.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;convo_layers_config:&nbsp;This&nbsp;parameter&nbsp;allows&nbsp;you&nbsp;to&nbsp;specify&nbsp;a&nbsp;convolutional&nbsp;network<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;a&nbsp;configuration&nbsp;string.&nbsp;&nbsp;Must&nbsp;be&nbsp;formatted&nbsp;as&nbsp;explained&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;comment&nbsp;block&nbsp;associated&nbsp;with&nbsp;the&nbsp;method<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"parse_config_string_for_convo_layers()"<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;dataroot:&nbsp;This&nbsp;points&nbsp;to&nbsp;where&nbsp;your&nbsp;dataset&nbsp;is&nbsp;located.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;debug_test:&nbsp;Setting&nbsp;it&nbsp;allow&nbsp;you&nbsp;to&nbsp;see&nbsp;images&nbsp;being&nbsp;used&nbsp;and&nbsp;their&nbsp;predicted<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;class&nbsp;labels&nbsp;every&nbsp;2000&nbsp;batch-based&nbsp;iterations&nbsp;of&nbsp;testing.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;debug_train:&nbsp;Does&nbsp;the&nbsp;same&nbsp;thing&nbsp;during&nbsp;training&nbsp;that&nbsp;debug_test&nbsp;does&nbsp;during<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;testing.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;epochs:&nbsp;Specifies&nbsp;the&nbsp;number&nbsp;of&nbsp;epochs&nbsp;to&nbsp;be&nbsp;used&nbsp;for&nbsp;training&nbsp;the&nbsp;network.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;fc_layers_config:&nbsp;This&nbsp;parameter&nbsp;allows&nbsp;you&nbsp;to&nbsp;specify&nbsp;the&nbsp;final<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fully-connected&nbsp;portion&nbsp;of&nbsp;the&nbsp;network&nbsp;with&nbsp;just&nbsp;a&nbsp;list&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;number&nbsp;of&nbsp;nodes&nbsp;in&nbsp;each&nbsp;layer&nbsp;of&nbsp;this&nbsp;portion.&nbsp;&nbsp;The<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;first&nbsp;entry&nbsp;in&nbsp;this&nbsp;list&nbsp;must&nbsp;be&nbsp;the&nbsp;number&nbsp;'-1',&nbsp;which<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stands&nbsp;for&nbsp;the&nbsp;fact&nbsp;that&nbsp;the&nbsp;number&nbsp;of&nbsp;nodes&nbsp;in&nbsp;the&nbsp;first<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;layer&nbsp;will&nbsp;be&nbsp;determined&nbsp;by&nbsp;the&nbsp;final&nbsp;activation&nbsp;volume&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;convolutional&nbsp;portion&nbsp;of&nbsp;the&nbsp;network.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;image_size:&nbsp;&nbsp;The&nbsp;heightxwidth&nbsp;size&nbsp;of&nbsp;the&nbsp;images&nbsp;in&nbsp;your&nbsp;dataset.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;learning_rate:&nbsp;&nbsp;Again&nbsp;carries&nbsp;the&nbsp;usual&nbsp;meaning.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;momentum:&nbsp;&nbsp;Carries&nbsp;the&nbsp;usual&nbsp;meaning&nbsp;and&nbsp;needed&nbsp;by&nbsp;the&nbsp;optimizer.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;path_saved_model:&nbsp;The&nbsp;path&nbsp;to&nbsp;where&nbsp;you&nbsp;want&nbsp;the&nbsp;trained&nbsp;model&nbsp;to&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;saved&nbsp;in&nbsp;your&nbsp;disk&nbsp;so&nbsp;that&nbsp;it&nbsp;can&nbsp;be&nbsp;retrieved&nbsp;later<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;inference.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;use_gpu:&nbsp;You&nbsp;must&nbsp;set&nbsp;it&nbsp;to&nbsp;True&nbsp;if&nbsp;you&nbsp;want&nbsp;the&nbsp;GPU&nbsp;to&nbsp;be&nbsp;used&nbsp;for&nbsp;training.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:red; font-size:150%"><strong><a id="117">PUBLIC METHODS</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(1)&nbsp;&nbsp;build_convo_layers()<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;creates&nbsp;the&nbsp;convolutional&nbsp;layers&nbsp;from&nbsp;the&nbsp;parameters&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;configuration&nbsp;string&nbsp;that&nbsp;was&nbsp;supplied&nbsp;through&nbsp;the&nbsp;constructor&nbsp;option<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'convo_layers_config'.&nbsp;&nbsp;The&nbsp;output&nbsp;produced&nbsp;by&nbsp;the&nbsp;call&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'parse_config_string_for_convo_layers()'&nbsp;is&nbsp;supplied&nbsp;as&nbsp;the&nbsp;argument&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;build_convo_layers().<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(2)&nbsp;&nbsp;build_fc_layers()<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;From&nbsp;the&nbsp;list&nbsp;of&nbsp;ints&nbsp;supplied&nbsp;through&nbsp;the&nbsp;constructor&nbsp;option<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'fc_layers_config',&nbsp;this&nbsp;method&nbsp;constructs&nbsp;the&nbsp;fully-connected&nbsp;portion&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;overall&nbsp;network.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(3)&nbsp;&nbsp;check_a_sampling_of_images()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Displays&nbsp;the&nbsp;first&nbsp;batch_size&nbsp;number&nbsp;of&nbsp;images&nbsp;in&nbsp;your&nbsp;dataset.<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(4)&nbsp;&nbsp;display_tensor_as_image()<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;will&nbsp;display&nbsp;any&nbsp;tensor&nbsp;of&nbsp;shape&nbsp;(3,H,W),&nbsp;(1,H,W),&nbsp;or&nbsp;just<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(H,W)&nbsp;as&nbsp;an&nbsp;image.&nbsp;If&nbsp;any&nbsp;further&nbsp;data&nbsp;normalizations&nbsp;is&nbsp;needed&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;constructing&nbsp;a&nbsp;displayable&nbsp;image,&nbsp;the&nbsp;method&nbsp;takes&nbsp;care&nbsp;of&nbsp;that.&nbsp;&nbsp;It&nbsp;has<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;two&nbsp;input&nbsp;parameters:&nbsp;one&nbsp;for&nbsp;the&nbsp;tensor&nbsp;you&nbsp;want&nbsp;displayed&nbsp;as&nbsp;an&nbsp;image<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;the&nbsp;other&nbsp;for&nbsp;a&nbsp;title&nbsp;for&nbsp;the&nbsp;image&nbsp;display.&nbsp;&nbsp;The&nbsp;latter&nbsp;parameter&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;default&nbsp;initialized&nbsp;to&nbsp;an&nbsp;empty&nbsp;string.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(5)&nbsp;&nbsp;load_cifar_10_dataset()<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;is&nbsp;just&nbsp;a&nbsp;convenience&nbsp;method&nbsp;that&nbsp;calls&nbsp;on&nbsp;Torchvision's<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;functionality&nbsp;for&nbsp;creating&nbsp;a&nbsp;data&nbsp;loader.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(6)&nbsp;&nbsp;load_cifar_10_dataset_with_augmentation()&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;convenience&nbsp;method&nbsp;also&nbsp;creates&nbsp;a&nbsp;data&nbsp;loader&nbsp;but&nbsp;it&nbsp;also&nbsp;includes<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;syntax&nbsp;for&nbsp;data&nbsp;augmentation.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(7)&nbsp;&nbsp;parse_config_string_for_convo_layers()<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;As&nbsp;mentioned&nbsp;in&nbsp;the&nbsp;Introduction,&nbsp;DLStudio&nbsp;allows&nbsp;you&nbsp;to&nbsp;specify&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;convolutional&nbsp;network&nbsp;with&nbsp;a&nbsp;string&nbsp;provided&nbsp;the&nbsp;string&nbsp;obeys&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;formatting&nbsp;convention&nbsp;described&nbsp;in&nbsp;the&nbsp;comment&nbsp;block&nbsp;of&nbsp;this&nbsp;method.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;method&nbsp;is&nbsp;for&nbsp;parsing&nbsp;such&nbsp;a&nbsp;string.&nbsp;The&nbsp;string&nbsp;itself&nbsp;is&nbsp;presented<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;the&nbsp;module&nbsp;through&nbsp;the&nbsp;constructor&nbsp;option&nbsp;'convo_layers_config'.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(8)&nbsp;&nbsp;run_code_for_testing()<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;is&nbsp;the&nbsp;method&nbsp;runs&nbsp;the&nbsp;trained&nbsp;model&nbsp;on&nbsp;the&nbsp;test&nbsp;data.&nbsp;Its&nbsp;output&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;confusion&nbsp;matrix&nbsp;for&nbsp;the&nbsp;classes&nbsp;and&nbsp;the&nbsp;overall&nbsp;accuracy&nbsp;for&nbsp;each<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;class.&nbsp;&nbsp;The&nbsp;method&nbsp;has&nbsp;one&nbsp;input&nbsp;parameter&nbsp;which&nbsp;is&nbsp;set&nbsp;to&nbsp;the&nbsp;network&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;be&nbsp;tested.&nbsp;&nbsp;This&nbsp;learnable&nbsp;parameters&nbsp;in&nbsp;the&nbsp;network&nbsp;are&nbsp;initialized&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;disk-stored&nbsp;version&nbsp;of&nbsp;the&nbsp;trained&nbsp;model.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(9)&nbsp;&nbsp;run_code_for_training()<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;is&nbsp;the&nbsp;method&nbsp;that&nbsp;does&nbsp;all&nbsp;the&nbsp;training&nbsp;work.&nbsp;If&nbsp;a&nbsp;GPU&nbsp;was&nbsp;detected<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;at&nbsp;the&nbsp;time&nbsp;an&nbsp;instance&nbsp;of&nbsp;the&nbsp;module&nbsp;was&nbsp;created,&nbsp;this&nbsp;method&nbsp;takes&nbsp;care<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;making&nbsp;the&nbsp;appropriate&nbsp;calls&nbsp;in&nbsp;order&nbsp;to&nbsp;transfer&nbsp;the&nbsp;tensors&nbsp;involved<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;into&nbsp;the&nbsp;GPU&nbsp;memory.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(10)&nbsp;save_model()<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Writes&nbsp;the&nbsp;model&nbsp;out&nbsp;to&nbsp;the&nbsp;disk&nbsp;at&nbsp;the&nbsp;location&nbsp;specified&nbsp;by&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;constructor&nbsp;option&nbsp;'path_saved_model'.&nbsp;&nbsp;Has&nbsp;one&nbsp;input&nbsp;parameter&nbsp;for&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;model&nbsp;that&nbsp;needs&nbsp;to&nbsp;be&nbsp;written&nbsp;out.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(11)&nbsp;show_network_summary()<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Displays&nbsp;a&nbsp;print&nbsp;representation&nbsp;of&nbsp;your&nbsp;network&nbsp;and&nbsp;calls&nbsp;on&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;torchsummary&nbsp;module&nbsp;to&nbsp;print&nbsp;out&nbsp;the&nbsp;shape&nbsp;of&nbsp;the&nbsp;tensor&nbsp;at&nbsp;the&nbsp;output&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;each&nbsp;layer&nbsp;in&nbsp;the&nbsp;network.&nbsp;The&nbsp;method&nbsp;has&nbsp;one&nbsp;input&nbsp;parameter&nbsp;which&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;set&nbsp;to&nbsp;the&nbsp;network&nbsp;whose&nbsp;summary&nbsp;you&nbsp;want&nbsp;to&nbsp;see.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:red; font-size:150%"><strong><a id="118">THE MAIN INNER CLASSES OF THE DLStudio CLASS</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;By&nbsp;"inner&nbsp;classes"&nbsp;I&nbsp;mean&nbsp;the&nbsp;classes&nbsp;that&nbsp;are&nbsp;defined&nbsp;within&nbsp;the&nbsp;class&nbsp;file<br>
&nbsp;&nbsp;&nbsp;&nbsp;DLStudio.py&nbsp;in&nbsp;the&nbsp;DLStudio&nbsp;directory&nbsp;of&nbsp;the&nbsp;distribution.&nbsp;&nbsp;The&nbsp;DLStudio&nbsp;platform<br>
&nbsp;&nbsp;&nbsp;&nbsp;also&nbsp;includes&nbsp;several&nbsp;modules&nbsp;that&nbsp;reside&nbsp;at&nbsp;the&nbsp;same&nbsp;level&nbsp;of&nbsp;software<br>
&nbsp;&nbsp;&nbsp;&nbsp;abstraction&nbsp;as&nbsp;the&nbsp;main&nbsp;DLStudio&nbsp;class&nbsp;defined&nbsp;in&nbsp;the&nbsp;DLStudio.py&nbsp;file.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;purpose&nbsp;of&nbsp;the&nbsp;following&nbsp;two&nbsp;inner&nbsp;classes&nbsp;is&nbsp;to&nbsp;demonstrate&nbsp;how&nbsp;you&nbsp;can<br>
&nbsp;&nbsp;&nbsp;&nbsp;create&nbsp;a&nbsp;custom&nbsp;class&nbsp;for&nbsp;your&nbsp;own&nbsp;network&nbsp;and&nbsp;test&nbsp;it&nbsp;within&nbsp;the&nbsp;framework<br>
&nbsp;&nbsp;&nbsp;&nbsp;provided&nbsp;by&nbsp;DLStudio.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(1)&nbsp;&nbsp;class&nbsp;ExperimentsWithSequential<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;class&nbsp;is&nbsp;my&nbsp;demonstration&nbsp;of&nbsp;experimenting&nbsp;with&nbsp;a&nbsp;network&nbsp;that&nbsp;I&nbsp;found<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;GitHub.&nbsp;&nbsp;I&nbsp;copy-and-pasted&nbsp;it&nbsp;in&nbsp;this&nbsp;class&nbsp;to&nbsp;test&nbsp;its&nbsp;capabilities.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;How&nbsp;to&nbsp;call&nbsp;on&nbsp;such&nbsp;a&nbsp;custom&nbsp;class&nbsp;is&nbsp;shown&nbsp;by&nbsp;the&nbsp;following&nbsp;script&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Examples&nbsp;directory:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;playing_with_sequential.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(2)&nbsp;&nbsp;class&nbsp;ExperimentsWithCIFAR<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;is&nbsp;very&nbsp;similar&nbsp;to&nbsp;the&nbsp;previous&nbsp;inner&nbsp;class,&nbsp;but&nbsp;uses&nbsp;a&nbsp;common&nbsp;example<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;a&nbsp;network&nbsp;for&nbsp;experimenting&nbsp;with&nbsp;the&nbsp;CIFAR-10&nbsp;dataset.&nbsp;Consisting&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;32x32&nbsp;images,&nbsp;this&nbsp;is&nbsp;a&nbsp;great&nbsp;dataset&nbsp;for&nbsp;creating&nbsp;classroom&nbsp;demonstrations<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;convolutional&nbsp;networks.&nbsp;&nbsp;As&nbsp;to&nbsp;how&nbsp;you&nbsp;should&nbsp;use&nbsp;this&nbsp;class&nbsp;is&nbsp;shown&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;following&nbsp;script<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;playing_with_cifar10.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;the&nbsp;Examples&nbsp;directory&nbsp;of&nbsp;the&nbsp;distribution.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(4)&nbsp;&nbsp;class&nbsp;BMEnet&nbsp;(for&nbsp;connection&nbsp;skipping&nbsp;experiments)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;class&nbsp;is&nbsp;for&nbsp;investigating&nbsp;the&nbsp;power&nbsp;of&nbsp;skip&nbsp;connections&nbsp;in&nbsp;deep<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;networks.&nbsp;&nbsp;Skip&nbsp;connections&nbsp;are&nbsp;used&nbsp;to&nbsp;mitigate&nbsp;a&nbsp;serious&nbsp;problem<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;associated&nbsp;with&nbsp;deep&nbsp;networks&nbsp;---&nbsp;the&nbsp;problem&nbsp;of&nbsp;vanishing&nbsp;gradients.&nbsp;&nbsp;It<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;has&nbsp;been&nbsp;argued&nbsp;theoretically&nbsp;and&nbsp;demonstrated&nbsp;empirically&nbsp;that&nbsp;as&nbsp;the&nbsp;depth<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;a&nbsp;neural&nbsp;network&nbsp;increases,&nbsp;the&nbsp;gradients&nbsp;of&nbsp;the&nbsp;loss&nbsp;become&nbsp;more&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;more&nbsp;muted&nbsp;for&nbsp;the&nbsp;early&nbsp;layers&nbsp;in&nbsp;the&nbsp;network.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(5)&nbsp;&nbsp;class&nbsp;DetectAndLocalize<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;code&nbsp;in&nbsp;this&nbsp;inner&nbsp;class&nbsp;is&nbsp;for&nbsp;demonstrating&nbsp;how&nbsp;the&nbsp;same&nbsp;convolutional<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;network&nbsp;can&nbsp;simultaneously&nbsp;solve&nbsp;the&nbsp;twin&nbsp;problems&nbsp;of&nbsp;object&nbsp;detection&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;localization.&nbsp;&nbsp;Note&nbsp;that,&nbsp;unlike&nbsp;the&nbsp;previous&nbsp;four&nbsp;inner&nbsp;classes,&nbsp;class<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DetectAndLocalize&nbsp;comes&nbsp;with&nbsp;its&nbsp;own&nbsp;implementations&nbsp;for&nbsp;the&nbsp;training&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;testing&nbsp;methods.&nbsp;The&nbsp;main&nbsp;reason&nbsp;for&nbsp;that&nbsp;is&nbsp;that&nbsp;the&nbsp;training&nbsp;for&nbsp;detection<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;localization&nbsp;must&nbsp;use&nbsp;two&nbsp;different&nbsp;loss&nbsp;functions&nbsp;simultaneously,&nbsp;one<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;classification&nbsp;of&nbsp;the&nbsp;objects&nbsp;and&nbsp;the&nbsp;other&nbsp;for&nbsp;regression.&nbsp;The&nbsp;function<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;testing&nbsp;is&nbsp;also&nbsp;a&nbsp;bit&nbsp;more&nbsp;involved&nbsp;since&nbsp;it&nbsp;must&nbsp;now&nbsp;compute&nbsp;two&nbsp;kinds<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;errors,&nbsp;the&nbsp;classification&nbsp;error&nbsp;and&nbsp;the&nbsp;regression&nbsp;error&nbsp;on&nbsp;the&nbsp;unseen<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;data.&nbsp;Although&nbsp;you&nbsp;will&nbsp;find&nbsp;a&nbsp;couple&nbsp;of&nbsp;different&nbsp;choices&nbsp;for&nbsp;the&nbsp;training<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;testing&nbsp;functions&nbsp;for&nbsp;detection&nbsp;and&nbsp;localization&nbsp;inside<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DetectAndLocalize,&nbsp;the&nbsp;ones&nbsp;I&nbsp;have&nbsp;worked&nbsp;with&nbsp;the&nbsp;most&nbsp;are&nbsp;those&nbsp;that&nbsp;are<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;used&nbsp;in&nbsp;the&nbsp;following&nbsp;two&nbsp;scripts&nbsp;in&nbsp;the&nbsp;Examples&nbsp;directory:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;run_code_for_training_with_CrossEntropy_and_MSE_Losses()<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;run_code_for_testing_detection_and_localization()<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(6)&nbsp;&nbsp;class&nbsp;CustomDataLoading<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;is&nbsp;a&nbsp;testbed&nbsp;for&nbsp;experimenting&nbsp;with&nbsp;a&nbsp;completely&nbsp;grounds-up&nbsp;attempt&nbsp;at<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;designing&nbsp;a&nbsp;custom&nbsp;data&nbsp;loader.&nbsp;&nbsp;Ordinarily,&nbsp;if&nbsp;the&nbsp;basic&nbsp;format&nbsp;of&nbsp;how&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dataset&nbsp;is&nbsp;stored&nbsp;is&nbsp;similar&nbsp;to&nbsp;one&nbsp;of&nbsp;the&nbsp;datasets&nbsp;that&nbsp;Torchvision&nbsp;knows<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;about,&nbsp;you&nbsp;can&nbsp;go&nbsp;ahead&nbsp;and&nbsp;use&nbsp;that&nbsp;for&nbsp;your&nbsp;own&nbsp;dataset.&nbsp;&nbsp;At&nbsp;worst,&nbsp;you<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;may&nbsp;need&nbsp;to&nbsp;carry&nbsp;out&nbsp;some&nbsp;light&nbsp;customizations&nbsp;depending&nbsp;on&nbsp;the&nbsp;number&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;classes&nbsp;involved,&nbsp;etc.&nbsp;&nbsp;However,&nbsp;if&nbsp;the&nbsp;underlying&nbsp;dataset&nbsp;is&nbsp;stored&nbsp;in&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;manner&nbsp;that&nbsp;does&nbsp;not&nbsp;look&nbsp;like&nbsp;anything&nbsp;in&nbsp;Torchvision,&nbsp;you&nbsp;have&nbsp;no&nbsp;choice<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;but&nbsp;to&nbsp;supply&nbsp;yourself&nbsp;all&nbsp;of&nbsp;the&nbsp;data&nbsp;loading&nbsp;infrastructure.&nbsp;&nbsp;That&nbsp;is&nbsp;what<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;inner&nbsp;class&nbsp;of&nbsp;the&nbsp;DLStudio&nbsp;module&nbsp;is&nbsp;all&nbsp;about.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(7)&nbsp;&nbsp;class&nbsp;SemanticSegmentation<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;inner&nbsp;class&nbsp;is&nbsp;for&nbsp;working&nbsp;with&nbsp;the&nbsp;mUNet&nbsp;convolutional&nbsp;network&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;semantic&nbsp;segmentation&nbsp;of&nbsp;images.&nbsp;&nbsp;This&nbsp;network&nbsp;allows&nbsp;you&nbsp;to&nbsp;segment&nbsp;out<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;multiple&nbsp;objects&nbsp;simultaneously&nbsp;from&nbsp;an&nbsp;image.&nbsp;&nbsp;Each&nbsp;object&nbsp;type&nbsp;is&nbsp;assigned<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;different&nbsp;channel&nbsp;in&nbsp;the&nbsp;output&nbsp;of&nbsp;the&nbsp;network.&nbsp;&nbsp;So,&nbsp;for&nbsp;segmenting&nbsp;out<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;objects&nbsp;of&nbsp;a&nbsp;specified&nbsp;type&nbsp;in&nbsp;a&nbsp;given&nbsp;input&nbsp;image,&nbsp;all&nbsp;you&nbsp;have&nbsp;to&nbsp;do<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;is&nbsp;examine&nbsp;the&nbsp;corresponding&nbsp;channel&nbsp;in&nbsp;the&nbsp;output.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(8)&nbsp;&nbsp;class&nbsp;Autoencoder<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;man&nbsp;reason&nbsp;for&nbsp;the&nbsp;existence&nbsp;of&nbsp;this&nbsp;class&nbsp;in&nbsp;DLStudio&nbsp;is&nbsp;for&nbsp;it&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;serve&nbsp;as&nbsp;the&nbsp;base&nbsp;class&nbsp;for&nbsp;VAE&nbsp;(Variational&nbsp;Auto-Encoder).&nbsp;&nbsp;That&nbsp;way,&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;VAE&nbsp;class&nbsp;can&nbsp;focus&nbsp;exclusively&nbsp;on&nbsp;the&nbsp;random-sampling&nbsp;logic&nbsp;specific&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;variational&nbsp;encoding&nbsp;while&nbsp;the&nbsp;base&nbsp;class&nbsp;Autoencoder&nbsp;does&nbsp;the&nbsp;convolutional<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;transpose-convolutional&nbsp;heavy&nbsp;lifting&nbsp;associated&nbsp;with&nbsp;the&nbsp;usual<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;encoding-decoding&nbsp;of&nbsp;image&nbsp;data.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(9)&nbsp;&nbsp;class&nbsp;VAE<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;As&nbsp;mentioned&nbsp;above,&nbsp;VAE&nbsp;stands&nbsp;for&nbsp;"Variational&nbsp;Auto-Encoder".&nbsp;This&nbsp;class<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;extends&nbsp;the&nbsp;base&nbsp;class&nbsp;Autoencoder&nbsp;with&nbsp;the&nbsp;variational&nbsp;logic&nbsp;for&nbsp;learning<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;latent&nbsp;distribution&nbsp;representation&nbsp;of&nbsp;a&nbsp;training&nbsp;dataset.&nbsp;&nbsp;As&nbsp;to&nbsp;what<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;that&nbsp;means,&nbsp;latent&nbsp;representations&nbsp;are&nbsp;based&nbsp;on&nbsp;the&nbsp;assumption&nbsp;that&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"essence"&nbsp;of&nbsp;each&nbsp;sample&nbsp;of&nbsp;the&nbsp;input&nbsp;data&nbsp;can&nbsp;be&nbsp;represented&nbsp;by&nbsp;a&nbsp;vector&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;lower-dimensional&nbsp;space&nbsp;that&nbsp;has&nbsp;a&nbsp;much&nbsp;simpler&nbsp;distribution&nbsp;(ideally&nbsp;an<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;isotropic&nbsp;zero-mean&nbsp;and&nbsp;unit-covariance&nbsp;distribution)&nbsp;than&nbsp;what&nbsp;is&nbsp;possessed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;by&nbsp;the&nbsp;original&nbsp;training&nbsp;data&nbsp;samples.&nbsp;&nbsp;Separating&nbsp;out&nbsp;the&nbsp;"essence"&nbsp;from<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;rest&nbsp;in&nbsp;the&nbsp;input&nbsp;images&nbsp;in&nbsp;this&nbsp;manner&nbsp;is&nbsp;referred&nbsp;to&nbsp;as<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Disentanglement&nbsp;Learning."&nbsp;&nbsp;One&nbsp;you&nbsp;have&nbsp;learned&nbsp;the&nbsp;latent&nbsp;distribution,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;you&nbsp;can&nbsp;sample&nbsp;it&nbsp;and&nbsp;embellish&nbsp;it&nbsp;in&nbsp;a&nbsp;Decoder&nbsp;to&nbsp;produce&nbsp;an&nbsp;output&nbsp;that&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;useful&nbsp;to&nbsp;the&nbsp;user.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(10)&nbsp;class&nbsp;VQVAE<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;VQVAE&nbsp;stands&nbsp;for&nbsp;"Vector&nbsp;Quantized&nbsp;Variational&nbsp;Auto&nbsp;Encoder",&nbsp;which&nbsp;is&nbsp;also<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;frequently&nbsp;represented&nbsp;by&nbsp;the&nbsp;acronym&nbsp;VQ-VAE.&nbsp;&nbsp;The&nbsp;concept&nbsp;of&nbsp;VQ-VAE&nbsp;was<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;formulated&nbsp;in&nbsp;the&nbsp;2018&nbsp;paper&nbsp;"Neural&nbsp;Discrete&nbsp;Representation&nbsp;Learning"&nbsp;by<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;van&nbsp;den&nbsp;Oord,&nbsp;Vinyals,&nbsp;and&nbsp;Kavukcuoglu.&nbsp;VQVAE&nbsp;is&nbsp;an&nbsp;important&nbsp;architecture<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;deep&nbsp;learning&nbsp;because&nbsp;it&nbsp;teaches&nbsp;us&nbsp;about&nbsp;what&nbsp;has&nbsp;come&nbsp;to&nbsp;be&nbsp;known&nbsp;as<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Codebook&nbsp;Learning"&nbsp;for&nbsp;created&nbsp;discrete&nbsp;representations&nbsp;for&nbsp;images.&nbsp;The<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Codebook&nbsp;learned&nbsp;consists&nbsp;of&nbsp;a&nbsp;fixed&nbsp;number&nbsp;of&nbsp;embedding&nbsp;vectors.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Subsequently,&nbsp;in&nbsp;an&nbsp;overall&nbsp;Encoder-Decoder&nbsp;architectures,&nbsp;you&nbsp;replace&nbsp;each<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pixel&nbsp;at&nbsp;the&nbsp;output&nbsp;of&nbsp;the&nbsp;Encoder&nbsp;with&nbsp;the&nbsp;closest&nbsp;embedding&nbsp;vector&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Codebook.&nbsp;The&nbsp;dimensionality&nbsp;you&nbsp;associate&nbsp;with&nbsp;a&nbsp;pixel&nbsp;at&nbsp;the&nbsp;output&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Encoder&nbsp;is&nbsp;the&nbsp;number&nbsp;of&nbsp;channels&nbsp;at&nbsp;that&nbsp;point.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(11)&nbsp;class&nbsp;VQGAN<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;VQGAN&nbsp;stands&nbsp;for&nbsp;"Vector&nbsp;Quantized&nbsp;Generative&nbsp;Adversarial&nbsp;Network".&nbsp;&nbsp;There<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;two&nbsp;main&nbsp;differences&nbsp;between&nbsp;VQVAE&nbsp;and&nbsp;VQGAN:&nbsp;(1)&nbsp;The<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Encoder-VQ-Decoder"&nbsp;network&nbsp;in&nbsp;a&nbsp;VQGAN&nbsp;is&nbsp;trained&nbsp;through&nbsp;adversarial<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;learning&nbsp;by&nbsp;encapsulating&nbsp;it&nbsp;in&nbsp;a&nbsp;GAN.&nbsp;&nbsp;The&nbsp;concept&nbsp;of&nbsp;a&nbsp;GAN&nbsp;requires&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Discriminator&nbsp;and&nbsp;a&nbsp;Generator,&nbsp;with&nbsp;the&nbsp;Discriminator&nbsp;trained&nbsp;to&nbsp;become&nbsp;an<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;expert&nbsp;at&nbsp;recognizing&nbsp;the&nbsp;training&nbsp;images,&nbsp;while&nbsp;at&nbsp;the&nbsp;same&nbsp;time<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;disbelieving&nbsp;the&nbsp;output&nbsp;of&nbsp;the&nbsp;Generator&nbsp;as&nbsp;looking&nbsp;like&nbsp;it&nbsp;came&nbsp;from&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;training&nbsp;set&nbsp;of&nbsp;images.&nbsp;&nbsp;While&nbsp;you&nbsp;can&nbsp;use&nbsp;any&nbsp;run-of-the-mill&nbsp;discriminator<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;such&nbsp;adversarial&nbsp;learning,&nbsp;the&nbsp;Generator&nbsp;is&nbsp;nothing&nbsp;but&nbsp;our&nbsp;Encoder-VQ-<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Decoder&nbsp;network.&nbsp;&nbsp;And&nbsp;(2)&nbsp;After&nbsp;training&nbsp;the&nbsp;VQGAN&nbsp;network,&nbsp;you&nbsp;train&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;transformer-based&nbsp;network&nbsp;for&nbsp;autoregressive&nbsp;modeling&nbsp;of&nbsp;the&nbsp;codebook<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;indices&nbsp;as&nbsp;produced&nbsp;by&nbsp;the&nbsp;VectorQuantizer&nbsp;(VQ).&nbsp;&nbsp;The&nbsp;codebook&nbsp;indices&nbsp;are<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;integer&nbsp;index&nbsp;values&nbsp;that&nbsp;point&nbsp;to&nbsp;the&nbsp;codebook&nbsp;vectors&nbsp;that&nbsp;are&nbsp;chosen<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;as&nbsp;being&nbsp;the&nbsp;closest&nbsp;to&nbsp;the&nbsp;embedding&nbsp;vectors&nbsp;at&nbsp;the&nbsp;output&nbsp;of&nbsp;the&nbsp;Encoder.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(12)&nbsp;class&nbsp;TextClassification<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;purpose&nbsp;of&nbsp;this&nbsp;inner&nbsp;class&nbsp;is&nbsp;to&nbsp;be&nbsp;able&nbsp;to&nbsp;use&nbsp;DLStudio&nbsp;for&nbsp;simple<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;experiments&nbsp;in&nbsp;text&nbsp;classification.&nbsp;&nbsp;Consider,&nbsp;for&nbsp;example,&nbsp;the&nbsp;problem&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;automatic&nbsp;classification&nbsp;of&nbsp;variable-length&nbsp;user&nbsp;feedback:&nbsp;you&nbsp;want&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;create&nbsp;a&nbsp;neural&nbsp;network&nbsp;that&nbsp;can&nbsp;label&nbsp;an&nbsp;uploaded&nbsp;product&nbsp;review&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;arbitrary&nbsp;length&nbsp;as&nbsp;positive&nbsp;or&nbsp;negative.&nbsp;&nbsp;One&nbsp;way&nbsp;to&nbsp;solve&nbsp;this&nbsp;problem&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;a&nbsp;Recurrent&nbsp;Neural&nbsp;Network&nbsp;in&nbsp;which&nbsp;you&nbsp;use&nbsp;a&nbsp;hidden&nbsp;state&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;characterizing&nbsp;a&nbsp;variable-length&nbsp;product&nbsp;review&nbsp;with&nbsp;a&nbsp;fixed-length&nbsp;state<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;vector.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(13)&nbsp;class&nbsp;TextClassificationWithEmbeddings<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;class&nbsp;has&nbsp;the&nbsp;same&nbsp;functionality&nbsp;as&nbsp;the&nbsp;previous&nbsp;text&nbsp;processing&nbsp;class<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;except&nbsp;that&nbsp;now&nbsp;we&nbsp;use&nbsp;embeddings&nbsp;for&nbsp;representing&nbsp;the&nbsp;words.&nbsp;&nbsp;Word<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;embeddings&nbsp;are&nbsp;fixed-sized&nbsp;numerical&nbsp;vectors&nbsp;that&nbsp;are&nbsp;learned&nbsp;on&nbsp;the&nbsp;basis<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;the&nbsp;contextual&nbsp;similarity&nbsp;of&nbsp;the&nbsp;words.&nbsp;The&nbsp;implementation&nbsp;of&nbsp;this&nbsp;inner<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;class&nbsp;uses&nbsp;the&nbsp;pre-trained&nbsp;300-element&nbsp;word2vec&nbsp;embeddings&nbsp;as&nbsp;made&nbsp;available<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;by&nbsp;Google&nbsp;for&nbsp;3&nbsp;million&nbsp;words&nbsp;and&nbsp;phrases&nbsp;drawn&nbsp;from&nbsp;the&nbsp;Google&nbsp;News<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dataset.&nbsp;In&nbsp;DLStudio,&nbsp;we&nbsp;access&nbsp;these&nbsp;embeddings&nbsp;through&nbsp;the&nbsp;popular&nbsp;gensim<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;library.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:red; font-size:150%"><strong><a id="119">MODULES IN THE DLStudio PLATFORM</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;As&nbsp;stated&nbsp;at&nbsp;the&nbsp;beginning&nbsp;of&nbsp;the&nbsp;previous&nbsp;section,&nbsp;a&nbsp;module&nbsp;resides&nbsp;at&nbsp;the&nbsp;same<br>
&nbsp;&nbsp;&nbsp;&nbsp;level&nbsp;of&nbsp;software&nbsp;abstraction&nbsp;in&nbsp;the&nbsp;distribution&nbsp;directory&nbsp;as&nbsp;the&nbsp;main&nbsp;DLStudio<br>
&nbsp;&nbsp;&nbsp;&nbsp;class&nbsp;in&nbsp;the&nbsp;platform.&nbsp;Each&nbsp;module&nbsp;is&nbsp;defined&nbsp;in&nbsp;a&nbsp;separate&nbsp;subdirectory&nbsp;at&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;top&nbsp;level&nbsp;of&nbsp;the&nbsp;distribution&nbsp;directory.&nbsp;&nbsp;While&nbsp;the&nbsp;main&nbsp;DLStudio&nbsp;class&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;defined&nbsp;in&nbsp;a&nbsp;subdirectory&nbsp;of&nbsp;the&nbsp;same&nbsp;name,&nbsp;the&nbsp;other&nbsp;subdirectories&nbsp;that&nbsp;contain<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;definitions&nbsp;for&nbsp;the&nbsp;modules&nbsp;are&nbsp;named&nbsp;AdversarialLearning,&nbsp;Seq2SeqLearning,<br>
&nbsp;&nbsp;&nbsp;&nbsp;DataPrediction,&nbsp;Transformers,&nbsp;GenerativeDiffusion,&nbsp;and&nbsp;MetricLearning.&nbsp;&nbsp;What<br>
&nbsp;&nbsp;&nbsp;&nbsp;follows&nbsp;in&nbsp;this&nbsp;section&nbsp;are&nbsp;additional&nbsp;details&nbsp;regarding&nbsp;these&nbsp;co-classes:<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:red; font-size:150%"><strong>    </strong></span><br>&nbsp;&nbsp;&nbsp;&nbsp;AdversarialLearning:<br>
&nbsp;&nbsp;&nbsp;&nbsp;===============<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;As&nbsp;I&nbsp;mentioned&nbsp;in&nbsp;the&nbsp;Introduction,&nbsp;the&nbsp;purpose&nbsp;of&nbsp;the&nbsp;AdversarialLearning&nbsp;class<br>
&nbsp;&nbsp;&nbsp;&nbsp;is&nbsp;to&nbsp;demonstrate&nbsp;probabilistic&nbsp;data&nbsp;modeling&nbsp;using&nbsp;Generative&nbsp;Adversarial<br>
&nbsp;&nbsp;&nbsp;&nbsp;Networks&nbsp;(GAN).&nbsp;&nbsp;GANs&nbsp;use&nbsp;Discriminator-Generator&nbsp;or&nbsp;Critic-Generator&nbsp;pairs&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;learn&nbsp;probabilistic&nbsp;data&nbsp;models&nbsp;that&nbsp;can&nbsp;subsequently&nbsp;be&nbsp;used&nbsp;to&nbsp;create&nbsp;new&nbsp;image<br>
&nbsp;&nbsp;&nbsp;&nbsp;instances&nbsp;that&nbsp;look&nbsp;surprisingly&nbsp;similar&nbsp;to&nbsp;those&nbsp;in&nbsp;the&nbsp;training&nbsp;set.&nbsp;&nbsp;At&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;moment,&nbsp;you&nbsp;will&nbsp;find&nbsp;the&nbsp;following&nbsp;three&nbsp;such&nbsp;pairs&nbsp;inside&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;AdversarialLearning&nbsp;class:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;&nbsp;Discriminator-Generator&nbsp;DG1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;---&nbsp;&nbsp;implements&nbsp;the&nbsp;DCGAN&nbsp;logic<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;&nbsp;Discriminator-Generator&nbsp;DG2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;---&nbsp;&nbsp;a&nbsp;slight&nbsp;modification&nbsp;of&nbsp;the&nbsp;previous<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.&nbsp;&nbsp;Critic-Generator&nbsp;CG1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;---&nbsp;&nbsp;implements&nbsp;the&nbsp;Wasserstein&nbsp;GAN&nbsp;logic<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.&nbsp;&nbsp;Critic-Generator&nbsp;CG2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;---&nbsp;&nbsp;adds&nbsp;the&nbsp;Gradient&nbsp;Penalty&nbsp;to&nbsp;the&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Wasserstein&nbsp;GAN&nbsp;logic.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;In&nbsp;the&nbsp;ExamplesAdversarialLearning&nbsp;directory&nbsp;of&nbsp;the&nbsp;distro&nbsp;you&nbsp;will&nbsp;see&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;following&nbsp;scripts&nbsp;that&nbsp;demonstrate&nbsp;adversarial&nbsp;learning&nbsp;as&nbsp;incorporated&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;above&nbsp;networks:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;&nbsp;dcgan_DG1.py&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;---&nbsp;&nbsp;demonstrates&nbsp;the&nbsp;DCGAN&nbsp;DG1<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;&nbsp;dcgan_DG2.py&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;---&nbsp;&nbsp;demonstrates&nbsp;the&nbsp;DCGAN&nbsp;DG2<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.&nbsp;&nbsp;wgan_CG1.py&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;---&nbsp;&nbsp;demonstrates&nbsp;the&nbsp;Wasserstein&nbsp;GAN&nbsp;CG1<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.&nbsp;&nbsp;wgan_with_gp_CG2.py&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;---&nbsp;&nbsp;demonstrates&nbsp;the&nbsp;Wasserstein&nbsp;GAN&nbsp;CG2<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;All&nbsp;of&nbsp;these&nbsp;scripts&nbsp;use&nbsp;the&nbsp;training&nbsp;dataset&nbsp;PurdueShapes5GAN&nbsp;that&nbsp;consists&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;20,000&nbsp;images&nbsp;containing&nbsp;randomly&nbsp;shaped,&nbsp;randomly&nbsp;colored,&nbsp;and&nbsp;randomly<br>
&nbsp;&nbsp;&nbsp;&nbsp;positioned&nbsp;objects&nbsp;in&nbsp;64x64&nbsp;arrays.&nbsp;&nbsp;The&nbsp;dataset&nbsp;comes&nbsp;in&nbsp;the&nbsp;form&nbsp;of&nbsp;a&nbsp;gzipped<br>
&nbsp;&nbsp;&nbsp;&nbsp;archive&nbsp;named&nbsp;"datasets_for_AdversarialLearning.tar.gz"&nbsp;that&nbsp;is&nbsp;provided&nbsp;under<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;link&nbsp;"Download&nbsp;the&nbsp;image&nbsp;dataset&nbsp;for&nbsp;AdversarialLearning"&nbsp;at&nbsp;the&nbsp;top&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;HTML&nbsp;version&nbsp;of&nbsp;this&nbsp;doc&nbsp;page.&nbsp;&nbsp;See&nbsp;the&nbsp;README&nbsp;in&nbsp;the&nbsp;ExamplesAdversarialLearning<br>
&nbsp;&nbsp;&nbsp;&nbsp;directory&nbsp;for&nbsp;how&nbsp;to&nbsp;unpack&nbsp;the&nbsp;archive.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:red; font-size:150%"><strong>    </strong></span><br>&nbsp;&nbsp;&nbsp;&nbsp;GenerativeDiffusion<br>
&nbsp;&nbsp;&nbsp;&nbsp;===============<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;During&nbsp;the&nbsp;last&nbsp;couple&nbsp;of&nbsp;years,&nbsp;Denoising&nbsp;Diffusion&nbsp;has&nbsp;emerged&nbsp;as&nbsp;a&nbsp;strong<br>
&nbsp;&nbsp;&nbsp;&nbsp;alternative&nbsp;to&nbsp;generative&nbsp;data&nbsp;modeling.&nbsp;&nbsp;As&nbsp;mentioned&nbsp;previously&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;Introduction&nbsp;section&nbsp;on&nbsp;this&nbsp;webpage,&nbsp;learning&nbsp;a&nbsp;data&nbsp;model&nbsp;through&nbsp;diffusion<br>
&nbsp;&nbsp;&nbsp;&nbsp;involves&nbsp;two&nbsp;Markov&nbsp;chains,&nbsp;one&nbsp;that&nbsp;incrementally&nbsp;diffuses&nbsp;a&nbsp;training&nbsp;image<br>
&nbsp;&nbsp;&nbsp;&nbsp;until&nbsp;it&nbsp;turns&nbsp;into&nbsp;pure&nbsp;noise,&nbsp;and&nbsp;the&nbsp;other&nbsp;that&nbsp;incrementally&nbsp;denoises&nbsp;pure<br>
&nbsp;&nbsp;&nbsp;&nbsp;noise&nbsp;until&nbsp;what&nbsp;you&nbsp;see&nbsp;is&nbsp;something&nbsp;like&nbsp;an&nbsp;image&nbsp;in&nbsp;your&nbsp;training&nbsp;dataset.<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;former&nbsp;is&nbsp;called&nbsp;the&nbsp;q-chain&nbsp;and&nbsp;the&nbsp;latter&nbsp;the&nbsp;p-chain.&nbsp;&nbsp;The&nbsp;incremental<br>
&nbsp;&nbsp;&nbsp;&nbsp;diffusion&nbsp;in&nbsp;the&nbsp;q-chain&nbsp;is&nbsp;with&nbsp;known&nbsp;amount&nbsp;of&nbsp;Gaussian&nbsp;isotropic&nbsp;noise.&nbsp;&nbsp;In<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;p-chain,&nbsp;on&nbsp;the&nbsp;other&nbsp;hand,&nbsp;the&nbsp;goal&nbsp;is&nbsp;for&nbsp;a&nbsp;neural&nbsp;network&nbsp;to&nbsp;learn&nbsp;from<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;diffusion&nbsp;carried&nbsp;out&nbsp;in&nbsp;the&nbsp;q-chain&nbsp;how&nbsp;to&nbsp;carry&nbsp;out&nbsp;a&nbsp;denoising&nbsp;operation<br>
&nbsp;&nbsp;&nbsp;&nbsp;that&nbsp;would&nbsp;amount&nbsp;to&nbsp;a&nbsp;reversal&nbsp;of&nbsp;that&nbsp;diffusion.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;All&nbsp;of&nbsp;the&nbsp;key&nbsp;elements&nbsp;of&nbsp;the&nbsp;code&nbsp;that&nbsp;I&nbsp;have&nbsp;presented&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;GenerativeDiffusion&nbsp;module&nbsp;are&nbsp;extracted&nbsp;from&nbsp;OpenAI's&nbsp;"Improved&nbsp;Diffusion"<br>
&nbsp;&nbsp;&nbsp;&nbsp;project&nbsp;at&nbsp;GitHub&nbsp;that&nbsp;presents&nbsp;a&nbsp;PyTorch&nbsp;implementation&nbsp;of&nbsp;the&nbsp;work&nbsp;authored&nbsp;by<br>
&nbsp;&nbsp;&nbsp;&nbsp;Nichol&nbsp;and&nbsp;Dhariwal&nbsp;in&nbsp;their&nbsp;very&nbsp;famous&nbsp;paper&nbsp;"Improved&nbsp;Denoising&nbsp;Diffusion<br>
&nbsp;&nbsp;&nbsp;&nbsp;Probabilistic&nbsp;Models".&nbsp;See&nbsp;the&nbsp;beginning&nbsp;part&nbsp;of&nbsp;the&nbsp;doc&nbsp;page&nbsp;for&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;GenerativeDiffusion&nbsp;module&nbsp;for&nbsp;URLs&nbsp;to&nbsp;the&nbsp;GitHub&nbsp;code&nbsp;and&nbsp;their&nbsp;publication.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;you&nbsp;want&nbsp;to&nbsp;play&nbsp;with&nbsp;the&nbsp;code&nbsp;in&nbsp;GenerativeDiffusion,&nbsp;your&nbsp;starting&nbsp;point<br>
&nbsp;&nbsp;&nbsp;&nbsp;should&nbsp;be&nbsp;the&nbsp;README&nbsp;in&nbsp;the&nbsp;ExamplesDiffusion&nbsp;directory&nbsp;of&nbsp;DLStudio&nbsp;distribution.<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;script&nbsp;RunCodeForDiffusion.py&nbsp;in&nbsp;that&nbsp;directory&nbsp;is&nbsp;what&nbsp;you&nbsp;will&nbsp;need&nbsp;to&nbsp;use<br>
&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;train&nbsp;the&nbsp;model&nbsp;for&nbsp;your&nbsp;own&nbsp;dataset.&nbsp;&nbsp;As&nbsp;mentioned&nbsp;earlier,&nbsp;the&nbsp;goal&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;training&nbsp;is&nbsp;to&nbsp;make&nbsp;the&nbsp;neural&nbsp;network&nbsp;adept&nbsp;at&nbsp;estimating&nbsp;the&nbsp;p-chain&nbsp;transition<br>
&nbsp;&nbsp;&nbsp;&nbsp;probabilities&nbsp;p(&nbsp;x_{t-1}&nbsp;|&nbsp;x_t&nbsp;)&nbsp;at&nbsp;all&nbsp;timesteps.&nbsp;&nbsp;Once&nbsp;you&nbsp;have&nbsp;finished<br>
&nbsp;&nbsp;&nbsp;&nbsp;training,&nbsp;you&nbsp;would&nbsp;need&nbsp;to&nbsp;execute&nbsp;the&nbsp;script&nbsp;GenerateNewImageSamples.py&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;generating&nbsp;new&nbsp;images.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:red; font-size:150%"><strong>    </strong></span><br>&nbsp;&nbsp;&nbsp;&nbsp;Seq2SeqLearning:<br>
&nbsp;&nbsp;&nbsp;&nbsp;===========<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;As&nbsp;mentioned&nbsp;earlier&nbsp;in&nbsp;the&nbsp;Introduction,&nbsp;sequence-to-sequence&nbsp;learning&nbsp;(seq2seq)<br>
&nbsp;&nbsp;&nbsp;&nbsp;is&nbsp;about&nbsp;predicting&nbsp;an&nbsp;outcome&nbsp;sequence&nbsp;from&nbsp;a&nbsp;causation&nbsp;sequence,&nbsp;or,&nbsp;said<br>
&nbsp;&nbsp;&nbsp;&nbsp;another&nbsp;way,&nbsp;a&nbsp;target&nbsp;sequence&nbsp;from&nbsp;a&nbsp;source&nbsp;sequence.&nbsp;&nbsp;Automatic&nbsp;machine<br>
&nbsp;&nbsp;&nbsp;&nbsp;translation&nbsp;is&nbsp;probably&nbsp;one&nbsp;of&nbsp;the&nbsp;most&nbsp;popular&nbsp;applications&nbsp;of&nbsp;seq2seq.<br>
&nbsp;&nbsp;&nbsp;&nbsp;DLStudio&nbsp;uses&nbsp;English-to-Spanish&nbsp;translation&nbsp;to&nbsp;illustrate&nbsp;the&nbsp;programming&nbsp;idioms<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;the&nbsp;PyTorch&nbsp;structures&nbsp;you&nbsp;would&nbsp;need&nbsp;for&nbsp;writing&nbsp;your&nbsp;own&nbsp;code&nbsp;for&nbsp;seq2seq.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Any&nbsp;attempt&nbsp;at&nbsp;seq2seq&nbsp;for&nbsp;machine&nbsp;translation&nbsp;must&nbsp;answer&nbsp;the&nbsp;following&nbsp;question<br>
&nbsp;&nbsp;&nbsp;&nbsp;at&nbsp;the&nbsp;outset:&nbsp;How&nbsp;to&nbsp;represent&nbsp;the&nbsp;words&nbsp;of&nbsp;a&nbsp;language&nbsp;for&nbsp;neural-network&nbsp;based<br>
&nbsp;&nbsp;&nbsp;&nbsp;processing?&nbsp;In&nbsp;general,&nbsp;you&nbsp;have&nbsp;two&nbsp;options:&nbsp;(1)&nbsp;Have&nbsp;your&nbsp;overall&nbsp;network&nbsp;learn<br>
&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;its&nbsp;own&nbsp;what&nbsp;are&nbsp;known&nbsp;as&nbsp;vector&nbsp;embeddings&nbsp;for&nbsp;the&nbsp;words;&nbsp;or&nbsp;(2)&nbsp;Use<br>
&nbsp;&nbsp;&nbsp;&nbsp;pre-trained&nbsp;embeddings&nbsp;as&nbsp;provided&nbsp;by&nbsp;word2vec&nbsp;or&nbsp;Fasttext.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;After&nbsp;you&nbsp;have&nbsp;resolved&nbsp;the&nbsp;issue&nbsp;of&nbsp;word&nbsp;representation,&nbsp;your&nbsp;next&nbsp;challenge&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;how&nbsp;to&nbsp;implement&nbsp;the&nbsp;attention&nbsp;mechanism&nbsp;that&nbsp;you're&nbsp;going&nbsp;to&nbsp;need&nbsp;for&nbsp;aligning<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;similar&nbsp;grammatical&nbsp;units&nbsp;in&nbsp;the&nbsp;two&nbsp;languages.&nbsp;The&nbsp;seq2seq&nbsp;code&nbsp;demonstrated<br>
&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;this&nbsp;module&nbsp;uses&nbsp;the&nbsp;attention&nbsp;model&nbsp;proposed&nbsp;by&nbsp;Bahdanau,&nbsp;Cho,&nbsp;and&nbsp;Bengio&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;form&nbsp;of&nbsp;a&nbsp;separate&nbsp;Attention&nbsp;class.&nbsp;&nbsp;The&nbsp;name&nbsp;of&nbsp;this&nbsp;attention&nbsp;class&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;Attention_BCB.&nbsp;&nbsp;In&nbsp;a&nbsp;separate&nbsp;attention&nbsp;class&nbsp;named&nbsp;Attention_SR,&nbsp;I&nbsp;have&nbsp;also<br>
&nbsp;&nbsp;&nbsp;&nbsp;included&nbsp;the&nbsp;attention&nbsp;mechanism&nbsp;used&nbsp;by&nbsp;Sean&nbsp;Robertson&nbsp;in&nbsp;his&nbsp;very&nbsp;popular&nbsp;NLP<br>
&nbsp;&nbsp;&nbsp;&nbsp;tutorial&nbsp;at&nbsp;the&nbsp;main&nbsp;PyTorch&nbsp;website.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Seq2SeqLearning&nbsp;contains&nbsp;the&nbsp;following&nbsp;two&nbsp;inner&nbsp;classes&nbsp;for&nbsp;illustrating<br>
&nbsp;&nbsp;&nbsp;&nbsp;seq2seq:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;&nbsp;Seq2SeqWithLearnableEmbeddings<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;&nbsp;Seq2SeqWithPretrainedEmbeddings<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;In&nbsp;the&nbsp;first&nbsp;of&nbsp;these,&nbsp;Seq2SeqWithLearnableEmbeddings,&nbsp;the&nbsp;words&nbsp;embeddings&nbsp;are<br>
&nbsp;&nbsp;&nbsp;&nbsp;learned&nbsp;automatically&nbsp;by&nbsp;using&nbsp;the&nbsp;nn.Embeddings&nbsp;layer.&nbsp;On&nbsp;the&nbsp;other&nbsp;hand,&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;Seq2SeqWithPretrainedEmbeddings,&nbsp;I&nbsp;have&nbsp;used&nbsp;the&nbsp;word2vec&nbsp;embeddings&nbsp;for&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;source&nbsp;language&nbsp;English&nbsp;and&nbsp;allowed&nbsp;the&nbsp;system&nbsp;to&nbsp;learn&nbsp;the&nbsp;embeddings&nbsp;for&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;target&nbsp;language&nbsp;Spanish.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;In&nbsp;order&nbsp;to&nbsp;become&nbsp;familiar&nbsp;with&nbsp;these&nbsp;classes,&nbsp;your&nbsp;best&nbsp;entry&nbsp;points&nbsp;would&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;following&nbsp;scripts&nbsp;in&nbsp;the&nbsp;ExamplesSeq2SeqLearning&nbsp;directory:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;seq2seq_with_learnable_embeddings.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;seq2seq_with_pretrained_embeddings.py<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:red; font-size:150%"><strong>    </strong></span><br>&nbsp;&nbsp;&nbsp;&nbsp;DataPrediction<br>
&nbsp;&nbsp;&nbsp;&nbsp;==========<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;As&nbsp;mentioned&nbsp;earlier&nbsp;in&nbsp;the&nbsp;Introduction,&nbsp;time-series&nbsp;data&nbsp;prediction&nbsp;differs<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;the&nbsp;more&nbsp;symbolic&nbsp;sequence-based&nbsp;learning&nbsp;frameworks&nbsp;with&nbsp;regard&nbsp;to&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;following:&nbsp;(1)&nbsp;Data&nbsp;normalization;&nbsp;(2)&nbsp;Data&nbsp;chunking;&nbsp;and&nbsp;(3)&nbsp;Datetime<br>
&nbsp;&nbsp;&nbsp;&nbsp;conditioning.&nbsp;The&nbsp;reason&nbsp;I&nbsp;mention&nbsp;data&nbsp;normalization&nbsp;is&nbsp;that&nbsp;now&nbsp;you&nbsp;have&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;remember&nbsp;the&nbsp;scaling&nbsp;parameters&nbsp;used&nbsp;for&nbsp;data&nbsp;normalization&nbsp;since&nbsp;you&nbsp;are&nbsp;going<br>
&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;need&nbsp;to&nbsp;inverse-normalize&nbsp;the&nbsp;predicted&nbsp;values.&nbsp;You&nbsp;would&nbsp;want&nbsp;to&nbsp;your<br>
&nbsp;&nbsp;&nbsp;&nbsp;predicted&nbsp;values&nbsp;to&nbsp;be&nbsp;at&nbsp;the&nbsp;same&nbsp;scale&nbsp;as&nbsp;the&nbsp;time-series&nbsp;observations.&nbsp;&nbsp;The<br>
&nbsp;&nbsp;&nbsp;&nbsp;second&nbsp;issue,&nbsp;data&nbsp;chunking,&nbsp;refers&nbsp;to&nbsp;the&nbsp;fact&nbsp;that&nbsp;the&nbsp;notion&nbsp;of&nbsp;a&nbsp;"sentence"<br>
&nbsp;&nbsp;&nbsp;&nbsp;does&nbsp;not&nbsp;exist&nbsp;in&nbsp;time-series&nbsp;data.&nbsp;&nbsp;What&nbsp;that&nbsp;implies&nbsp;that&nbsp;the&nbsp;user&nbsp;has&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;decide&nbsp;how&nbsp;to&nbsp;extract&nbsp;sequences&nbsp;from&nbsp;arbitrary&nbsp;long&nbsp;time-series&nbsp;data&nbsp;for&nbsp;training<br>
&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;prediction&nbsp;framework.&nbsp;&nbsp;Finally,&nbsp;the&nbsp;the&nbsp;third&nbsp;issue,&nbsp;datetime&nbsp;conditioning,<br>
&nbsp;&nbsp;&nbsp;&nbsp;refers&nbsp;to&nbsp;creating&nbsp;a&nbsp;multi-dimensional&nbsp;encoding&nbsp;for&nbsp;the&nbsp;datetime&nbsp;stamp&nbsp;associated<br>
&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;each&nbsp;observation&nbsp;to&nbsp;account&nbsp;for&nbsp;the&nbsp;diurnal,&nbsp;weekly,&nbsp;seasonal,&nbsp;and&nbsp;other<br>
&nbsp;&nbsp;&nbsp;&nbsp;such&nbsp;temporal&nbsp;effects.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;data&nbsp;prediction&nbsp;framework&nbsp;in&nbsp;the&nbsp;DataPrediction&nbsp;part&nbsp;of&nbsp;DLStudio&nbsp;is&nbsp;based&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;following&nbsp;inner&nbsp;class:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pmGRU<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;"Poor&nbsp;Man's&nbsp;GRU".&nbsp;&nbsp;This&nbsp;GRU&nbsp;is&nbsp;my&nbsp;implementation&nbsp;of&nbsp;the&nbsp;"Minimal&nbsp;Gated&nbsp;Unit"<br>
&nbsp;&nbsp;&nbsp;&nbsp;GRU&nbsp;variant&nbsp;that&nbsp;was&nbsp;first&nbsp;presented&nbsp;by&nbsp;Joel&nbsp;Heck&nbsp;and&nbsp;Fathi&nbsp;Salem&nbsp;in&nbsp;their&nbsp;paper<br>
&nbsp;&nbsp;&nbsp;&nbsp;"Simplified&nbsp;Minimal&nbsp;Gated&nbsp;Unit&nbsp;Variations&nbsp;for&nbsp;Recurrent&nbsp;Neural&nbsp;Networks"&nbsp;and&nbsp;it<br>
&nbsp;&nbsp;&nbsp;&nbsp;combines&nbsp;the&nbsp;Update&nbsp;and&nbsp;the&nbsp;Reset&nbsp;gates&nbsp;of&nbsp;a&nbsp;regular&nbsp;GRU&nbsp;into&nbsp;a&nbsp;single&nbsp;gate<br>
&nbsp;&nbsp;&nbsp;&nbsp;called&nbsp;the&nbsp;Forget&nbsp;Gate.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;My&nbsp;reason&nbsp;for&nbsp;using&nbsp;pmGRU&nbsp;is&nbsp;purely&nbsp;educational.&nbsp;While&nbsp;you&nbsp;are&nbsp;likely&nbsp;to&nbsp;use<br>
&nbsp;&nbsp;&nbsp;&nbsp;PyTorch's&nbsp;GRU&nbsp;for&nbsp;any&nbsp;production&nbsp;work&nbsp;that&nbsp;requires&nbsp;a&nbsp;GRU,&nbsp;using&nbsp;a&nbsp;pre-programmed<br>
&nbsp;&nbsp;&nbsp;&nbsp;piece&nbsp;of&nbsp;code&nbsp;makes&nbsp;it&nbsp;more&nbsp;difficult&nbsp;to&nbsp;gain&nbsp;insights&nbsp;into&nbsp;how&nbsp;the&nbsp;logic&nbsp;of&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;GRU&nbsp;(especially&nbsp;with&nbsp;regard&nbsp;to&nbsp;the&nbsp;gating&nbsp;action&nbsp;it&nbsp;needs)&nbsp;is&nbsp;actually<br>
&nbsp;&nbsp;&nbsp;&nbsp;implemented.&nbsp;&nbsp;The&nbsp;implementation&nbsp;code&nbsp;shown&nbsp;for&nbsp;pmGRU&nbsp;is&nbsp;supposed&nbsp;to&nbsp;help&nbsp;remedy<br>
&nbsp;&nbsp;&nbsp;&nbsp;that.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;As&nbsp;I&nbsp;mentioned&nbsp;in&nbsp;the&nbsp;Introduction,&nbsp;your&nbsp;main&nbsp;entry&nbsp;point&nbsp;for&nbsp;experimenting&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;data&nbsp;prediction&nbsp;is&nbsp;the&nbsp;following&nbsp;script&nbsp;in&nbsp;the&nbsp;ExamplesDataPrediction&nbsp;directory<br>
&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;the&nbsp;DLStudio&nbsp;distribution:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;power_load_prediction_with_pmGRU.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;However,&nbsp;before&nbsp;you&nbsp;can&nbsp;run&nbsp;this&nbsp;script,&nbsp;you&nbsp;would&nbsp;need&nbsp;to&nbsp;download&nbsp;the&nbsp;training<br>
&nbsp;&nbsp;&nbsp;&nbsp;dataset&nbsp;used&nbsp;in&nbsp;this&nbsp;example.&nbsp;&nbsp;See&nbsp;the&nbsp;"For&nbsp;Data&nbsp;Prediction"&nbsp;part&nbsp;of&nbsp;the&nbsp;"The<br>
&nbsp;&nbsp;&nbsp;&nbsp;Datasets&nbsp;Included"&nbsp;section&nbsp;of&nbsp;the&nbsp;doc&nbsp;page&nbsp;for&nbsp;that.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:red; font-size:150%"><strong>    </strong></span><br>&nbsp;&nbsp;&nbsp;&nbsp;Transformers<br>
&nbsp;&nbsp;&nbsp;&nbsp;========<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;code&nbsp;in&nbsp;this&nbsp;module&nbsp;of&nbsp;DLStudio&nbsp;consists&nbsp;of&nbsp;three&nbsp;different&nbsp;implementations<br>
&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;the&nbsp;transformer&nbsp;architecture:&nbsp;(1)&nbsp;TransformerFG,&nbsp;(2)&nbsp;TransformerPreLN,&nbsp;and&nbsp;(3)<br>
&nbsp;&nbsp;&nbsp;&nbsp;visTransformer.&nbsp;&nbsp;The&nbsp;first&nbsp;two&nbsp;of&nbsp;these&nbsp;are&nbsp;meant&nbsp;for&nbsp;seq2seq&nbsp;learning,&nbsp;as&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;language&nbsp;translation,&nbsp;and&nbsp;the&nbsp;last&nbsp;is&nbsp;for&nbsp;solving&nbsp;the&nbsp;problem&nbsp;of&nbsp;image<br>
&nbsp;&nbsp;&nbsp;&nbsp;recognition.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;TransformerFG&nbsp;is&nbsp;my&nbsp;implementation&nbsp;of&nbsp;the&nbsp;architecture&nbsp;as&nbsp;conceptualized&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;famous&nbsp;paper&nbsp;"Attention&nbsp;is&nbsp;All&nbsp;You&nbsp;Need"&nbsp;by&nbsp;Vaswani&nbsp;et&nbsp;el.&nbsp;&nbsp;And&nbsp;TransformerPreLN<br>
&nbsp;&nbsp;&nbsp;&nbsp;is&nbsp;my&nbsp;implementation&nbsp;of&nbsp;the&nbsp;original&nbsp;idea&nbsp;along&nbsp;with&nbsp;the&nbsp;modifications&nbsp;suggested<br>
&nbsp;&nbsp;&nbsp;&nbsp;by&nbsp;Xiong&nbsp;et&nbsp;al.&nbsp;in&nbsp;their&nbsp;paper&nbsp;"On&nbsp;Layer&nbsp;Normalization&nbsp;in&nbsp;the&nbsp;Transformer<br>
&nbsp;&nbsp;&nbsp;&nbsp;Architecture"&nbsp;for&nbsp;more&nbsp;stable&nbsp;learning.&nbsp;&nbsp;The&nbsp;two&nbsp;versions&nbsp;of&nbsp;transformers&nbsp;differ<br>
&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;only&nbsp;one&nbsp;respect:&nbsp;The&nbsp;placement&nbsp;of&nbsp;the&nbsp;LayerNorm&nbsp;in&nbsp;relation&nbsp;to&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;architectural&nbsp;components&nbsp;related&nbsp;to&nbsp;attention&nbsp;and&nbsp;the&nbsp;feedforward&nbsp;network.<br>
&nbsp;&nbsp;&nbsp;&nbsp;Literally,&nbsp;the&nbsp;difference&nbsp;is&nbsp;small,&nbsp;yet&nbsp;its&nbsp;consequences&nbsp;are&nbsp;significant<br>
&nbsp;&nbsp;&nbsp;&nbsp;regarding&nbsp;the&nbsp;stability&nbsp;of&nbsp;learning.&nbsp;&nbsp;Finally,&nbsp;visTransformer&nbsp;is&nbsp;my<br>
&nbsp;&nbsp;&nbsp;&nbsp;implementation&nbsp;of&nbsp;the&nbsp;Vision&nbsp;Transformer&nbsp;as&nbsp;presented&nbsp;in&nbsp;the&nbsp;famous&nbsp;paper&nbsp;"An<br>
&nbsp;&nbsp;&nbsp;&nbsp;Image&nbsp;is&nbsp;Worth&nbsp;16x16$&nbsp;Words:&nbsp;Transformers&nbsp;for&nbsp;Image&nbsp;Recognition&nbsp;at&nbsp;Scale''&nbsp;by<br>
&nbsp;&nbsp;&nbsp;&nbsp;Dosovitskiy&nbsp;et&nbsp;al.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;fundamentals&nbsp;of&nbsp;how&nbsp;the&nbsp;attention&nbsp;works&nbsp;in&nbsp;all&nbsp;three&nbsp;transformer&nbsp;based<br>
&nbsp;&nbsp;&nbsp;&nbsp;implementations&nbsp;in&nbsp;the&nbsp;Transformers&nbsp;module&nbsp;are&nbsp;exactly&nbsp;the&nbsp;same.&nbsp;&nbsp;For<br>
&nbsp;&nbsp;&nbsp;&nbsp;self-attention,&nbsp;you&nbsp;associate&nbsp;a&nbsp;Query&nbsp;Vector&nbsp;Q_i&nbsp;and&nbsp;a&nbsp;Key&nbsp;Vector&nbsp;K_i&nbsp;with&nbsp;each<br>
&nbsp;&nbsp;&nbsp;&nbsp;word&nbsp;w_i&nbsp;in&nbsp;a&nbsp;sentence.&nbsp;&nbsp;For&nbsp;a&nbsp;given&nbsp;w_i,&nbsp;the&nbsp;dot&nbsp;product&nbsp;of&nbsp;its&nbsp;Q_i&nbsp;with&nbsp;the&nbsp;K_j<br>
&nbsp;&nbsp;&nbsp;&nbsp;vectors&nbsp;for&nbsp;all&nbsp;the&nbsp;other&nbsp;words&nbsp;w_j&nbsp;is&nbsp;a&nbsp;measure&nbsp;of&nbsp;how&nbsp;related&nbsp;w_i&nbsp;is&nbsp;to&nbsp;each<br>
&nbsp;&nbsp;&nbsp;&nbsp;w_j&nbsp;with&nbsp;regard&nbsp;to&nbsp;what's&nbsp;needed&nbsp;for&nbsp;the&nbsp;translation&nbsp;of&nbsp;a&nbsp;source&nbsp;sentence&nbsp;into<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;target&nbsp;sentence.&nbsp;&nbsp;One&nbsp;more&nbsp;vector&nbsp;you&nbsp;associate&nbsp;with&nbsp;each&nbsp;word&nbsp;w_i&nbsp;is&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;Value&nbsp;Vector&nbsp;V_i.&nbsp;&nbsp;The&nbsp;value&nbsp;vectors&nbsp;for&nbsp;the&nbsp;words&nbsp;in&nbsp;a&nbsp;sentence&nbsp;are&nbsp;weighted&nbsp;by<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;output&nbsp;of&nbsp;the&nbsp;activation&nbsp;nn.LogSoftmax&nbsp;applied&nbsp;to&nbsp;the&nbsp;dot-products.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;self-attention&nbsp;mechanism&nbsp;described&nbsp;above&nbsp;is&nbsp;half&nbsp;of&nbsp;what&nbsp;goes&nbsp;into&nbsp;each&nbsp;base<br>
&nbsp;&nbsp;&nbsp;&nbsp;encoder&nbsp;of&nbsp;a&nbsp;transformer,&nbsp;the&nbsp;other&nbsp;half&nbsp;is&nbsp;a&nbsp;feedforward&nbsp;network&nbsp;(FFN).&nbsp;The<br>
&nbsp;&nbsp;&nbsp;&nbsp;overall&nbsp;encoder&nbsp;consists&nbsp;of&nbsp;a&nbsp;cascade&nbsp;of&nbsp;these&nbsp;base&nbsp;encoders.&nbsp;&nbsp;In&nbsp;my<br>
&nbsp;&nbsp;&nbsp;&nbsp;implementation,&nbsp;I&nbsp;have&nbsp;referred&nbsp;to&nbsp;the&nbsp;overall&nbsp;encoder&nbsp;as&nbsp;the&nbsp;MasterEncoder.&nbsp;&nbsp;The<br>
&nbsp;&nbsp;&nbsp;&nbsp;MasterDecoder&nbsp;also&nbsp;consists&nbsp;of&nbsp;a&nbsp;cascade&nbsp;of&nbsp;base&nbsp;decoders.&nbsp;&nbsp;A&nbsp;base&nbsp;decoder&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;similar&nbsp;to&nbsp;a&nbsp;base&nbsp;encoder&nbsp;except&nbsp;for&nbsp;there&nbsp;being&nbsp;a&nbsp;layer&nbsp;of&nbsp;cross-attention<br>
&nbsp;&nbsp;&nbsp;&nbsp;interposed&nbsp;between&nbsp;the&nbsp;self-attention&nbsp;layer&nbsp;and&nbsp;the&nbsp;feedforward&nbsp;network.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Referring&nbsp;to&nbsp;the&nbsp;attention&nbsp;half&nbsp;of&nbsp;each&nbsp;base&nbsp;encoder&nbsp;or&nbsp;a&nbsp;decoder&nbsp;as&nbsp;one<br>
&nbsp;&nbsp;&nbsp;&nbsp;half-unit&nbsp;and&nbsp;the&nbsp;FFN&nbsp;as&nbsp;the&nbsp;other&nbsp;half&nbsp;unit,&nbsp;the&nbsp;problem&nbsp;of&nbsp;vanishing&nbsp;gradients<br>
&nbsp;&nbsp;&nbsp;&nbsp;that&nbsp;would&nbsp;otherwise&nbsp;be&nbsp;caused&nbsp;by&nbsp;the&nbsp;depth&nbsp;of&nbsp;the&nbsp;overall&nbsp;network&nbsp;is&nbsp;mitigated<br>
&nbsp;&nbsp;&nbsp;&nbsp;by&nbsp;using&nbsp;LayerNorm&nbsp;and&nbsp;residual&nbsp;connections.&nbsp;&nbsp;In&nbsp;TransformerFG,&nbsp;on&nbsp;the&nbsp;encoder<br>
&nbsp;&nbsp;&nbsp;&nbsp;side,&nbsp;LayerNorm&nbsp;is&nbsp;applied&nbsp;to&nbsp;the&nbsp;output&nbsp;of&nbsp;the&nbsp;self-attention&nbsp;layer&nbsp;and&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;residual&nbsp;connection&nbsp;wraps&nbsp;around&nbsp;both.&nbsp;&nbsp;Along&nbsp;the&nbsp;same&nbsp;lines,&nbsp;LayerNorm&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;applied&nbsp;to&nbsp;the&nbsp;output&nbsp;of&nbsp;FFN&nbsp;and&nbsp;the&nbsp;residual&nbsp;connection&nbsp;wraps&nbsp;around&nbsp;both.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;In&nbsp;TransformerPreLN,&nbsp;on&nbsp;the&nbsp;other&nbsp;hand,&nbsp;LayerNorm&nbsp;is&nbsp;applied&nbsp;to&nbsp;the&nbsp;input&nbsp;to&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;self-attention&nbsp;layer&nbsp;and&nbsp;residual&nbsp;connection&nbsp;wraps&nbsp;around&nbsp;both.&nbsp;&nbsp;Similarly,<br>
&nbsp;&nbsp;&nbsp;&nbsp;LayerNorm&nbsp;is&nbsp;applied&nbsp;to&nbsp;the&nbsp;input&nbsp;to&nbsp;FFN&nbsp;and&nbsp;the&nbsp;residual&nbsp;connection&nbsp;wraps&nbsp;around<br>
&nbsp;&nbsp;&nbsp;&nbsp;both.&nbsp;&nbsp;Similar&nbsp;considerations&nbsp;applied&nbsp;to&nbsp;the&nbsp;decoder&nbsp;side,&nbsp;except&nbsp;we&nbsp;now&nbsp;also<br>
&nbsp;&nbsp;&nbsp;&nbsp;have&nbsp;a&nbsp;layer&nbsp;of&nbsp;cross-attention&nbsp;interposed&nbsp;between&nbsp;the&nbsp;self-attention&nbsp;and&nbsp;FFN.<br>
&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;As&nbsp;I&nbsp;mentioned&nbsp;in&nbsp;the&nbsp;Introduction,&nbsp;your&nbsp;main&nbsp;entry&nbsp;point&nbsp;for&nbsp;experimenting&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;seq2seq&nbsp;based&nbsp;transformer&nbsp;code&nbsp;in&nbsp;DLStudio&nbsp;are&nbsp;the&nbsp;following&nbsp;two&nbsp;scripts&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;ExamplesTransformers&nbsp;directory&nbsp;of&nbsp;the&nbsp;distribution:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;seq2seq_with_transformerFG.py<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;seq2seq_with_transformerPreLN.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;However,&nbsp;before&nbsp;you&nbsp;can&nbsp;run&nbsp;these&nbsp;scripts,&nbsp;you&nbsp;would&nbsp;need&nbsp;to&nbsp;download&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;training&nbsp;dataset&nbsp;used&nbsp;in&nbsp;these&nbsp;examples.&nbsp;&nbsp;See&nbsp;the&nbsp;"For&nbsp;Transformers"&nbsp;part&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;"The&nbsp;Datasets&nbsp;Included"&nbsp;section&nbsp;of&nbsp;this&nbsp;doc&nbsp;page&nbsp;for&nbsp;that.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;And&nbsp;your&nbsp;main&nbsp;entry&nbsp;point&nbsp;for&nbsp;experimenting&nbsp;with&nbsp;image&nbsp;recognition&nbsp;by&nbsp;playing<br>
&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;the&nbsp;visTransformer&nbsp;class&nbsp;are&nbsp;the&nbsp;scripts:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;image_recog_with_visTransformer.py<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;test_checkpoint_for_visTransformer.py&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Both&nbsp;these&nbsp;script&nbsp;use&nbsp;the&nbsp;CIFAR10&nbsp;dataset&nbsp;for&nbsp;demonstrating&nbsp;image&nbsp;recognition.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:red; font-size:150%"><strong>    </strong></span><br>&nbsp;&nbsp;&nbsp;&nbsp;MetricLearning:<br>
&nbsp;&nbsp;&nbsp;&nbsp;=============<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;As&nbsp;mentioned&nbsp;in&nbsp;the&nbsp;Introduction,&nbsp;the&nbsp;main&nbsp;idea&nbsp;of&nbsp;metric&nbsp;learning&nbsp;is&nbsp;to&nbsp;learn&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;mapping&nbsp;from&nbsp;the&nbsp;images&nbsp;to&nbsp;their&nbsp;embedding&nbsp;vector&nbsp;representations&nbsp;in&nbsp;such&nbsp;a&nbsp;way<br>
&nbsp;&nbsp;&nbsp;&nbsp;that&nbsp;the&nbsp;embeddings&nbsp;for&nbsp;what&nbsp;are&nbsp;supposed&nbsp;to&nbsp;be&nbsp;similar&nbsp;images&nbsp;are&nbsp;pulled<br>
&nbsp;&nbsp;&nbsp;&nbsp;together&nbsp;and&nbsp;those&nbsp;for&nbsp;dissimilar&nbsp;images&nbsp;are&nbsp;pulled&nbsp;as&nbsp;far&nbsp;apart&nbsp;as&nbsp;possible.<br>
&nbsp;&nbsp;&nbsp;&nbsp;Two&nbsp;loss&nbsp;functions&nbsp;are&nbsp;commonly&nbsp;used&nbsp;for&nbsp;this&nbsp;type&nbsp;of&nbsp;deep&nbsp;learning:&nbsp;Pairwise<br>
&nbsp;&nbsp;&nbsp;&nbsp;Contrastive&nbsp;Loss&nbsp;and&nbsp;Triplet&nbsp;Loss.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;To&nbsp;calculate&nbsp;the&nbsp;Pairwise&nbsp;Contrastive&nbsp;Loss,&nbsp;you&nbsp;must&nbsp;first&nbsp;extract&nbsp;Positive&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;Negative&nbsp;Pairs&nbsp;from&nbsp;a&nbsp;batch.&nbsp;&nbsp;A&nbsp;Positive&nbsp;Pair&nbsp;means&nbsp;that&nbsp;both&nbsp;the&nbsp;embeddings&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;pair&nbsp;carry&nbsp;the&nbsp;same&nbsp;class&nbsp;label&nbsp;and&nbsp;a&nbsp;Negative&nbsp;Pair&nbsp;means&nbsp;that&nbsp;the&nbsp;two<br>
&nbsp;&nbsp;&nbsp;&nbsp;embeddings&nbsp;in&nbsp;the&nbsp;pair&nbsp;have&nbsp;dissimilar&nbsp;labels.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;From&nbsp;a&nbsp;programming&nbsp;standpoint,&nbsp;the&nbsp;challenge&nbsp;is&nbsp;how&nbsp;to&nbsp;form&nbsp;these&nbsp;pairs&nbsp;without<br>
&nbsp;&nbsp;&nbsp;&nbsp;scanning&nbsp;through&nbsp;a&nbsp;batch&nbsp;with&nbsp;'for'&nbsp;loops&nbsp;---&nbsp;since&nbsp;such&nbsp;loops&nbsp;are&nbsp;an&nbsp;anathema&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;any&nbsp;GPU&nbsp;based&nbsp;processing&nbsp;of&nbsp;data.&nbsp;What&nbsp;comes&nbsp;to&nbsp;our&nbsp;rescue&nbsp;are&nbsp;a&nbsp;combination&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;broadcast&nbsp;properties&nbsp;of&nbsp;tensors&nbsp;(inherited&nbsp;from&nbsp;numpy)&nbsp;and&nbsp;tensor-based<br>
&nbsp;&nbsp;&nbsp;&nbsp;Boolean&nbsp;logic.&nbsp;For&nbsp;example,&nbsp;by&nbsp;comparing&nbsp;a&nbsp;column&nbsp;tensor&nbsp;of&nbsp;the&nbsp;sample&nbsp;labels&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;batch&nbsp;with&nbsp;a&nbsp;row&nbsp;tensor&nbsp;of&nbsp;the&nbsp;same&nbsp;and&nbsp;testing&nbsp;for&nbsp;the&nbsp;equality&nbsp;of&nbsp;the&nbsp;sample<br>
&nbsp;&nbsp;&nbsp;&nbsp;labels,&nbsp;you&nbsp;instantly&nbsp;have&nbsp;a&nbsp;2D&nbsp;array&nbsp;whose&nbsp;(i,j)&nbsp;element&nbsp;is&nbsp;True&nbsp;if&nbsp;the&nbsp;i-th&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;j-th&nbsp;batch&nbsp;samples&nbsp;carry&nbsp;the&nbsp;same&nbsp;class&nbsp;label.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Even&nbsp;after&nbsp;you&nbsp;have&nbsp;constructed&nbsp;the&nbsp;Positive&nbsp;and&nbsp;the&nbsp;Negative&nbsp;Pairs&nbsp;from&nbsp;a&nbsp;batch,<br>
&nbsp;&nbsp;&nbsp;&nbsp;your&nbsp;next&nbsp;mini-challenge&nbsp;is&nbsp;to&nbsp;reformat&nbsp;the&nbsp;batch&nbsp;sample&nbsp;indices&nbsp;in&nbsp;the&nbsp;pairs&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;order&nbsp;to&nbsp;conform&nbsp;to&nbsp;the&nbsp;input&nbsp;requirements&nbsp;of&nbsp;PyTorch's&nbsp;loss&nbsp;function<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.nn.CosineEmbeddingLoss.&nbsp;&nbsp;The&nbsp;input&nbsp;consists&nbsp;of&nbsp;three&nbsp;tensors,&nbsp;the&nbsp;first&nbsp;two<br>
&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;which&nbsp;are&nbsp;of&nbsp;shape&nbsp;(N,M),&nbsp;where&nbsp;N&nbsp;is&nbsp;the&nbsp;total&nbsp;number&nbsp;of&nbsp;pairs&nbsp;extracted&nbsp;from<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;batch&nbsp;and&nbsp;M&nbsp;the&nbsp;size&nbsp;of&nbsp;the&nbsp;embedding&nbsp;vectors.&nbsp;The&nbsp;first&nbsp;such&nbsp;NxM&nbsp;tensor<br>
&nbsp;&nbsp;&nbsp;&nbsp;corresponds&nbsp;to&nbsp;the&nbsp;fist&nbsp;batch&nbsp;sample&nbsp;index&nbsp;in&nbsp;each&nbsp;pair.&nbsp;And&nbsp;the&nbsp;second&nbsp;such&nbsp;NxM<br>
&nbsp;&nbsp;&nbsp;&nbsp;tensor&nbsp;corresponds&nbsp;to&nbsp;the&nbsp;second&nbsp;batch&nbsp;sample&nbsp;index&nbsp;in&nbsp;each&nbsp;pair.&nbsp;The&nbsp;last&nbsp;tensor<br>
&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;the&nbsp;input&nbsp;args&nbsp;to&nbsp;the&nbsp;CosineEmbeddingLoss&nbsp;loss&nbsp;function&nbsp;is&nbsp;of&nbsp;shape&nbsp;Nx1,&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;which&nbsp;the&nbsp;individual&nbsp;values&nbsp;are&nbsp;either&nbsp;+1.0&nbsp;or&nbsp;-1.0,&nbsp;depending&nbsp;on&nbsp;whether&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;pair&nbsp;formed&nbsp;by&nbsp;the&nbsp;first&nbsp;two&nbsp;embeddings&nbsp;is&nbsp;a&nbsp;Positive&nbsp;Pair&nbsp;or&nbsp;a&nbsp;Negative&nbsp;Pair.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;For&nbsp;the&nbsp;Triplet&nbsp;Loss,&nbsp;you&nbsp;construct&nbsp;triplets&nbsp;of&nbsp;the&nbsp;samples&nbsp;in&nbsp;a&nbsp;batch&nbsp;in&nbsp;which<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;first&nbsp;two&nbsp;embeddings&nbsp;must&nbsp;carry&nbsp;the&nbsp;same&nbsp;class&nbsp;label&nbsp;and&nbsp;the&nbsp;label&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;third&nbsp;embedding&nbsp;must&nbsp;not&nbsp;be&nbsp;same&nbsp;as&nbsp;for&nbsp;the&nbsp;other&nbsp;two.&nbsp;&nbsp;Such&nbsp;a&nbsp;triplet&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;commonly&nbsp;denoted&nbsp;(Anchor,&nbsp;Pos,&nbsp;Neg).&nbsp;&nbsp;That&nbsp;is,&nbsp;you&nbsp;treat&nbsp;the&nbsp;first&nbsp;element&nbsp;as&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;Anchor,&nbsp;the&nbsp;second&nbsp;as&nbsp;the&nbsp;Positive&nbsp;and&nbsp;the&nbsp;third&nbsp;as&nbsp;the&nbsp;Negative.&nbsp;&nbsp;A&nbsp;triplet&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;formed&nbsp;only&nbsp;if&nbsp;the&nbsp;distance&nbsp;between&nbsp;the&nbsp;Anchor&nbsp;and&nbsp;the&nbsp;Neg&nbsp;is&nbsp;greater&nbsp;than&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;distance&nbsp;between&nbsp;the&nbsp;Anchor&nbsp;and&nbsp;the&nbsp;Pos.&nbsp;&nbsp;We&nbsp;want&nbsp;all&nbsp;such&nbsp;Neg&nbsp;element&nbsp;to&nbsp;get<br>
&nbsp;&nbsp;&nbsp;&nbsp;farther&nbsp;away&nbsp;from&nbsp;the&nbsp;Anchor&nbsp;compared&nbsp;to&nbsp;how&nbsp;far&nbsp;the&nbsp;Pos&nbsp;element&nbsp;is&nbsp;---&nbsp;but&nbsp;no<br>
&nbsp;&nbsp;&nbsp;&nbsp;farther&nbsp;than&nbsp;what's&nbsp;known&nbsp;as&nbsp;the&nbsp;Margin.&nbsp;&nbsp;The&nbsp;idea&nbsp;is&nbsp;that&nbsp;if&nbsp;the&nbsp;Neg&nbsp;element&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;already&nbsp;beyond&nbsp;the&nbsp;Margin&nbsp;distance&nbsp;added&nbsp;to&nbsp;how&nbsp;far&nbsp;the&nbsp;Pos&nbsp;is,&nbsp;the&nbsp;Neg&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;already&nbsp;well&nbsp;separated&nbsp;from&nbsp;Pos&nbsp;and&nbsp;would&nbsp;not&nbsp;contribute&nbsp;to&nbsp;the&nbsp;learning&nbsp;process.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;programming&nbsp;challenge&nbsp;for&nbsp;calculating&nbsp;the&nbsp;Triplet&nbsp;Loss&nbsp;is&nbsp;similar&nbsp;to&nbsp;what&nbsp;it<br>
&nbsp;&nbsp;&nbsp;&nbsp;is&nbsp;for&nbsp;the&nbsp;Pairwise&nbsp;Contrastive&nbsp;Loss:&nbsp;How&nbsp;to&nbsp;extract&nbsp;all&nbsp;the&nbsp;triplets&nbsp;from&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;batch&nbsp;without&nbsp;using&nbsp;'for'&nbsp;loops.&nbsp;&nbsp;The&nbsp;first&nbsp;step&nbsp;is&nbsp;to&nbsp;form&nbsp;array&nbsp;index&nbsp;triplets<br>
&nbsp;&nbsp;&nbsp;&nbsp;(i,j,k)&nbsp;in&nbsp;which&nbsp;two&nbsp;indices&nbsp;are&nbsp;the&nbsp;same.&nbsp;&nbsp;If&nbsp;B&nbsp;is&nbsp;the&nbsp;batch&nbsp;size,&nbsp;this&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;easily&nbsp;done&nbsp;by&nbsp;first&nbsp;forming&nbsp;a&nbsp;BxB&nbsp;array&nbsp;that&nbsp;is&nbsp;the&nbsp;logical&nbsp;negation&nbsp;of&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;Boolean&nbsp;array&nbsp;of&nbsp;the&nbsp;same&nbsp;size&nbsp;whose&nbsp;True&nbsp;values&nbsp;are&nbsp;only&nbsp;on&nbsp;the&nbsp;diagonal.&nbsp;&nbsp;We<br>
&nbsp;&nbsp;&nbsp;&nbsp;can&nbsp;reshape&nbsp;this&nbsp;BxB&nbsp;Boolean&nbsp;array&nbsp;into&nbsp;three&nbsp;BxBxB&nbsp;shaped&nbsp;Boolean&nbsp;arrays,&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;first&nbsp;in&nbsp;which&nbsp;the&nbsp;True&nbsp;values&nbsp;exist&nbsp;only&nbsp;where&nbsp;i&nbsp;and&nbsp;j&nbsp;values&nbsp;are&nbsp;not&nbsp;the&nbsp;same,<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;second&nbsp;in&nbsp;which&nbsp;the&nbsp;True&nbsp;values&nbsp;occur&nbsp;only&nbsp;when&nbsp;i&nbsp;and&nbsp;k&nbsp;index&nbsp;values&nbsp;are&nbsp;not<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;same,&nbsp;and&nbsp;the&nbsp;third&nbsp;that&nbsp;has&nbsp;True&nbsp;values&nbsp;only&nbsp;when&nbsp;the&nbsp;j&nbsp;and&nbsp;k&nbsp;index&nbsp;values<br>
&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;not&nbsp;the&nbsp;same.&nbsp;&nbsp;By&nbsp;taking&nbsp;a&nbsp;logical&nbsp;AND&nbsp;of&nbsp;all&nbsp;three&nbsp;BxBxB&nbsp;Boolean&nbsp;arrays,&nbsp;we<br>
&nbsp;&nbsp;&nbsp;&nbsp;get&nbsp;the&nbsp;result&nbsp;we&nbsp;want.&nbsp;&nbsp;Next,&nbsp;we&nbsp;construct&nbsp;a&nbsp;BxBxB&nbsp;Boolean&nbsp;tensor&nbsp;in&nbsp;which&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;True&nbsp;values&nbsp;occur&nbsp;only&nbsp;where&nbsp;the&nbsp;first&nbsp;two&nbsp;index&nbsp;values&nbsp;imply&nbsp;that&nbsp;their<br>
&nbsp;&nbsp;&nbsp;&nbsp;corresponding&nbsp;labels&nbsp;are&nbsp;identical&nbsp;and&nbsp;where&nbsp;the&nbsp;last&nbsp;index&nbsp;corresponds&nbsp;to&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;label&nbsp;that&nbsp;does&nbsp;not&nbsp;agree&nbsp;with&nbsp;that&nbsp;for&nbsp;the&nbsp;first&nbsp;two&nbsp;index&nbsp;values.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Even&nbsp;after&nbsp;you&nbsp;have&nbsp;formed&nbsp;the&nbsp;triplets,&nbsp;your&nbsp;next&nbsp;mini-challenge&nbsp;is&nbsp;to&nbsp;reformat<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;triplets&nbsp;into&nbsp;what&nbsp;you&nbsp;need&nbsp;to&nbsp;feed&nbsp;into&nbsp;the&nbsp;PyTorch&nbsp;loss&nbsp;function<br>
&nbsp;&nbsp;&nbsp;&nbsp;torch.nn.TripletMarginLoss.&nbsp;The&nbsp;loss&nbsp;function&nbsp;takes&nbsp;three&nbsp;arguments,&nbsp;each&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;shape&nbsp;(N,M)&nbsp;where&nbsp;N&nbsp;is&nbsp;the&nbsp;total&nbsp;number&nbsp;of&nbsp;triplets&nbsp;extracted&nbsp;from&nbsp;the&nbsp;batch&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;M&nbsp;the&nbsp;size&nbsp;of&nbsp;the&nbsp;embedding&nbsp;vectors.&nbsp;&nbsp;The&nbsp;first&nbsp;such&nbsp;NxM&nbsp;tensor&nbsp;is&nbsp;the&nbsp;Anchor<br>
&nbsp;&nbsp;&nbsp;&nbsp;embedding&nbsp;vectors,&nbsp;the&nbsp;second&nbsp;for&nbsp;the&nbsp;Positive&nbsp;embedding&nbsp;vectors,&nbsp;the&nbsp;last&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;Negative&nbsp;embedding&nbsp;vectors.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;you&nbsp;wish&nbsp;to&nbsp;use&nbsp;this&nbsp;module&nbsp;to&nbsp;learn&nbsp;about&nbsp;metric&nbsp;learning,&nbsp;your&nbsp;entry&nbsp;points<br>
&nbsp;&nbsp;&nbsp;&nbsp;should&nbsp;be&nbsp;the&nbsp;following&nbsp;scripts&nbsp;in&nbsp;the&nbsp;ExamplesMetricLearning&nbsp;directory&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;distro:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;&nbsp;example_for_pairwise_contrastive_loss.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;&nbsp;example_for_triplet_loss.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;As&nbsp;the&nbsp;names&nbsp;imply,&nbsp;the&nbsp;first&nbsp;script&nbsp;demonstrates&nbsp;using&nbsp;the&nbsp;Pairwise&nbsp;Contrastive<br>
&nbsp;&nbsp;&nbsp;&nbsp;Loss&nbsp;for&nbsp;metric&nbsp;learning&nbsp;and&nbsp;the&nbsp;second&nbsp;script&nbsp;using&nbsp;the&nbsp;Triplet&nbsp;Loss&nbsp;for&nbsp;doing<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;same.&nbsp;&nbsp;Both&nbsp;scripts&nbsp;can&nbsp;work&nbsp;with&nbsp;either&nbsp;the&nbsp;pre-trained&nbsp;ResNet-50&nbsp;trunk<br>
&nbsp;&nbsp;&nbsp;&nbsp;model&nbsp;or&nbsp;the&nbsp;homebrewed&nbsp;network&nbsp;supplied&nbsp;with&nbsp;the&nbsp;MetricLearning&nbsp;module.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:red; font-size:150%"><strong><a id="120">Examples DIRECTORY</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;Examples&nbsp;subdirectory&nbsp;in&nbsp;the&nbsp;distribution&nbsp;contains&nbsp;the&nbsp;following&nbsp;scripts:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(1)&nbsp;&nbsp;playing_with_reconfig.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Shows&nbsp;how&nbsp;you&nbsp;can&nbsp;specify&nbsp;a&nbsp;convolution&nbsp;network&nbsp;with&nbsp;a&nbsp;configuration&nbsp;string.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;main&nbsp;DLStudio&nbsp;class&nbsp;parses&nbsp;the&nbsp;string&nbsp;constructs&nbsp;the&nbsp;network.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(2)&nbsp;&nbsp;playing_with_sequential.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Shows&nbsp;you&nbsp;how&nbsp;you&nbsp;can&nbsp;call&nbsp;on&nbsp;a&nbsp;custom&nbsp;inner&nbsp;class&nbsp;of&nbsp;the&nbsp;'DLStudio'&nbsp;module<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;that&nbsp;is&nbsp;meant&nbsp;to&nbsp;experiment&nbsp;with&nbsp;your&nbsp;own&nbsp;network.&nbsp;&nbsp;The&nbsp;name&nbsp;of&nbsp;the&nbsp;inner<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;class&nbsp;in&nbsp;this&nbsp;example&nbsp;script&nbsp;is&nbsp;ExperimentsWithSequential<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(3)&nbsp;&nbsp;playing_with_cifar10.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;is&nbsp;very&nbsp;similar&nbsp;to&nbsp;the&nbsp;previous&nbsp;example&nbsp;script&nbsp;but&nbsp;is&nbsp;based&nbsp;on&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;inner&nbsp;class&nbsp;ExperimentsWithCIFAR&nbsp;which&nbsp;uses&nbsp;more&nbsp;common&nbsp;examples&nbsp;of&nbsp;networks<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;playing&nbsp;with&nbsp;the&nbsp;CIFAR-10&nbsp;dataset.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(5)&nbsp;&nbsp;playing_with_skip_connections.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;script&nbsp;illustrates&nbsp;how&nbsp;to&nbsp;use&nbsp;the&nbsp;inner&nbsp;class&nbsp;BMEnet&nbsp;of&nbsp;the&nbsp;module&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;experimenting&nbsp;with&nbsp;skip&nbsp;connections&nbsp;in&nbsp;a&nbsp;CNN.&nbsp;As&nbsp;the&nbsp;script&nbsp;shows,&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;constructor&nbsp;of&nbsp;the&nbsp;BMEnet&nbsp;class&nbsp;comes&nbsp;with&nbsp;two&nbsp;options:&nbsp;skip_connections&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;depth.&nbsp;&nbsp;By&nbsp;turning&nbsp;the&nbsp;first&nbsp;on&nbsp;and&nbsp;off,&nbsp;you&nbsp;can&nbsp;directly&nbsp;illustrate&nbsp;in&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;classroom&nbsp;setting&nbsp;the&nbsp;improvement&nbsp;you&nbsp;can&nbsp;get&nbsp;with&nbsp;skip&nbsp;connections.&nbsp;&nbsp;And&nbsp;by<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;giving&nbsp;an&nbsp;appropriate&nbsp;value&nbsp;to&nbsp;the&nbsp;"depth"&nbsp;option,&nbsp;you&nbsp;can&nbsp;show&nbsp;results&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;networks&nbsp;of&nbsp;different&nbsp;depths.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(6)&nbsp;&nbsp;custom_data_loading.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;script&nbsp;shows&nbsp;how&nbsp;to&nbsp;use&nbsp;the&nbsp;custom&nbsp;dataloader&nbsp;in&nbsp;the&nbsp;inner&nbsp;class<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;CustomDataLoading&nbsp;of&nbsp;the&nbsp;main&nbsp;DLStudio&nbsp;class.&nbsp;&nbsp;That&nbsp;custom&nbsp;dataloader&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;meant&nbsp;specifically&nbsp;for&nbsp;the&nbsp;PurdueShapes5&nbsp;dataset&nbsp;that&nbsp;is&nbsp;used&nbsp;in&nbsp;object<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;detection&nbsp;and&nbsp;localization&nbsp;experiments&nbsp;in&nbsp;DLStudio.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(7)&nbsp;&nbsp;object_detection_and_localization.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;script&nbsp;shows&nbsp;how&nbsp;you&nbsp;can&nbsp;use&nbsp;the&nbsp;functionality&nbsp;provided&nbsp;by&nbsp;the&nbsp;inner<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;class&nbsp;DetectAndLocalize&nbsp;of&nbsp;the&nbsp;main&nbsp;DLStudio&nbsp;class&nbsp;for&nbsp;experimenting&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;object&nbsp;detection&nbsp;and&nbsp;localization.&nbsp;&nbsp;Detecting&nbsp;and&nbsp;localizing&nbsp;(D&amp;L)&nbsp;objects<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;images&nbsp;is&nbsp;a&nbsp;more&nbsp;difficult&nbsp;problem&nbsp;than&nbsp;just&nbsp;classifying&nbsp;the&nbsp;objects.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;D&amp;L&nbsp;requires&nbsp;that&nbsp;your&nbsp;CNN&nbsp;make&nbsp;two&nbsp;different&nbsp;types&nbsp;of&nbsp;inferences<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;simultaneously,&nbsp;one&nbsp;for&nbsp;classification&nbsp;and&nbsp;the&nbsp;other&nbsp;for&nbsp;localization.&nbsp;&nbsp;For<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;localization&nbsp;part,&nbsp;the&nbsp;CNN&nbsp;must&nbsp;carry&nbsp;out&nbsp;what&nbsp;is&nbsp;known&nbsp;as<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;regression.&nbsp;What&nbsp;that&nbsp;means&nbsp;is&nbsp;that&nbsp;the&nbsp;CNN&nbsp;must&nbsp;output&nbsp;the&nbsp;numerical&nbsp;values<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;the&nbsp;bounding&nbsp;box&nbsp;that&nbsp;encloses&nbsp;the&nbsp;object&nbsp;that&nbsp;was&nbsp;detected.&nbsp;&nbsp;Generating<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;these&nbsp;two&nbsp;types&nbsp;of&nbsp;inferences&nbsp;requires&nbsp;two&nbsp;DIFFERENT&nbsp;loss&nbsp;functions,&nbsp;one&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;classification&nbsp;and&nbsp;the&nbsp;other&nbsp;for&nbsp;regression.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(8)&nbsp;&nbsp;noisy_object_detection_and_localization.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;script&nbsp;in&nbsp;the&nbsp;Examples&nbsp;directory&nbsp;is&nbsp;exactly&nbsp;the&nbsp;same&nbsp;as&nbsp;the&nbsp;one<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;described&nbsp;above,&nbsp;the&nbsp;only&nbsp;difference&nbsp;is&nbsp;that&nbsp;it&nbsp;calls&nbsp;on&nbsp;the&nbsp;noise-corrupted<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;training&nbsp;and&nbsp;testing&nbsp;dataset&nbsp;files.&nbsp;&nbsp;I&nbsp;thought&nbsp;it&nbsp;would&nbsp;be&nbsp;best&nbsp;to&nbsp;create&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;separate&nbsp;script&nbsp;for&nbsp;studying&nbsp;the&nbsp;effects&nbsp;of&nbsp;noise,&nbsp;just&nbsp;to&nbsp;allow&nbsp;for&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;possibility&nbsp;that&nbsp;the&nbsp;noise-related&nbsp;studies&nbsp;with&nbsp;DLStudio&nbsp;may&nbsp;evolve<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;differently&nbsp;in&nbsp;the&nbsp;future.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(9)&nbsp;&nbsp;object_detection_and_localization_iou.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;script&nbsp;in&nbsp;the&nbsp;Examples&nbsp;directory&nbsp;is&nbsp;for&nbsp;experimenting&nbsp;with&nbsp;the&nbsp;variants<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;the&nbsp;IoU&nbsp;(Intersection&nbsp;over&nbsp;Union)&nbsp;loss&nbsp;functions&nbsp;provided&nbsp;by&nbsp;the&nbsp;class<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DIoULoss&nbsp;class&nbsp;that&nbsp;is&nbsp;a&nbsp;part&nbsp;of&nbsp;DLStudio's&nbsp;inner&nbsp;class&nbsp;DetectAndLocalize.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;script&nbsp;uses&nbsp;the&nbsp;same&nbsp;datasets&nbsp;as&nbsp;the&nbsp;script&nbsp;mentioned&nbsp;in&nbsp;item&nbsp;7&nbsp;above.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(10)&nbsp;semantic_segmentation.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;script&nbsp;should&nbsp;be&nbsp;your&nbsp;starting&nbsp;point&nbsp;if&nbsp;you&nbsp;wish&nbsp;to&nbsp;learn&nbsp;how&nbsp;to&nbsp;use<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;mUNet&nbsp;neural&nbsp;network&nbsp;for&nbsp;semantic&nbsp;segmentation&nbsp;of&nbsp;images.&nbsp;&nbsp;As&nbsp;mentioned<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;elsewhere&nbsp;in&nbsp;this&nbsp;documentation&nbsp;page,&nbsp;mUNet&nbsp;assigns&nbsp;an&nbsp;output&nbsp;channel&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;each&nbsp;different&nbsp;type&nbsp;of&nbsp;object&nbsp;that&nbsp;you&nbsp;wish&nbsp;to&nbsp;segment&nbsp;out&nbsp;from&nbsp;an<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;image.&nbsp;So,&nbsp;given&nbsp;a&nbsp;test&nbsp;image&nbsp;at&nbsp;the&nbsp;input&nbsp;to&nbsp;the&nbsp;network,&nbsp;all&nbsp;you&nbsp;have&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;do&nbsp;is&nbsp;to&nbsp;examine&nbsp;each&nbsp;channel&nbsp;at&nbsp;the&nbsp;output&nbsp;for&nbsp;segmenting&nbsp;out&nbsp;the&nbsp;objects<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;that&nbsp;correspond&nbsp;to&nbsp;that&nbsp;output&nbsp;channel.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(11)&nbsp;run_autoencoder.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Even&nbsp;though&nbsp;the&nbsp;main&nbsp;purpose&nbsp;of&nbsp;the&nbsp;Autoencoder&nbsp;class&nbsp;in&nbsp;DLStudio&nbsp;is&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;serve&nbsp;as&nbsp;the&nbsp;Base&nbsp;class&nbsp;for&nbsp;the&nbsp;VAE&nbsp;class,&nbsp;this&nbsp;script&nbsp;allows&nbsp;you&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;experiment&nbsp;with&nbsp;just&nbsp;the&nbsp;Autoencoder&nbsp;class&nbsp;by&nbsp;itself.&nbsp;&nbsp;For&nbsp;example,&nbsp;if&nbsp;you<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;wanted&nbsp;to&nbsp;experiment&nbsp;with&nbsp;dimensionality&nbsp;reduction&nbsp;with&nbsp;an&nbsp;Autoencoder,&nbsp;all<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;you&nbsp;would&nbsp;need&nbsp;to&nbsp;do&nbsp;would&nbsp;be&nbsp;to&nbsp;change&nbsp;the&nbsp;last&nbsp;or&nbsp;the&nbsp;last&nbsp;couple&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;layers&nbsp;of&nbsp;the&nbsp;Decoder&nbsp;in&nbsp;the&nbsp;Autoencoder&nbsp;class&nbsp;and&nbsp;see&nbsp;for&nbsp;yourself&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;results&nbsp;by&nbsp;running&nbsp;this&nbsp;script.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(12)&nbsp;run_vae.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;You&nbsp;can&nbsp;use&nbsp;this&nbsp;script&nbsp;to&nbsp;experiment&nbsp;with&nbsp;the&nbsp;variational&nbsp;autoencoding&nbsp;code<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;the&nbsp;VAE&nbsp;(Variational&nbsp;Auto-Encoder)&nbsp;inner&nbsp;class&nbsp;of&nbsp;DLStudio.&nbsp;&nbsp;Variational<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;autoencoding&nbsp;means&nbsp;mapping&nbsp;an&nbsp;input&nbsp;image&nbsp;to&nbsp;a&nbsp;latent&nbsp;vector&nbsp;that&nbsp;captures<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;"essence"&nbsp;of&nbsp;what's&nbsp;in&nbsp;the&nbsp;image&nbsp;with&nbsp;the&nbsp;assumption&nbsp;that&nbsp;the&nbsp;latent<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;vectors&nbsp;form&nbsp;a&nbsp;much&nbsp;distribution&nbsp;(ideally&nbsp;a&nbsp;zero-mean,&nbsp;unit&nbsp;covariance<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Gaussian)&nbsp;than&nbsp;the&nbsp;original&nbsp;input&nbsp;data.&nbsp;&nbsp;The&nbsp;Decoder&nbsp;part&nbsp;of&nbsp;a&nbsp;VAE&nbsp;samples<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;latent&nbsp;distribution&nbsp;and&nbsp;can&nbsp;be&nbsp;trained&nbsp;to&nbsp;create&nbsp;useful&nbsp;variants&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;input&nbsp;data.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(13)&nbsp;run_vae_for_image_generation.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;script&nbsp;allows&nbsp;you&nbsp;to&nbsp;experiment&nbsp;with&nbsp;just&nbsp;the&nbsp;Decoder&nbsp;part&nbsp;of&nbsp;the&nbsp;VAE<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(Variational&nbsp;Auto-Encoder)&nbsp;class.&nbsp;&nbsp;The&nbsp;VAE&nbsp;Decoder&nbsp;is&nbsp;in&nbsp;reality&nbsp;a&nbsp;Generator<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;samples&nbsp;a&nbsp;Gaussian&nbsp;probability&nbsp;distribution,&nbsp;as&nbsp;specified&nbsp;by&nbsp;its&nbsp;mean&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;covariance,&nbsp;and&nbsp;transforms&nbsp;the&nbsp;sample&nbsp;thus&nbsp;created&nbsp;into&nbsp;an&nbsp;output&nbsp;image&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;will&nbsp;bear&nbsp;some&nbsp;resemblance&nbsp;with&nbsp;the&nbsp;training&nbsp;images.&nbsp;&nbsp;As&nbsp;to&nbsp;how&nbsp;close&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;output&nbsp;image&nbsp;will&nbsp;come&nbsp;to&nbsp;looking&nbsp;like&nbsp;your&nbsp;images&nbsp;in&nbsp;the&nbsp;training&nbsp;dataset<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;would&nbsp;depend&nbsp;on&nbsp;the&nbsp;size&nbsp;of&nbsp;the&nbsp;dataset,&nbsp;the&nbsp;complexity&nbsp;of&nbsp;the&nbsp;images,&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dimensionality&nbsp;of&nbsp;the&nbsp;latent&nbsp;space&nbsp;(this&nbsp;dimensionality&nbsp;is&nbsp;8192)&nbsp;for&nbsp;the&nbsp;VAE<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;network&nbsp;as&nbsp;implemented&nbsp;in&nbsp;DLStudio),&nbsp;etc.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(14)&nbsp;run_vqvae.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Run&nbsp;this&nbsp;script&nbsp;to&nbsp;experiment&nbsp;with&nbsp;the&nbsp;VQVAE&nbsp;inner&nbsp;class&nbsp;in&nbsp;the&nbsp;main<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;DLStudio&nbsp;class.&nbsp;VQVAE&nbsp;is&nbsp;about&nbsp;what&nbsp;has&nbsp;come&nbsp;to&nbsp;he&nbsp;known&nbsp;as&nbsp;Codebook<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;learning&nbsp;for&nbsp;more&nbsp;efficient&nbsp;discrete&nbsp;representation&nbsp;of&nbsp;images&nbsp;with&nbsp;a&nbsp;finite<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;vocabulary&nbsp;of&nbsp;embedding&nbsp;vectors.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(15)&nbsp;run_vqgan.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;While&nbsp;the&nbsp;overall&nbsp;goal&nbsp;in&nbsp;this&nbsp;script&nbsp;is&nbsp;the&nbsp;same&nbsp;as&nbsp;in&nbsp;the&nbsp;previous&nbsp;script<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;---&nbsp;Codebook&nbsp;learning&nbsp;---&nbsp;this&nbsp;script&nbsp;does&nbsp;a&nbsp;couple&nbsp;of&nbsp;fancier&nbsp;things.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;First,&nbsp;it&nbsp;encapsulates&nbsp;the&nbsp;Encoder-VQ-Decoder&nbsp;network&nbsp;in&nbsp;a&nbsp;GAN&nbsp;and&nbsp;trains<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;network&nbsp;in&nbsp;an&nbsp;adversarial&nbsp;fashion.&nbsp;&nbsp;The&nbsp;basic&nbsp;VQGAN&nbsp;network&nbsp;trained&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;fashion&nbsp;is&nbsp;then&nbsp;used&nbsp;in&nbsp;the&nbsp;next&nbsp;script&nbsp;for&nbsp;transformer-based<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;autoregressive&nbsp;modeling&nbsp;of&nbsp;the&nbsp;Codebook&nbsp;indices&nbsp;to&nbsp;which&nbsp;each&nbsp;input&nbsp;image&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mapped.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(16)&nbsp;run_vqgan_transformer.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;script&nbsp;is&nbsp;paired&nbsp;with&nbsp;the&nbsp;previous&nbsp;script.&nbsp;&nbsp;That&nbsp;is,&nbsp;you&nbsp;must&nbsp;first&nbsp;run<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;previous&nbsp;script&nbsp;and&nbsp;train&nbsp;well&nbsp;the&nbsp;basic&nbsp;Encoder-VQ-Decoder&nbsp;network&nbsp;of&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;VQGAN.&nbsp;&nbsp;Only&nbsp;after&nbsp;that,&nbsp;you&nbsp;can&nbsp;run&nbsp;this&nbsp;script&nbsp;for&nbsp;autoregressive&nbsp;modeling<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;the&nbsp;images&nbsp;based&nbsp;on&nbsp;representing&nbsp;the&nbsp;images&nbsp;in&nbsp;the&nbsp;Latest&nbsp;Space&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sequences&nbsp;of&nbsp;integer&nbsp;indices,&nbsp;with&nbsp;each&nbsp;integer&nbsp;index&nbsp;representing&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;codebook&nbsp;vector.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(17)&nbsp;run_vqgan_map_image_to_codebook.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;script&nbsp;is&nbsp;also&nbsp;meant&nbsp;to&nbsp;be&nbsp;run&nbsp;after&nbsp;you&nbsp;have&nbsp;already&nbsp;trained&nbsp;the&nbsp;basic<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;VQGAN&nbsp;network&nbsp;with&nbsp;the&nbsp;script&nbsp;run_vqgan.py.&nbsp;&nbsp;The&nbsp;goal&nbsp;of&nbsp;this&nbsp;script&nbsp;is&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;make&nbsp;it&nbsp;easy&nbsp;for&nbsp;you&nbsp;play&nbsp;with&nbsp;individual&nbsp;images&nbsp;or&nbsp;a&nbsp;batch&nbsp;of&nbsp;images&nbsp;to&nbsp;see<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;mappings&nbsp;between&nbsp;the&nbsp;images&nbsp;and&nbsp;the&nbsp;codebook&nbsp;vectors.&nbsp;See&nbsp;the&nbsp;commented<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;out&nbsp;block&nbsp;at&nbsp;the&nbsp;top&nbsp;of&nbsp;this&nbsp;file&nbsp;to&nbsp;appreciate&nbsp;the&nbsp;reasons&nbsp;for&nbsp;why&nbsp;you<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;might&nbsp;want&nbsp;to&nbsp;play&nbsp;with&nbsp;this&nbsp;script<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(18)&nbsp;text_classification_with_TEXTnet.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;script&nbsp;is&nbsp;your&nbsp;first&nbsp;introduction&nbsp;in&nbsp;DLStudio&nbsp;to&nbsp;a&nbsp;Recurrent&nbsp;Neural<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Network,&nbsp;meaning&nbsp;a&nbsp;neural-network&nbsp;with&nbsp;feedback.&nbsp;&nbsp;Such&nbsp;networks&nbsp;are&nbsp;needed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;solving&nbsp;problems&nbsp;related&nbsp;to&nbsp;variable&nbsp;length&nbsp;input&nbsp;data&nbsp;in&nbsp;applications<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;such&nbsp;as&nbsp;text&nbsp;classification,&nbsp;sentiment&nbsp;analysis,&nbsp;machine&nbsp;translation,&nbsp;etc.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Unfortunately,&nbsp;unless&nbsp;care&nbsp;is&nbsp;taken,&nbsp;the&nbsp;feedback&nbsp;in&nbsp;such&nbsp;networks&nbsp;results<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;long&nbsp;chains&nbsp;of&nbsp;dependencies&nbsp;and&nbsp;thus&nbsp;exacerbates&nbsp;the&nbsp;vanishing&nbsp;gradients<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;problem.&nbsp;&nbsp;The&nbsp;specific&nbsp;goal&nbsp;of&nbsp;this&nbsp;script&nbsp;is&nbsp;neural&nbsp;learning&nbsp;for&nbsp;automatic<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;classification&nbsp;of&nbsp;product&nbsp;reviews.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(19)&nbsp;text_classification_with_TEXTnet_word2vec.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;script&nbsp;uses&nbsp;the&nbsp;same&nbsp;learning&nbsp;network&nbsp;as&nbsp;in&nbsp;the&nbsp;previous&nbsp;script,&nbsp;but<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;there&nbsp;is&nbsp;a&nbsp;big&nbsp;difference&nbsp;between&nbsp;the&nbsp;two.&nbsp;&nbsp;The&nbsp;previous&nbsp;network&nbsp;uses<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;one-hot&nbsp;vectors&nbsp;for&nbsp;representing&nbsp;the&nbsp;words.&nbsp;On&nbsp;the&nbsp;other&nbsp;hand,&nbsp;this&nbsp;script<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;uses&nbsp;pre-trained&nbsp;word2vec&nbsp;embeddings.&nbsp;&nbsp;These&nbsp;are&nbsp;fixed-sized&nbsp;numerical<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;vectors&nbsp;that&nbsp;are&nbsp;learned&nbsp;on&nbsp;the&nbsp;basis&nbsp;of&nbsp;contextual&nbsp;similarities.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(20)&nbsp;text_classification_with_TEXTnetOrder2.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;As&nbsp;mentioned&nbsp;earlier&nbsp;for&nbsp;the&nbsp;script&nbsp;in&nbsp;item&nbsp;10&nbsp;above,&nbsp;the&nbsp;vanishing<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gradients&nbsp;problem&nbsp;becomes&nbsp;worse&nbsp;in&nbsp;neural&nbsp;networks&nbsp;with&nbsp;feedback.&nbsp;&nbsp;One&nbsp;way<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;get&nbsp;around&nbsp;this&nbsp;problem&nbsp;is&nbsp;to&nbsp;use&nbsp;what's&nbsp;known&nbsp;as&nbsp;"gated&nbsp;recurrence".<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;script&nbsp;uses&nbsp;the&nbsp;TEXTnetOrder2&nbsp;network&nbsp;as&nbsp;a&nbsp;stepping&nbsp;stone&nbsp;to&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;full-blown&nbsp;implementation&nbsp;of&nbsp;gating&nbsp;as&nbsp;provided&nbsp;by&nbsp;the&nbsp;nn.GRU&nbsp;class&nbsp;in&nbsp;item<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;14&nbsp;below.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(21)&nbsp;text_classification_with_TEXTnetOrder2_word2vec.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;script&nbsp;uses&nbsp;the&nbsp;same&nbsp;network&nbsp;as&nbsp;the&nbsp;previous&nbsp;script,&nbsp;but&nbsp;now&nbsp;we&nbsp;use&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;word2vec&nbsp;embeddings&nbsp;for&nbsp;representing&nbsp;the&nbsp;words.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(22)&nbsp;text_classification_with_GRU.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;script&nbsp;demonstrates&nbsp;how&nbsp;one&nbsp;can&nbsp;use&nbsp;a&nbsp;GRU&nbsp;(Gated&nbsp;Recurrent&nbsp;Unit)&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;remediate&nbsp;one&nbsp;of&nbsp;the&nbsp;main&nbsp;problems&nbsp;associated&nbsp;with&nbsp;recurrence&nbsp;--&nbsp;vanishing<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gradients&nbsp;in&nbsp;the&nbsp;long&nbsp;chains&nbsp;of&nbsp;dependencies&nbsp;created&nbsp;by&nbsp;feedback.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(23)&nbsp;text_classification_with_GRU_word2vec.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;While&nbsp;this&nbsp;script&nbsp;uses&nbsp;the&nbsp;same&nbsp;learning&nbsp;network&nbsp;as&nbsp;the&nbsp;previous&nbsp;one,&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;words&nbsp;are&nbsp;now&nbsp;represented&nbsp;by&nbsp;fixed-sized&nbsp;word2vec&nbsp;embeddings.<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:red; font-size:150%"><strong><a id="121">ExamplesAdversarialLearning DIRECTORY</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;ExamplesAdversarialLearning&nbsp;directory&nbsp;of&nbsp;the&nbsp;distribution&nbsp;contains&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;following&nbsp;scripts&nbsp;for&nbsp;demonstrating&nbsp;adversarial&nbsp;learning&nbsp;for&nbsp;data&nbsp;modeling:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;&nbsp;dcgan_DG1.py&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;&nbsp;dcgan_DG2.py&nbsp;&nbsp;&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.&nbsp;&nbsp;wgan_CG1.py&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.&nbsp;&nbsp;wgan_with_gp_CG2.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;first&nbsp;script&nbsp;demonstrates&nbsp;the&nbsp;DCGAN&nbsp;logic&nbsp;on&nbsp;the&nbsp;PurdueShapes5GAN&nbsp;dataset.<br>
&nbsp;&nbsp;&nbsp;&nbsp;In&nbsp;order&nbsp;to&nbsp;show&nbsp;the&nbsp;sensitivity&nbsp;of&nbsp;the&nbsp;basic&nbsp;DCGAN&nbsp;logic&nbsp;to&nbsp;any&nbsp;variations&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;network&nbsp;or&nbsp;the&nbsp;weight&nbsp;initializations,&nbsp;the&nbsp;second&nbsp;script&nbsp;introduces&nbsp;a&nbsp;small<br>
&nbsp;&nbsp;&nbsp;&nbsp;change&nbsp;in&nbsp;the&nbsp;network.&nbsp;&nbsp;The&nbsp;third&nbsp;script&nbsp;is&nbsp;a&nbsp;demonstration&nbsp;of&nbsp;using&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;Wasserstein&nbsp;distance&nbsp;for&nbsp;data&nbsp;modeling&nbsp;through&nbsp;adversarial&nbsp;learning.&nbsp;&nbsp;The&nbsp;fourth<br>
&nbsp;&nbsp;&nbsp;&nbsp;script&nbsp;adds&nbsp;a&nbsp;Gradient&nbsp;Penalty&nbsp;term&nbsp;to&nbsp;the&nbsp;Wasserstein&nbsp;Distance&nbsp;based&nbsp;logic&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;third&nbsp;script.&nbsp;&nbsp;The&nbsp;PurdueShapes5GAN&nbsp;dataset&nbsp;consists&nbsp;of&nbsp;64x64&nbsp;images&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;randomly&nbsp;shaped,&nbsp;randomly&nbsp;positioned,&nbsp;and&nbsp;randomly&nbsp;colored&nbsp;shapes.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;results&nbsp;produced&nbsp;by&nbsp;these&nbsp;scripts&nbsp;(for&nbsp;the&nbsp;constructor&nbsp;options&nbsp;shown&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;scripts)&nbsp;are&nbsp;included&nbsp;in&nbsp;a&nbsp;subdirectory&nbsp;named&nbsp;RVLCloud_based_results.&nbsp;&nbsp;If&nbsp;you&nbsp;are<br>
&nbsp;&nbsp;&nbsp;&nbsp;just&nbsp;becoming&nbsp;familiar&nbsp;with&nbsp;the&nbsp;AdversarialLearning&nbsp;class&nbsp;of&nbsp;DLStudio,&nbsp;I'd&nbsp;urge<br>
&nbsp;&nbsp;&nbsp;&nbsp;you&nbsp;to&nbsp;run&nbsp;the&nbsp;script&nbsp;with&nbsp;the&nbsp;constructor&nbsp;options&nbsp;as&nbsp;shown&nbsp;and&nbsp;to&nbsp;compare&nbsp;your<br>
&nbsp;&nbsp;&nbsp;&nbsp;results&nbsp;with&nbsp;those&nbsp;that&nbsp;are&nbsp;in&nbsp;the&nbsp;RVLCloud_based_results&nbsp;directory.<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:red; font-size:150%"><strong><a id="122">ExamplesDiffusion DIRECTORY</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;ExamplesDiffusion&nbsp;directory&nbsp;of&nbsp;DLStudio&nbsp;contains&nbsp;the&nbsp;following&nbsp;files&nbsp;that&nbsp;you<br>
&nbsp;&nbsp;&nbsp;&nbsp;will&nbsp;find&nbsp;helpful&nbsp;for&nbsp;your&nbsp;experiments&nbsp;with&nbsp;diffusion:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.&nbsp;&nbsp;README<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;&nbsp;RunCodeForDiffusion.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;&nbsp;GenerateNewImageSamples.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.&nbsp;&nbsp;VisualizeSamples.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Any&nbsp;experiment&nbsp;with&nbsp;diffusion&nbsp;will&nbsp;involve&nbsp;all&nbsp;three&nbsp;scripts&nbsp;mentioned&nbsp;above.<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;script&nbsp;RunCodeForDiffusion.py&nbsp;is&nbsp;for&nbsp;training&nbsp;the&nbsp;neural&nbsp;network&nbsp;to&nbsp;become<br>
&nbsp;&nbsp;&nbsp;&nbsp;adept&nbsp;at&nbsp;learning&nbsp;the&nbsp;p-chain&nbsp;transition&nbsp;probabilities&nbsp;p(&nbsp;x_{t-1}&nbsp;|&nbsp;x_t&nbsp;).&nbsp;&nbsp;The<br>
&nbsp;&nbsp;&nbsp;&nbsp;script&nbsp;GenerateNewImageSamples.py&nbsp;is&nbsp;for&nbsp;generating&nbsp;the&nbsp;images&nbsp;using&nbsp;the&nbsp;learned<br>
&nbsp;&nbsp;&nbsp;&nbsp;model.&nbsp;&nbsp;This&nbsp;script&nbsp;deposits&nbsp;all&nbsp;the&nbsp;generated&nbsp;images&nbsp;in&nbsp;a&nbsp;numpy&nbsp;archive&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;ndarrays.&nbsp;&nbsp;The&nbsp;last&nbsp;script,&nbsp;VisualizeSamples.py,&nbsp;is&nbsp;for&nbsp;extracting&nbsp;the&nbsp;individual<br>
&nbsp;&nbsp;&nbsp;&nbsp;images&nbsp;from&nbsp;that&nbsp;archive.&nbsp;&nbsp;Please&nbsp;make&nbsp;sure&nbsp;that&nbsp;you&nbsp;have&nbsp;gone&nbsp;through&nbsp;the&nbsp;README<br>
&nbsp;&nbsp;&nbsp;&nbsp;mentioned&nbsp;above&nbsp;before&nbsp;starting&nbsp;your&nbsp;experiments&nbsp;with&nbsp;the&nbsp;diffusion&nbsp;part&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;DLStudio.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:red; font-size:150%"><strong><a id="123">ExamplesSeq2SeqLearning DIRECTORY</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;ExamplesSeq2SeqLearning&nbsp;directory&nbsp;of&nbsp;the&nbsp;distribution&nbsp;contains&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;scripts&nbsp;for&nbsp;demonstrating&nbsp;sequence-to-sequence&nbsp;learning:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(1)&nbsp;seq2seq_with_learnable_embeddings.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;script&nbsp;demonstrates&nbsp;the&nbsp;basic&nbsp;PyTorch&nbsp;structures&nbsp;and&nbsp;idioms&nbsp;to&nbsp;use&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;seq2seq&nbsp;learning.&nbsp;&nbsp;The&nbsp;application&nbsp;example&nbsp;addressed&nbsp;in&nbsp;the&nbsp;script&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;English-to-Spanish&nbsp;translation.&nbsp;&nbsp;And&nbsp;the&nbsp;attention&nbsp;mechanism&nbsp;used&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;seq2seq&nbsp;is&nbsp;the&nbsp;one&nbsp;proposed&nbsp;by&nbsp;Bahdanau,&nbsp;Cho,&nbsp;and&nbsp;Bengio.&nbsp;&nbsp;This&nbsp;network&nbsp;used<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;this&nbsp;example&nbsp;calls&nbsp;on&nbsp;the&nbsp;nn.Embeddings&nbsp;layer&nbsp;in&nbsp;the&nbsp;encoder&nbsp;to&nbsp;learn&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;embeddings&nbsp;for&nbsp;the&nbsp;words&nbsp;in&nbsp;the&nbsp;source&nbsp;language&nbsp;and&nbsp;a&nbsp;similar&nbsp;layer&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;decoder&nbsp;to&nbsp;learn&nbsp;the&nbsp;embeddings&nbsp;to&nbsp;use&nbsp;for&nbsp;the&nbsp;target&nbsp;language.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;(2)&nbsp;seq2seq_with_pretrained_embeddings.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;script,&nbsp;also&nbsp;for&nbsp;seq2seq&nbsp;learning,&nbsp;differs&nbsp;from&nbsp;the&nbsp;previous&nbsp;one&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;only&nbsp;one&nbsp;respect:&nbsp;it&nbsp;uses&nbsp;Google's&nbsp;word2vec&nbsp;embeddings&nbsp;for&nbsp;representing&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;words&nbsp;in&nbsp;the&nbsp;source-language&nbsp;sentences&nbsp;(English).&nbsp;&nbsp;As&nbsp;to&nbsp;why&nbsp;I&nbsp;have&nbsp;not&nbsp;used<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;at&nbsp;this&nbsp;time&nbsp;the&nbsp;pre-trained&nbsp;embeddings&nbsp;for&nbsp;the&nbsp;target&nbsp;language&nbsp;is&nbsp;explained<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;the&nbsp;main&nbsp;comment&nbsp;doc&nbsp;associated&nbsp;with&nbsp;the&nbsp;class<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Seq2SeqWithPretrainedEmbeddings.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:red; font-size:150%"><strong><a id="124">ExamplesDataPrediction DIRECTORY</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;ExamplesDataPrediction&nbsp;directory&nbsp;of&nbsp;the&nbsp;distribution&nbsp;contains&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;script&nbsp;for&nbsp;demonstrating&nbsp;data&nbsp;prediction&nbsp;for&nbsp;time-series&nbsp;data:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;power_load_prediction_with_pmGRU.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;script&nbsp;uses&nbsp;a&nbsp;subset&nbsp;of&nbsp;the&nbsp;dataset&nbsp;provided&nbsp;by&nbsp;Kaggle&nbsp;for&nbsp;one&nbsp;of&nbsp;their<br>
&nbsp;&nbsp;&nbsp;&nbsp;machine&nbsp;learning&nbsp;competitions.&nbsp;&nbsp;The&nbsp;dataset&nbsp;consists&nbsp;of&nbsp;over&nbsp;10-years&nbsp;worth&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;hourly&nbsp;electric&nbsp;load&nbsp;recordings&nbsp;made&nbsp;available&nbsp;by&nbsp;several&nbsp;utilities&nbsp;in&nbsp;the&nbsp;east<br>
&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;the&nbsp;Midwest&nbsp;of&nbsp;the&nbsp;United&nbsp;States.&nbsp;&nbsp;You&nbsp;can&nbsp;download&nbsp;this&nbsp;dataset&nbsp;from&nbsp;a&nbsp;link<br>
&nbsp;&nbsp;&nbsp;&nbsp;at&nbsp;the&nbsp;top&nbsp;of&nbsp;the&nbsp;main&nbsp;DLStudio&nbsp;doc&nbsp;page.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:red; font-size:150%"><strong><a id="125">ExamplesTransformers DIRECTORY</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;ExamplesTransformers&nbsp;directory&nbsp;of&nbsp;the&nbsp;distribution&nbsp;contains&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;four&nbsp;scripts&nbsp;for&nbsp;experimenting&nbsp;with&nbsp;transformers&nbsp;in&nbsp;DLStudio:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;seq2seq_with_transformerFG.py&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;seq2seq_with_transformerPreLN.py&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;image_recog_with_visTransformer.py<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;test_checkpoint_for_visTransformer.py&nbsp;<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;first&nbsp;two&nbsp;scripts&nbsp;deal&nbsp;with&nbsp;English-to-Spanish&nbsp;translation&nbsp;in&nbsp;a&nbsp;manner<br>
&nbsp;&nbsp;&nbsp;&nbsp;similar&nbsp;to&nbsp;what's&nbsp;demonstrated&nbsp;by&nbsp;the&nbsp;code&nbsp;in&nbsp;the&nbsp;Seq2SeqLearning&nbsp;module&nbsp;and&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;example&nbsp;scripts&nbsp;associated&nbsp;with&nbsp;that&nbsp;module.&nbsp;The&nbsp;last&nbsp;two&nbsp;relate&nbsp;to&nbsp;my<br>
&nbsp;&nbsp;&nbsp;&nbsp;demonstration&nbsp;of&nbsp;image&nbsp;recognition&nbsp;with&nbsp;a&nbsp;transformer&nbsp;based&nbsp;implementation.&nbsp;&nbsp;I<br>
&nbsp;&nbsp;&nbsp;&nbsp;have&nbsp;used&nbsp;the&nbsp;CFAR10&nbsp;dataset&nbsp;for&nbsp;image&nbsp;recognition.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:red; font-size:150%"><strong><a id="126">ExamplesMetricLearning DIRECTORY</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;ExamplesMetricLearning&nbsp;directory&nbsp;at&nbsp;top&nbsp;level&nbsp;of&nbsp;the&nbsp;distribution&nbsp;contains<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;following&nbsp;scripts:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;&nbsp;example_for_pairwise_contrastive_loss.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;&nbsp;example_for_triplet_loss.py<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;As&nbsp;the&nbsp;names&nbsp;imply,&nbsp;the&nbsp;first&nbsp;script&nbsp;demonstrates&nbsp;using&nbsp;the&nbsp;Pairwise&nbsp;Contrastive<br>
&nbsp;&nbsp;&nbsp;&nbsp;Loss&nbsp;for&nbsp;metric&nbsp;learning&nbsp;and&nbsp;the&nbsp;second&nbsp;script&nbsp;using&nbsp;the&nbsp;Triplet&nbsp;Loss&nbsp;for&nbsp;doing<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;same.&nbsp;&nbsp;Both&nbsp;scripts&nbsp;can&nbsp;work&nbsp;with&nbsp;either&nbsp;the&nbsp;pre-trained&nbsp;ResNet-50&nbsp;trunk<br>
&nbsp;&nbsp;&nbsp;&nbsp;model&nbsp;or&nbsp;the&nbsp;homebrewed&nbsp;network&nbsp;supplied&nbsp;with&nbsp;the&nbsp;MetricLearning&nbsp;module.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:red; font-size:150%"><strong><a id="127">THE DATASETS INCLUDED</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;[must&nbsp;be&nbsp;downloaded&nbsp;separately]<br>
&nbsp;<br>
<span style="color:blue; font-size:100%"><strong>&nbsp;&nbsp;<a id="128">    FOR THE MAIN DLStudio CLASS and its INNER CLASSES</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Download&nbsp;the&nbsp;dataset&nbsp;archive&nbsp;'datasets_for_DLStudio.tar.gz'&nbsp;through&nbsp;the&nbsp;link<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"Download&nbsp;the&nbsp;image&nbsp;datasets&nbsp;for&nbsp;the&nbsp;main&nbsp;DLStudio&nbsp;Class"&nbsp;provided&nbsp;at&nbsp;the&nbsp;top<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;this&nbsp;documentation&nbsp;page&nbsp;and&nbsp;store&nbsp;it&nbsp;in&nbsp;the&nbsp;'Example'&nbsp;directory&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;distribution.&nbsp;&nbsp;Subsequently,&nbsp;execute&nbsp;the&nbsp;following&nbsp;command&nbsp;in&nbsp;the&nbsp;'Examples'<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;directory:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cd&nbsp;Examples<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tar&nbsp;zxvf&nbsp;datasets_for_DLStudio.tar.gz<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;command&nbsp;will&nbsp;create&nbsp;a&nbsp;'data'&nbsp;subdirectory&nbsp;in&nbsp;the&nbsp;'Examples'&nbsp;directory<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;deposit&nbsp;the&nbsp;datasets&nbsp;mentioned&nbsp;below&nbsp;in&nbsp;that&nbsp;subdirectory.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
<span style="color:red; font-size:80%"><strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        FOR OBJECT DETECTION AND LOCALIZATION</strong></span><br>&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Training&nbsp;a&nbsp;CNN&nbsp;for&nbsp;object&nbsp;detection&nbsp;and&nbsp;localization&nbsp;requires&nbsp;training&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;testing&nbsp;datasets&nbsp;that&nbsp;come&nbsp;with&nbsp;bounding-box&nbsp;annotations.&nbsp;This&nbsp;module&nbsp;comes<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;the&nbsp;PurdueShapes5&nbsp;dataset&nbsp;for&nbsp;that&nbsp;purpose.&nbsp;&nbsp;I&nbsp;created&nbsp;this<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;small-image-format&nbsp;dataset&nbsp;out&nbsp;of&nbsp;my&nbsp;admiration&nbsp;for&nbsp;the&nbsp;CIFAR-10&nbsp;dataset&nbsp;as<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;an&nbsp;educational&nbsp;tool&nbsp;for&nbsp;demonstrating&nbsp;classification&nbsp;networks&nbsp;in&nbsp;a&nbsp;classroom<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;setting.&nbsp;You&nbsp;will&nbsp;find&nbsp;the&nbsp;following&nbsp;dataset&nbsp;archive&nbsp;files&nbsp;in&nbsp;the&nbsp;"data"<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;subdirectory&nbsp;of&nbsp;the&nbsp;"Examples"&nbsp;directory&nbsp;of&nbsp;the&nbsp;distro:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(1)&nbsp;&nbsp;PurdueShapes5-10000-train.gz<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PurdueShapes5-1000-test.gz<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(2)&nbsp;&nbsp;PurdueShapes5-20-train.gz<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PurdueShapes5-20-test.gz&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;number&nbsp;that&nbsp;follows&nbsp;the&nbsp;main&nbsp;name&nbsp;string&nbsp;"PurdueShapes5-"&nbsp;is&nbsp;for&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;number&nbsp;of&nbsp;images&nbsp;in&nbsp;the&nbsp;dataset.&nbsp;&nbsp;You&nbsp;will&nbsp;find&nbsp;the&nbsp;last&nbsp;two&nbsp;datasets,&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;20&nbsp;images&nbsp;each,&nbsp;useful&nbsp;for&nbsp;debugging&nbsp;your&nbsp;logic&nbsp;for&nbsp;object&nbsp;detection&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bounding-box&nbsp;regression.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;As&nbsp;to&nbsp;how&nbsp;the&nbsp;image&nbsp;data&nbsp;is&nbsp;stored&nbsp;in&nbsp;the&nbsp;archives,&nbsp;please&nbsp;see&nbsp;the&nbsp;main<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;comment&nbsp;block&nbsp;for&nbsp;the&nbsp;inner&nbsp;class&nbsp;CustomLoading&nbsp;in&nbsp;this&nbsp;file.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
<span style="color:red; font-size:80%"><strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        FOR DETECTING OBJECTS IN NOISE-CORRUPTED IMAGES</strong></span><br>&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In&nbsp;terms&nbsp;of&nbsp;how&nbsp;the&nbsp;image&nbsp;data&nbsp;is&nbsp;stored&nbsp;in&nbsp;the&nbsp;dataset&nbsp;files,&nbsp;this&nbsp;dataset<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;is&nbsp;no&nbsp;different&nbsp;from&nbsp;the&nbsp;PurdueShapes5&nbsp;dataset&nbsp;described&nbsp;above.&nbsp;&nbsp;The&nbsp;only<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;difference&nbsp;is&nbsp;that&nbsp;we&nbsp;now&nbsp;add&nbsp;varying&nbsp;degrees&nbsp;of&nbsp;noise&nbsp;to&nbsp;the&nbsp;images&nbsp;to&nbsp;make<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;it&nbsp;more&nbsp;challenging&nbsp;for&nbsp;both&nbsp;classification&nbsp;and&nbsp;regression.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;archive&nbsp;files&nbsp;you&nbsp;will&nbsp;find&nbsp;in&nbsp;the&nbsp;'data'&nbsp;subdirectory&nbsp;of&nbsp;the&nbsp;'Examples'<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;directory&nbsp;for&nbsp;this&nbsp;dataset&nbsp;are:<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(3)&nbsp;&nbsp;PurdueShapes5-10000-train-noise-20.gz<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PurdueShapes5-1000-test-noise-20.gz<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(4)&nbsp;&nbsp;PurdueShapes5-10000-train-noise-50.gz<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PurdueShapes5-1000-test-noise-50.gz<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(5)&nbsp;&nbsp;PurdueShapes5-10000-train-noise-80.gz<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PurdueShapes5-1000-test-noise-80.gz<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In&nbsp;the&nbsp;names&nbsp;of&nbsp;these&nbsp;six&nbsp;archive&nbsp;files,&nbsp;the&nbsp;numbers&nbsp;20,&nbsp;50,&nbsp;and&nbsp;80&nbsp;stand&nbsp;for<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;level&nbsp;of&nbsp;noise&nbsp;in&nbsp;the&nbsp;images.&nbsp;&nbsp;For&nbsp;example,&nbsp;20&nbsp;means&nbsp;20%&nbsp;noise.&nbsp;&nbsp;The<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;percentage&nbsp;level&nbsp;indicates&nbsp;the&nbsp;fraction&nbsp;of&nbsp;the&nbsp;color&nbsp;value&nbsp;range&nbsp;that&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;added&nbsp;as&nbsp;randomly&nbsp;generated&nbsp;noise&nbsp;to&nbsp;the&nbsp;images.&nbsp;&nbsp;The&nbsp;first&nbsp;integer&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;of&nbsp;each&nbsp;archive&nbsp;carries&nbsp;the&nbsp;same&nbsp;meaning&nbsp;as&nbsp;mentioned&nbsp;above&nbsp;for&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;regular&nbsp;PurdueShapes5&nbsp;dataset:&nbsp;It&nbsp;stands&nbsp;for&nbsp;the&nbsp;number&nbsp;of&nbsp;images&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dataset.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
<span style="color:red; font-size:80%"><strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        FOR SEMANTIC SEGMENTATION</strong></span><br>&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Showing&nbsp;interesting&nbsp;results&nbsp;with&nbsp;semantic&nbsp;segmentation&nbsp;requires&nbsp;images&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contains&nbsp;multiple&nbsp;objects&nbsp;of&nbsp;different&nbsp;types.&nbsp;&nbsp;A&nbsp;good&nbsp;semantic&nbsp;segmenter<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;would&nbsp;then&nbsp;allow&nbsp;for&nbsp;each&nbsp;object&nbsp;type&nbsp;to&nbsp;be&nbsp;segmented&nbsp;out&nbsp;separately&nbsp;from&nbsp;an<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;image.&nbsp;&nbsp;A&nbsp;network&nbsp;that&nbsp;can&nbsp;carry&nbsp;out&nbsp;such&nbsp;segmentation&nbsp;needs&nbsp;training&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;testing&nbsp;datasets&nbsp;in&nbsp;which&nbsp;the&nbsp;images&nbsp;come&nbsp;up&nbsp;with&nbsp;multiple&nbsp;objects&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;different&nbsp;types&nbsp;in&nbsp;them.&nbsp;Towards&nbsp;that&nbsp;end,&nbsp;I&nbsp;have&nbsp;created&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dataset:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(6)&nbsp;PurdueShapes5MultiObject-10000-train.gz<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PurdueShapes5MultiObject-1000-test.gz<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(7)&nbsp;PurdueShapes5MultiObject-20-train.gz<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PurdueShapes5MultiObject-20-test.gz<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;number&nbsp;that&nbsp;follows&nbsp;the&nbsp;main&nbsp;name&nbsp;string&nbsp;"PurdueShapes5MultiObject-"&nbsp;is<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;the&nbsp;number&nbsp;of&nbsp;images&nbsp;in&nbsp;the&nbsp;dataset.&nbsp;&nbsp;You&nbsp;will&nbsp;find&nbsp;the&nbsp;last&nbsp;two<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;datasets,&nbsp;with&nbsp;20&nbsp;images&nbsp;each,&nbsp;useful&nbsp;for&nbsp;debugging&nbsp;your&nbsp;logic&nbsp;for&nbsp;semantic<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;segmentation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;As&nbsp;to&nbsp;how&nbsp;the&nbsp;image&nbsp;data&nbsp;is&nbsp;stored&nbsp;in&nbsp;the&nbsp;archive&nbsp;files&nbsp;listed&nbsp;above,&nbsp;please<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;see&nbsp;the&nbsp;main&nbsp;comment&nbsp;block&nbsp;for&nbsp;the&nbsp;class<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PurdueShapes5MultiObjectDataset<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;As&nbsp;explained&nbsp;there,&nbsp;in&nbsp;addition&nbsp;to&nbsp;the&nbsp;RGB&nbsp;values&nbsp;at&nbsp;the&nbsp;pixels&nbsp;that&nbsp;are<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stored&nbsp;in&nbsp;the&nbsp;form&nbsp;of&nbsp;three&nbsp;separate&nbsp;lists&nbsp;called&nbsp;R,&nbsp;G,&nbsp;and&nbsp;B,&nbsp;the&nbsp;shapes<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;themselves&nbsp;are&nbsp;stored&nbsp;in&nbsp;the&nbsp;form&nbsp;an&nbsp;array&nbsp;of&nbsp;masks,&nbsp;each&nbsp;of&nbsp;size&nbsp;64x64,&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;each&nbsp;mask&nbsp;array&nbsp;representing&nbsp;a&nbsp;particular&nbsp;shape.&nbsp;For&nbsp;illustration,&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rectangle&nbsp;shape&nbsp;is&nbsp;represented&nbsp;by&nbsp;the&nbsp;first&nbsp;such&nbsp;array.&nbsp;And&nbsp;so&nbsp;on.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
<span style="color:red; font-size:80%"><strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        FOR TEXT CLASSIFICATION</strong></span><br>&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;My&nbsp;experiments&nbsp;tell&nbsp;me&nbsp;that,&nbsp;when&nbsp;using&nbsp;gated&nbsp;RNNs,&nbsp;the&nbsp;size&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;vocabulary&nbsp;can&nbsp;significantly&nbsp;impact&nbsp;the&nbsp;time&nbsp;it&nbsp;takes&nbsp;to&nbsp;train&nbsp;a&nbsp;neural<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;network&nbsp;for&nbsp;text&nbsp;modeling&nbsp;and&nbsp;classification.&nbsp;&nbsp;My&nbsp;goal&nbsp;was&nbsp;to&nbsp;provide&nbsp;curated<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;datasets&nbsp;extract&nbsp;from&nbsp;the&nbsp;Amazon&nbsp;user-feedback&nbsp;archive&nbsp;that&nbsp;would&nbsp;lend<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;themselves&nbsp;to&nbsp;experimentation&nbsp;on,&nbsp;say,&nbsp;your&nbsp;personal&nbsp;laptop&nbsp;with&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rudimentary&nbsp;GPU&nbsp;like&nbsp;the&nbsp;Quadro.&nbsp;&nbsp;Here&nbsp;are&nbsp;the&nbsp;new&nbsp;datasets&nbsp;you&nbsp;can&nbsp;now<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;download&nbsp;from&nbsp;the&nbsp;main&nbsp;documentation&nbsp;page&nbsp;for&nbsp;this&nbsp;module:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sentiment_dataset_train_200.tar.gz&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;vocab_size&nbsp;=&nbsp;43,285<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sentiment_dataset_test_200.tar.gz&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sentiment_dataset_train_40.tar.gz&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;vocab_size&nbsp;=&nbsp;17,001<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sentiment_dataset_test_40.tar.gz&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sentiment_dataset_train_400.tar.gz&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;vocab_size&nbsp;=&nbsp;64,350<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sentiment_dataset_test_400.tar.gz&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;As&nbsp;with&nbsp;the&nbsp;other&nbsp;datasets,&nbsp;the&nbsp;integer&nbsp;in&nbsp;the&nbsp;name&nbsp;of&nbsp;each&nbsp;dataset&nbsp;is&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;number&nbsp;of&nbsp;reviews&nbsp;collected&nbsp;from&nbsp;the&nbsp;'positive.reviews'&nbsp;and&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'negative.reviews'&nbsp;files&nbsp;for&nbsp;each&nbsp;product&nbsp;category.&nbsp;&nbsp;Therefore,&nbsp;the&nbsp;dataset<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;200&nbsp;in&nbsp;its&nbsp;name&nbsp;has&nbsp;a&nbsp;total&nbsp;of&nbsp;400&nbsp;reviews&nbsp;for&nbsp;each&nbsp;product&nbsp;category.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Also&nbsp;provided&nbsp;are&nbsp;two&nbsp;datasets&nbsp;named&nbsp;"sentiment_dataset_train_3.tar.gz"&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sentiment_dataset_test_3.tar.gz"&nbsp;just&nbsp;for&nbsp;the&nbsp;purpose&nbsp;of&nbsp;debugging&nbsp;your&nbsp;code.<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;last&nbsp;dataset,&nbsp;the&nbsp;one&nbsp;with&nbsp;400&nbsp;in&nbsp;its&nbsp;name,&nbsp;was&nbsp;added&nbsp;in&nbsp;Version&nbsp;1.1.3&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;module.<br>
&nbsp;<br>
<span style="color:blue; font-size:100%"><strong>&nbsp;&nbsp;<a id="129">    FOR Seq2Seq LEARNING</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For&nbsp;sequence-to-sequence&nbsp;learning&nbsp;with&nbsp;DLStudio,&nbsp;you&nbsp;can&nbsp;download&nbsp;an<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;English-Spanish&nbsp;translation&nbsp;corpus&nbsp;through&nbsp;the&nbsp;following&nbsp;archive:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;en_es_corpus_for_seq2sq_learning_with_DLStudio.tar.gz<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;data&nbsp;archive&nbsp;is&nbsp;a&nbsp;lightly&nbsp;curated&nbsp;version&nbsp;of&nbsp;the&nbsp;main&nbsp;dataset&nbsp;posted&nbsp;at<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"<a href="http://www.manythings.org/anki/">http://www.manythings.org/anki/</a>"&nbsp;by&nbsp;the&nbsp;folks&nbsp;at&nbsp;"tatoeba.org".&nbsp;&nbsp;My<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;alterations&nbsp;to&nbsp;the&nbsp;original&nbsp;dataset&nbsp;consist&nbsp;mainly&nbsp;of&nbsp;expanding&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;contractions&nbsp;like&nbsp;"it's",&nbsp;"I'm",&nbsp;"don't",&nbsp;"didn't",&nbsp;"you'll",&nbsp;etc.,&nbsp;into<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;their&nbsp;"it&nbsp;is",&nbsp;"i&nbsp;am",&nbsp;"do&nbsp;not",&nbsp;"did&nbsp;not",&nbsp;"you&nbsp;will",&nbsp;etc.&nbsp;The&nbsp;original<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;form&nbsp;of&nbsp;the&nbsp;dataset&nbsp;contains&nbsp;417&nbsp;such&nbsp;unique&nbsp;contractions.&nbsp;&nbsp;Another<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;alteration&nbsp;I&nbsp;made&nbsp;to&nbsp;the&nbsp;original&nbsp;data&nbsp;archive&nbsp;is&nbsp;to&nbsp;surround&nbsp;each&nbsp;sentence<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;both&nbsp;English&nbsp;and&nbsp;Spanish&nbsp;by&nbsp;the&nbsp;"SOS"&nbsp;and&nbsp;"EOS"&nbsp;tokens,&nbsp;with&nbsp;the&nbsp;former<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;standing&nbsp;for&nbsp;"Start&nbsp;of&nbsp;Sentence"&nbsp;and&nbsp;the&nbsp;latter&nbsp;for&nbsp;"End&nbsp;of&nbsp;Sentence".<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Download&nbsp;the&nbsp;above&nbsp;archive&nbsp;in&nbsp;the&nbsp;ExamplesSeq2Seq2Learning&nbsp;directory&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;execute&nbsp;the&nbsp;following&nbsp;command&nbsp;in&nbsp;that&nbsp;directory:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tar&nbsp;zxvf&nbsp;en_es_corpus_for_seq2sq_learning_with_DLStudio.tar.gz<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;command&nbsp;will&nbsp;create&nbsp;a&nbsp;'data'&nbsp;subdirectory&nbsp;in&nbsp;the&nbsp;directory<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ExamplesSeq2Seq2Learning&nbsp;and&nbsp;deposit&nbsp;the&nbsp;following&nbsp;dataset&nbsp;archive&nbsp;in&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;subdirectory:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;en_es_8_98988.tar.gz<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Now&nbsp;execute&nbsp;the&nbsp;following&nbsp;in&nbsp;the&nbsp;'data'&nbsp;directory:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tar&nbsp;zxvf&nbsp;en_es_8_98988.tar.gz<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;With&nbsp;that,&nbsp;you&nbsp;should&nbsp;be&nbsp;able&nbsp;to&nbsp;execute&nbsp;the&nbsp;Seq2SeqLearning&nbsp;based&nbsp;scripts&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;'ExamplesSeq2SeqLearning'&nbsp;directory.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:blue; font-size:100%"><strong>&nbsp;&nbsp;<a id="130">    FOR ADVERSARIAL LEARNING AND DIFFUSION</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Download&nbsp;the&nbsp;dataset&nbsp;archive<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;datasets_for_AdversarialLearning.tar.gz&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;through&nbsp;the&nbsp;link&nbsp;"Download&nbsp;the&nbsp;image&nbsp;dataset&nbsp;for&nbsp;AdversarialLearning"<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;provided&nbsp;at&nbsp;the&nbsp;top&nbsp;of&nbsp;the&nbsp;HTML&nbsp;version&nbsp;of&nbsp;this&nbsp;doc&nbsp;page&nbsp;and&nbsp;store&nbsp;it&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'ExamplesAdversarialLearning'&nbsp;directory&nbsp;of&nbsp;the&nbsp;distribution.&nbsp;&nbsp;Subsequently,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;execute&nbsp;the&nbsp;following&nbsp;command&nbsp;in&nbsp;the&nbsp;directory&nbsp;'ExamplesAdversarialLearning':<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tar&nbsp;zxvf&nbsp;datasets_for_AdversarialLearning.tar.gz<br>
&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;command&nbsp;will&nbsp;create&nbsp;a&nbsp;'dataGAN'&nbsp;subdirectory&nbsp;and&nbsp;deposit&nbsp;the&nbsp;following<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dataset&nbsp;archive&nbsp;in&nbsp;that&nbsp;subdirectory:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PurdueShapes5GAN-20000.tar.gz<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Now&nbsp;execute&nbsp;the&nbsp;following&nbsp;in&nbsp;the&nbsp;"dataGAN"&nbsp;directory:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tar&nbsp;zxvf&nbsp;PurdueShapes5GAN-20000.tar.gz<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;With&nbsp;that,&nbsp;you&nbsp;should&nbsp;be&nbsp;able&nbsp;to&nbsp;execute&nbsp;the&nbsp;adversarial&nbsp;learning&nbsp;based<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;scripts&nbsp;in&nbsp;the&nbsp;'ExamplesAdversarialLearning'&nbsp;directory.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;NOTE&nbsp;ADDED&nbsp;IN&nbsp;VERSION&nbsp;2.5.1:&nbsp;This&nbsp;dataset&nbsp;is&nbsp;also&nbsp;used&nbsp;for&nbsp;the&nbsp;three&nbsp;scripts<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;related&nbsp;to&nbsp;autoencoding&nbsp;and&nbsp;variational&nbsp;autoencoding&nbsp;in&nbsp;the&nbsp;Examples<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;directory&nbsp;of&nbsp;the&nbsp;distribution.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:blue; font-size:100%"><strong>&nbsp;&nbsp;<a id="131">    FOR DATA PREDICTION</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Download&nbsp;the&nbsp;dataset&nbsp;archive&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dataset_for_DataPrediction.tar.gz&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;into&nbsp;the&nbsp;ExamplesDataPrediction&nbsp;directory&nbsp;of&nbsp;the&nbsp;DLStudio&nbsp;distribution.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Next,&nbsp;execute&nbsp;the&nbsp;following&nbsp;command&nbsp;in&nbsp;that&nbsp;directory:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tar&nbsp;zxvf&nbsp;dataset_for_DataPrediction.tar.gz&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;That&nbsp;will&nbsp;create&nbsp;data&nbsp;directory&nbsp;named&nbsp;"dataPred"&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ExamplesDataPrediction&nbsp;directory.&nbsp;&nbsp;With&nbsp;that&nbsp;you&nbsp;should&nbsp;be&nbsp;able&nbsp;to&nbsp;execute<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;data&nbsp;prediction&nbsp;script&nbsp;in&nbsp;that&nbsp;directory.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:blue; font-size:100%"><strong>&nbsp;&nbsp;<a id="132">    FOR TRANSFORMERS</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;For&nbsp;the&nbsp;seq2seq&nbsp;learning&nbsp;part&nbsp;of&nbsp;the&nbsp;Transformers&nbsp;module&nbsp;in&nbsp;DLStudio,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;download&nbsp;the&nbsp;dataset&nbsp;archive<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;en_es_corpus_for_learning_with_Transformers.tar.gz<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;into&nbsp;the&nbsp;ExamplesTransformers&nbsp;directory&nbsp;of&nbsp;the&nbsp;DLStudio&nbsp;distribution.&nbsp;&nbsp;Next,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;execute&nbsp;the&nbsp;following&nbsp;command&nbsp;in&nbsp;that&nbsp;directory:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;tar&nbsp;zxvf&nbsp;en_es_corpus_for_learning_with_Transformers.tar.gz<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;That&nbsp;will&nbsp;create&nbsp;a&nbsp;'data'&nbsp;subdirectory&nbsp;in&nbsp;the&nbsp;ExamplesTransformers&nbsp;directory<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;deposit&nbsp;in&nbsp;that&nbsp;subdirectory&nbsp;the&nbsp;following&nbsp;archives<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;en_es_xformer_8_10000.tar.gz<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;en_es_xformer_8_90000.tar.gz<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;These&nbsp;are&nbsp;both&nbsp;derived&nbsp;from&nbsp;the&nbsp;same&nbsp;data&nbsp;source&nbsp;as&nbsp;in&nbsp;the&nbsp;dataset&nbsp;for&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;examples&nbsp;associated&nbsp;with&nbsp;the&nbsp;Seq2SeqLearning&nbsp;module.&nbsp;&nbsp;The&nbsp;first&nbsp;has&nbsp;only<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;10,000&nbsp;pars&nbsp;of&nbsp;English-Spanish&nbsp;sentences&nbsp;and&nbsp;meant&nbsp;primarily&nbsp;for&nbsp;debugging<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;purposes.&nbsp;&nbsp;The&nbsp;second&nbsp;contains&nbsp;90000&nbsp;pairs&nbsp;of&nbsp;such&nbsp;sentences.&nbsp;&nbsp;The&nbsp;number&nbsp;'8'<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;the&nbsp;dataset&nbsp;names&nbsp;means&nbsp;that&nbsp;no&nbsp;sentence&nbsp;contains&nbsp;more&nbsp;than&nbsp;8&nbsp;real&nbsp;words.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;With&nbsp;the&nbsp;"SOS"&nbsp;and&nbsp;"EOS"&nbsp;tokens&nbsp;used&nbsp;as&nbsp;sentence&nbsp;delimiters,&nbsp;the&nbsp;maximum<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;number&nbsp;of&nbsp;words&nbsp;in&nbsp;each&nbsp;sentence&nbsp;in&nbsp;either&nbsp;language&nbsp;is&nbsp;10.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:red; font-size:150%"><strong><a id="133">BUGS</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Please&nbsp;notify&nbsp;the&nbsp;author&nbsp;if&nbsp;you&nbsp;encounter&nbsp;any&nbsp;bugs.&nbsp;&nbsp;When&nbsp;sending&nbsp;email,&nbsp;please<br>
&nbsp;&nbsp;&nbsp;&nbsp;place&nbsp;the&nbsp;string&nbsp;'DLStudio'&nbsp;in&nbsp;the&nbsp;subject&nbsp;line&nbsp;to&nbsp;get&nbsp;past&nbsp;the&nbsp;author's&nbsp;spam<br>
&nbsp;&nbsp;&nbsp;&nbsp;filter.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:red; font-size:150%"><strong><a id="134">ACKNOWLEDGMENTS</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Thanks&nbsp;to&nbsp;Praneet&nbsp;Singh&nbsp;and&nbsp;Noureldin&nbsp;Hendy&nbsp;for&nbsp;their&nbsp;comments&nbsp;related&nbsp;to&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;buggy&nbsp;behavior&nbsp;of&nbsp;the&nbsp;module&nbsp;when&nbsp;using&nbsp;the&nbsp;'depth'&nbsp;parameter&nbsp;to&nbsp;change&nbsp;the&nbsp;size<br>
&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;a&nbsp;network.&nbsp;Thanks&nbsp;also&nbsp;go&nbsp;to&nbsp;Christina&nbsp;Eberhardt&nbsp;for&nbsp;reminding&nbsp;me&nbsp;that&nbsp;I<br>
&nbsp;&nbsp;&nbsp;&nbsp;needed&nbsp;to&nbsp;change&nbsp;the&nbsp;value&nbsp;of&nbsp;the&nbsp;'dataroot'&nbsp;parameter&nbsp;in&nbsp;my&nbsp;Examples&nbsp;scripts<br>
&nbsp;&nbsp;&nbsp;&nbsp;prior&nbsp;to&nbsp;packaging&nbsp;a&nbsp;new&nbsp;distribution.&nbsp;&nbsp;Their&nbsp;feedback&nbsp;led&nbsp;to&nbsp;Version&nbsp;1.1.1&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;this&nbsp;module.&nbsp;&nbsp;Regarding&nbsp;the&nbsp;changes&nbsp;made&nbsp;in&nbsp;Version&nbsp;1.1.4,&nbsp;one&nbsp;of&nbsp;them&nbsp;is&nbsp;a&nbsp;fix<br>
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;the&nbsp;bug&nbsp;found&nbsp;by&nbsp;Serdar&nbsp;Ozguc&nbsp;in&nbsp;Version&nbsp;1.1.3.&nbsp;Thanks&nbsp;Serdar.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Version&nbsp;2.0.3:&nbsp;I&nbsp;owe&nbsp;thanks&nbsp;to&nbsp;Ankit&nbsp;Manerikar&nbsp;for&nbsp;many&nbsp;wonderful&nbsp;conversations<br>
&nbsp;&nbsp;&nbsp;&nbsp;related&nbsp;to&nbsp;the&nbsp;rapidly&nbsp;evolving&nbsp;area&nbsp;of&nbsp;generative&nbsp;adversarial&nbsp;networks&nbsp;in&nbsp;deep<br>
&nbsp;&nbsp;&nbsp;&nbsp;learning.&nbsp;&nbsp;It&nbsp;is&nbsp;obviously&nbsp;important&nbsp;to&nbsp;read&nbsp;research&nbsp;papers&nbsp;to&nbsp;become&nbsp;familiar<br>
&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;the&nbsp;goings-on&nbsp;in&nbsp;an&nbsp;area.&nbsp;&nbsp;However,&nbsp;if&nbsp;you&nbsp;wish&nbsp;to&nbsp;also&nbsp;develop&nbsp;deep<br>
&nbsp;&nbsp;&nbsp;&nbsp;intuitions&nbsp;in&nbsp;those&nbsp;concepts,&nbsp;nothing&nbsp;can&nbsp;beat&nbsp;having&nbsp;great&nbsp;conversations&nbsp;with&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;strong&nbsp;researcher&nbsp;like&nbsp;Ankit.&nbsp;&nbsp;Ankit&nbsp;is&nbsp;finishing&nbsp;his&nbsp;Ph.D.&nbsp;in&nbsp;the&nbsp;Robot&nbsp;Vision<br>
&nbsp;&nbsp;&nbsp;&nbsp;Lab&nbsp;at&nbsp;Purdue.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Version&nbsp;2.2.2:&nbsp;My&nbsp;laboratory's&nbsp;(RVL)&nbsp;journey&nbsp;into&nbsp;the&nbsp;world&nbsp;of&nbsp;transformers&nbsp;began<br>
&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;a&nbsp;series&nbsp;of&nbsp;lab&nbsp;seminars&nbsp;by&nbsp;Constantine&nbsp;Roros&nbsp;and&nbsp;Rahul&nbsp;Deshmukh.&nbsp;&nbsp;Several<br>
&nbsp;&nbsp;&nbsp;&nbsp;subsequent&nbsp;conversations&nbsp;with&nbsp;them&nbsp;were&nbsp;instrumental&nbsp;in&nbsp;helping&nbsp;me&nbsp;improve&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;understanding&nbsp;I&nbsp;had&nbsp;gained&nbsp;from&nbsp;the&nbsp;seminars.&nbsp;&nbsp;Additional&nbsp;conversations&nbsp;with<br>
&nbsp;&nbsp;&nbsp;&nbsp;Rahul&nbsp;about&nbsp;the&nbsp;issue&nbsp;of&nbsp;masking&nbsp;were&nbsp;important&nbsp;to&nbsp;how&nbsp;I&nbsp;eventually&nbsp;implemented<br>
&nbsp;&nbsp;&nbsp;&nbsp;those&nbsp;ideas&nbsp;in&nbsp;my&nbsp;code.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Rahul&nbsp;Deshmukh&nbsp;discovered&nbsp;the&nbsp;reason&nbsp;as&nbsp;to&nbsp;why&nbsp;my&nbsp;implementation&nbsp;of&nbsp;the&nbsp;skip<br>
&nbsp;&nbsp;&nbsp;&nbsp;connection&nbsp;code&nbsp;was&nbsp;not&nbsp;working&nbsp;with&nbsp;the&nbsp;more&nbsp;recent&nbsp;versions&nbsp;of&nbsp;PyTorch.&nbsp;&nbsp;My<br>
&nbsp;&nbsp;&nbsp;&nbsp;problem&nbsp;was&nbsp;using&nbsp;in-place&nbsp;operations&nbsp;in&nbsp;the&nbsp;forward()&nbsp;of&nbsp;the&nbsp;networks&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;called&nbsp;for&nbsp;connection&nbsp;skipping.&nbsp;This&nbsp;led&nbsp;to&nbsp;the&nbsp;release&nbsp;of&nbsp;Version&nbsp;2.3.3&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;DLStudio.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;main&nbsp;reason&nbsp;for&nbsp;Version&nbsp;2.3.4&nbsp;was&nbsp;my&nbsp;new&nbsp;design&nbsp;for&nbsp;the&nbsp;SkipBlock&nbsp;class&nbsp;and<br>
&nbsp;&nbsp;&nbsp;&nbsp;also&nbsp;my&nbsp;easier-to-understand&nbsp;code&nbsp;for&nbsp;the&nbsp;BMEnet&nbsp;class&nbsp;that&nbsp;showcases&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;importance&nbsp;of&nbsp;providing&nbsp;shortcut&nbsp;paths&nbsp;in&nbsp;a&nbsp;computational&nbsp;graph&nbsp;using&nbsp;skip<br>
&nbsp;&nbsp;&nbsp;&nbsp;blocks.&nbsp;&nbsp;After&nbsp;I&nbsp;broadcast&nbsp;that&nbsp;code&nbsp;to&nbsp;the&nbsp;students&nbsp;in&nbsp;my&nbsp;DL&nbsp;class&nbsp;at&nbsp;Purdue,<br>
&nbsp;&nbsp;&nbsp;&nbsp;Cheng-Hao&nbsp;Chen&nbsp;reported&nbsp;that&nbsp;when&nbsp;a&nbsp;SkipBlock&nbsp;was&nbsp;asked&nbsp;to&nbsp;change&nbsp;the&nbsp;channels<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;its&nbsp;input&nbsp;to&nbsp;its&nbsp;output&nbsp;but&nbsp;without&nbsp;downsampling&nbsp;the&nbsp;input,&nbsp;that&nbsp;elicited&nbsp;an<br>
&nbsp;&nbsp;&nbsp;&nbsp;error&nbsp;from&nbsp;the&nbsp;system.&nbsp;&nbsp;Cheng-Hao&nbsp;also&nbsp;provided&nbsp;a&nbsp;correction&nbsp;for&nbsp;the&nbsp;error.<br>
&nbsp;&nbsp;&nbsp;&nbsp;Thanks,&nbsp;Cheng-Hao!<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Aditya&nbsp;Chauhan&nbsp;proved&nbsp;to&nbsp;be&nbsp;a&nbsp;great&nbsp;sounding&nbsp;board&nbsp;in&nbsp;my&nbsp;journey&nbsp;into&nbsp;the&nbsp;land&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;diffusion&nbsp;that&nbsp;led&nbsp;to&nbsp;Version&nbsp;2.4.2.&nbsp;&nbsp;I&nbsp;particularly&nbsp;appreciated&nbsp;Aditya's&nbsp;help&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;understanding&nbsp;how&nbsp;the&nbsp;attention&nbsp;mechanism&nbsp;worked&nbsp;in&nbsp;the&nbsp;OpenAI&nbsp;code&nbsp;library&nbsp;at<br>
&nbsp;&nbsp;&nbsp;&nbsp;GitHub.&nbsp;&nbsp;Aditya&nbsp;is&nbsp;working&nbsp;for&nbsp;his&nbsp;PhD&nbsp;in&nbsp;RVL.&nbsp;&nbsp;Thanks,&nbsp;Aditya!<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Version&nbsp;2.5.0&nbsp;is&nbsp;a&nbsp;result&nbsp;of&nbsp;Rahul&nbsp;Deshmukh&nbsp;insisting&nbsp;that&nbsp;the&nbsp;number&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;learnable&nbsp;parameters&nbsp;in&nbsp;a&nbsp;transformer&nbsp;must&nbsp;not&nbsp;depend&nbsp;on&nbsp;the&nbsp;maximum&nbsp;expected<br>
&nbsp;&nbsp;&nbsp;&nbsp;length&nbsp;for&nbsp;the&nbsp;input&nbsp;sequence&nbsp;---&nbsp;which&nbsp;was&nbsp;not&nbsp;the&nbsp;case&nbsp;with&nbsp;the&nbsp;previous<br>
&nbsp;&nbsp;&nbsp;&nbsp;versions&nbsp;of&nbsp;the&nbsp;transformer&nbsp;code&nbsp;in&nbsp;DLStudio.&nbsp;As&nbsp;it&nbsp;turned&nbsp;out,&nbsp;my&nbsp;implementation<br>
&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;the&nbsp;FFN&nbsp;layer&nbsp;in&nbsp;the&nbsp;basic&nbsp;transformer&nbsp;encoder/decoder&nbsp;blocks&nbsp;was&nbsp;not&nbsp;in<br>
&nbsp;&nbsp;&nbsp;&nbsp;keeping&nbsp;with&nbsp;the&nbsp;design&nbsp;laid&nbsp;out&nbsp;in&nbsp;the&nbsp;original&nbsp;paper&nbsp;by&nbsp;Vaswani&nbsp;et&nbsp;al.&nbsp;&nbsp;This<br>
&nbsp;&nbsp;&nbsp;&nbsp;problem&nbsp;is&nbsp;now&nbsp;fixed&nbsp;in&nbsp;Version&nbsp;2.5.0.&nbsp;Rahul&nbsp;is&nbsp;at&nbsp;the&nbsp;very&nbsp;end&nbsp;of&nbsp;his&nbsp;Ph.D<br>
&nbsp;&nbsp;&nbsp;&nbsp;program&nbsp;in&nbsp;RVL&nbsp;at&nbsp;Purdue.&nbsp;&nbsp;Thanks,&nbsp;Rahul!<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;main&nbsp;reason&nbsp;for&nbsp;Version&nbsp;2.5.3&nbsp;is&nbsp;Aditya&nbsp;Chauhan's&nbsp;strongly&nbsp;held&nbsp;opinion&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;nn.Softmax&nbsp;normalization&nbsp;of&nbsp;the&nbsp;"Q.K^T"&nbsp;dot-products&nbsp;for&nbsp;Attention<br>
&nbsp;&nbsp;&nbsp;&nbsp;calculations&nbsp;must&nbsp;be&nbsp;along&nbsp;the&nbsp;word-axis&nbsp;for&nbsp;the&nbsp;K-tensors&nbsp;and&nbsp;NOT&nbsp;along&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;word-axis&nbsp;for&nbsp;the&nbsp;Q-tensors.&nbsp;His&nbsp;reasoning&nbsp;is&nbsp;compelling&nbsp;because,&nbsp;with&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;normalization&nbsp;along&nbsp;the&nbsp;K-axis,&nbsp;the&nbsp;individual&nbsp;rows&nbsp;of&nbsp;the&nbsp;normalized&nbsp;dot-product<br>
&nbsp;&nbsp;&nbsp;&nbsp;possess&nbsp;a&nbsp;more&nbsp;natural&nbsp;probability&nbsp;based&nbsp;interpretation:&nbsp;For&nbsp;each&nbsp;word&nbsp;in&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;Query&nbsp;tensor,&nbsp;the&nbsp;numbers&nbsp;in&nbsp;the&nbsp;corresponding&nbsp;row&nbsp;in&nbsp;the&nbsp;normalized&nbsp;dot&nbsp;product<br>
&nbsp;&nbsp;&nbsp;&nbsp;are&nbsp;the&nbsp;probabilities&nbsp;of&nbsp;the&nbsp;other&nbsp;words&nbsp;being&nbsp;relevant&nbsp;to&nbsp;the&nbsp;query&nbsp;word.&nbsp;&nbsp;I<br>
&nbsp;&nbsp;&nbsp;&nbsp;wish&nbsp;to&nbsp;thank&nbsp;Aditya&nbsp;for&nbsp;sharing&nbsp;his&nbsp;insights&nbsp;with&nbsp;me.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:red; font-size:150%"><strong><a id="135">ABOUT THE AUTHOR</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;author,&nbsp;Avinash&nbsp;Kak,&nbsp;is&nbsp;a&nbsp;professor&nbsp;of&nbsp;Electrical&nbsp;and&nbsp;Computer&nbsp;Engineering<br>
&nbsp;&nbsp;&nbsp;&nbsp;at&nbsp;Purdue&nbsp;University.&nbsp;&nbsp;For&nbsp;all&nbsp;issues&nbsp;related&nbsp;to&nbsp;this&nbsp;module,&nbsp;contact&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;author&nbsp;at&nbsp;kak@purdue.edu.&nbsp;If&nbsp;you&nbsp;send&nbsp;email,&nbsp;please&nbsp;place&nbsp;the&nbsp;string<br>
&nbsp;&nbsp;&nbsp;&nbsp;"DLStudio"&nbsp;in&nbsp;your&nbsp;subject&nbsp;line&nbsp;to&nbsp;get&nbsp;past&nbsp;the&nbsp;author's&nbsp;spam&nbsp;filter.<br>
&nbsp;<br>
&nbsp;<br>
<span style="color:red; font-size:150%"><strong><a id="136">COPYRIGHT</a></strong></span><br>&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Python&nbsp;Software&nbsp;Foundation&nbsp;License<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Copyright&nbsp;2025&nbsp;Avinash&nbsp;Kak<br>
&nbsp;<br>
@endofdocs</tt></p>
<p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#aa55cc">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial"><big><strong>Imported Modules</strong></big></font></td></tr>
    
<tr><td bgcolor="#aa55cc"><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</tt></td><td>&nbsp;</td>
<td width="100%"><table width="100%" summary="list"><tr><td width="25%" valign=top><a href="torch.nn.functional.html">torch.nn.functional</a><br>
<a href="PIL.Image.html">PIL.Image</a><br>
<a href="PIL.ImageFilter.html">PIL.ImageFilter</a><br>
<a href="copy.html">copy</a><br>
<a href="glob.html">glob</a><br>
<a href="gzip.html">gzip</a><br>
</td><td width="25%" valign=top><a href="logging.html">logging</a><br>
<a href="math.html">math</a><br>
<a href="torch.nn.html">torch.nn</a><br>
<a href="numpy.html">numpy</a><br>
<a href="numbers.html">numbers</a><br>
<a href="torch.optim.html">torch.optim</a><br>
</td><td width="25%" valign=top><a href="os.html">os</a><br>
<a href="pickle.html">pickle</a><br>
<a href="matplotlib.pyplot.html">matplotlib.pyplot</a><br>
<a href="pymsgbox.html">pymsgbox</a><br>
<a href="random.html">random</a><br>
<a href="re.html">re</a><br>
</td><td width="25%" valign=top><a href="sys.html">sys</a><br>
<a href="time.html">time</a><br>
<a href="torch.html">torch</a><br>
<a href="torchmetrics.html">torchmetrics</a><br>
<a href="torchvision.html">torchvision</a><br>
<a href="torchvision.transforms.html">torchvision.transforms</a><br>
</td></tr></table></td></tr></table><p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ee77aa">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial"><big><strong>Classes</strong></big></font></td></tr>
    
<tr><td bgcolor="#ee77aa"><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</tt></td><td>&nbsp;</td>
<td width="100%"><dl>
<dt><font face="helvetica, arial"><a href="builtins.html#object">builtins.object</a>
</font></dt><dd>
<dl>
<dt><font face="helvetica, arial"><a href="DLStudio.html#DLStudio">DLStudio</a>
</font></dt></dl>
</dd>
</dl>
 <p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ffc8d8">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#000000" face="helvetica, arial"><a name="DLStudio">class <strong>DLStudio</strong></a>(<a href="builtins.html#object">builtins.object</a>)</font></td></tr>
    
<tr bgcolor="#ffc8d8"><td rowspan=2><tt>&nbsp;&nbsp;&nbsp;</tt></td>
<td colspan=2><tt>DLStudio(*args,&nbsp;**kwargs)<br>
&nbsp;<br>
<br>&nbsp;</tt></td></tr>
<tr><td>&nbsp;</td>
<td width="100%">Methods defined here:<br>
<dl><dt><a name="DLStudio-__init__"><strong>__init__</strong></a>(self, *args, **kwargs)</dt><dd><tt>Initialize&nbsp;self.&nbsp;&nbsp;See&nbsp;help(type(self))&nbsp;for&nbsp;accurate&nbsp;signature.</tt></dd></dl>

<dl><dt><a name="DLStudio-build_convo_layers"><strong>build_convo_layers</strong></a>(self, configs_for_all_convo_layers)</dt></dl>

<dl><dt><a name="DLStudio-build_fc_layers"><strong>build_fc_layers</strong></a>(self)</dt></dl>

<dl><dt><a name="DLStudio-check_a_sampling_of_images"><strong>check_a_sampling_of_images</strong></a>(self)</dt><dd><tt>Displays&nbsp;the&nbsp;first&nbsp;batch_size&nbsp;number&nbsp;of&nbsp;images&nbsp;in&nbsp;your&nbsp;dataset.</tt></dd></dl>

<dl><dt><a name="DLStudio-display_tensor_as_image"><strong>display_tensor_as_image</strong></a>(self, tensor, title='')</dt><dd><tt>This&nbsp;method&nbsp;converts&nbsp;the&nbsp;argument&nbsp;tensor&nbsp;into&nbsp;a&nbsp;photo&nbsp;image&nbsp;that&nbsp;you&nbsp;can&nbsp;display<br>
in&nbsp;your&nbsp;terminal&nbsp;screen.&nbsp;It&nbsp;can&nbsp;convert&nbsp;tensors&nbsp;of&nbsp;three&nbsp;different&nbsp;shapes<br>
into&nbsp;images:&nbsp;(3,H,W),&nbsp;(1,H,W),&nbsp;and&nbsp;(H,W),&nbsp;where&nbsp;H,&nbsp;for&nbsp;height,&nbsp;stands&nbsp;for&nbsp;the<br>
number&nbsp;of&nbsp;pixels&nbsp;in&nbsp;the&nbsp;vertical&nbsp;direction&nbsp;and&nbsp;W,&nbsp;for&nbsp;width,&nbsp;for&nbsp;the&nbsp;same<br>
along&nbsp;the&nbsp;horizontal&nbsp;direction.&nbsp;&nbsp;When&nbsp;the&nbsp;first&nbsp;element&nbsp;of&nbsp;the&nbsp;shape&nbsp;is&nbsp;3,<br>
that&nbsp;means&nbsp;that&nbsp;the&nbsp;tensor&nbsp;represents&nbsp;a&nbsp;color&nbsp;image&nbsp;in&nbsp;which&nbsp;each&nbsp;pixel&nbsp;in<br>
the&nbsp;(H,W)&nbsp;plane&nbsp;has&nbsp;three&nbsp;values&nbsp;for&nbsp;the&nbsp;three&nbsp;color&nbsp;channels.&nbsp;&nbsp;On&nbsp;the&nbsp;other<br>
hand,&nbsp;when&nbsp;the&nbsp;first&nbsp;element&nbsp;is&nbsp;1,&nbsp;that&nbsp;stands&nbsp;for&nbsp;a&nbsp;tensor&nbsp;that&nbsp;will&nbsp;be<br>
shown&nbsp;as&nbsp;a&nbsp;grayscale&nbsp;image.&nbsp;&nbsp;And&nbsp;when&nbsp;the&nbsp;shape&nbsp;is&nbsp;just&nbsp;(H,W),&nbsp;that&nbsp;is<br>
automatically&nbsp;taken&nbsp;to&nbsp;be&nbsp;for&nbsp;a&nbsp;grayscale&nbsp;image.</tt></dd></dl>

<dl><dt><a name="DLStudio-imshow"><strong>imshow</strong></a>(self, img)</dt><dd><tt>called&nbsp;by&nbsp;<a href="#DLStudio-display_tensor_as_image">display_tensor_as_image</a>()&nbsp;for&nbsp;displaying&nbsp;the&nbsp;image</tt></dd></dl>

<dl><dt><a name="DLStudio-load_cifar_10_dataset"><strong>load_cifar_10_dataset</strong></a>(self)</dt><dd><tt>In&nbsp;the&nbsp;code&nbsp;shown&nbsp;below,&nbsp;the&nbsp;call&nbsp;to&nbsp;"ToTensor()"&nbsp;converts&nbsp;the&nbsp;usual&nbsp;int&nbsp;range&nbsp;0-255&nbsp;for&nbsp;pixel&nbsp;<br>
values&nbsp;to&nbsp;0-1.0&nbsp;float&nbsp;vals&nbsp;and&nbsp;then&nbsp;the&nbsp;call&nbsp;to&nbsp;"Normalize()"&nbsp;changes&nbsp;the&nbsp;range&nbsp;to&nbsp;-1.0-1.0&nbsp;float&nbsp;<br>
vals.&nbsp;For&nbsp;additional&nbsp;explanation&nbsp;of&nbsp;the&nbsp;call&nbsp;to&nbsp;"tvt.ToTensor()",&nbsp;see&nbsp;Slide&nbsp;31&nbsp;of&nbsp;my&nbsp;Week&nbsp;2&nbsp;<br>
slides&nbsp;at&nbsp;the&nbsp;DL&nbsp;course&nbsp;website.&nbsp;&nbsp;And&nbsp;see&nbsp;Slides&nbsp;32&nbsp;and&nbsp;33&nbsp;for&nbsp;the&nbsp;syntax&nbsp;<br>
"tvt.Normalize((0.5,&nbsp;0.5,&nbsp;0.5),&nbsp;(0.5,&nbsp;0.5,&nbsp;0.5))".&nbsp;&nbsp;In&nbsp;this&nbsp;call,&nbsp;the&nbsp;three&nbsp;numbers&nbsp;in&nbsp;the<br>
first&nbsp;tuple&nbsp;change&nbsp;the&nbsp;means&nbsp;in&nbsp;the&nbsp;three&nbsp;color&nbsp;channels&nbsp;and&nbsp;the&nbsp;three&nbsp;numbers&nbsp;in&nbsp;the&nbsp;second&nbsp;<br>
tuple&nbsp;change&nbsp;the&nbsp;standard&nbsp;deviations&nbsp;according&nbsp;to&nbsp;the&nbsp;formula:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;image_channel_val&nbsp;=&nbsp;(image_channel_val&nbsp;-&nbsp;mean)&nbsp;/&nbsp;std<br>
&nbsp;<br>
The&nbsp;end&nbsp;result&nbsp;is&nbsp;that&nbsp;the&nbsp;values&nbsp;in&nbsp;the&nbsp;image&nbsp;tensor&nbsp;will&nbsp;be&nbsp;normalized&nbsp;to&nbsp;fall&nbsp;between&nbsp;-1.0&nbsp;<br>
and&nbsp;+1.0.&nbsp;If&nbsp;needed&nbsp;we&nbsp;can&nbsp;do&nbsp;inverse&nbsp;normalization&nbsp;&nbsp;by<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;image_channel_val&nbsp;&nbsp;=&nbsp;&nbsp;&nbsp;(image_channel_val&nbsp;*&nbsp;std)&nbsp;+&nbsp;mean</tt></dd></dl>

<dl><dt><a name="DLStudio-load_cifar_10_dataset_with_augmentation"><strong>load_cifar_10_dataset_with_augmentation</strong></a>(self)</dt><dd><tt>In&nbsp;general,&nbsp;we&nbsp;want&nbsp;to&nbsp;do&nbsp;data&nbsp;augmentation&nbsp;for&nbsp;training:</tt></dd></dl>

<dl><dt><a name="DLStudio-parse_config_string_for_convo_layers"><strong>parse_config_string_for_convo_layers</strong></a>(self)</dt><dd><tt>Each&nbsp;collection&nbsp;of&nbsp;'n'&nbsp;otherwise&nbsp;identical&nbsp;layers&nbsp;in&nbsp;a&nbsp;convolutional&nbsp;network&nbsp;is&nbsp;<br>
specified&nbsp;by&nbsp;a&nbsp;string&nbsp;that&nbsp;looks&nbsp;like:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"nx[a,b,c,d]-MaxPool(k)"<br>
where&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;=&nbsp;&nbsp;num&nbsp;of&nbsp;this&nbsp;type&nbsp;of&nbsp;convo&nbsp;layer<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;=&nbsp;&nbsp;number&nbsp;of&nbsp;out_channels&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[in_channels&nbsp;determined&nbsp;by&nbsp;prev&nbsp;layer]&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;b,c&nbsp;&nbsp;&nbsp;&nbsp;=&nbsp;&nbsp;kernel&nbsp;for&nbsp;this&nbsp;layer&nbsp;is&nbsp;of&nbsp;size&nbsp;(b,c)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[b&nbsp;along&nbsp;height,&nbsp;c&nbsp;along&nbsp;width]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;d&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;=&nbsp;&nbsp;stride&nbsp;for&nbsp;convolutions<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;k&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;=&nbsp;&nbsp;maxpooling&nbsp;over&nbsp;kxk&nbsp;patches&nbsp;with&nbsp;stride&nbsp;of&nbsp;k<br>
&nbsp;<br>
Example:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"n1x[a1,b1,c1,d1]-MaxPool(k1)&nbsp;&nbsp;n2x[a2,b2,c2,d2]-MaxPool(k2)"</tt></dd></dl>

<dl><dt><a name="DLStudio-run_code_for_testing"><strong>run_code_for_testing</strong></a>(self, net, display_images=False)</dt></dl>

<dl><dt><a name="DLStudio-run_code_for_training"><strong>run_code_for_training</strong></a>(self, net, display_images=False)</dt></dl>

<dl><dt><a name="DLStudio-save_model"><strong>save_model</strong></a>(self, model)</dt><dd><tt>Save&nbsp;the&nbsp;trained&nbsp;model&nbsp;to&nbsp;a&nbsp;disk&nbsp;file</tt></dd></dl>

<hr>
Data descriptors defined here:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><tt>dictionary&nbsp;for&nbsp;instance&nbsp;variables&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<dl><dt><strong>__weakref__</strong></dt>
<dd><tt>list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<hr>
Data and other attributes defined here:<br>
<dl><dt><strong>AttentionHead</strong> = &lt;class 'DLStudio.DLStudio.AttentionHead'&gt;<dd><tt>Borrowed&nbsp;from&nbsp;the&nbsp;Transformers&nbsp;module&nbsp;of&nbsp;DLStudio</tt></dl>

<dl><dt><strong>Autoencoder</strong> = &lt;class 'DLStudio.DLStudio.Autoencoder'&gt;<dd><tt>&nbsp;The&nbsp;man&nbsp;reason&nbsp;for&nbsp;the&nbsp;existence&nbsp;of&nbsp;this&nbsp;inner&nbsp;class&nbsp;in&nbsp;DLStudio&nbsp;is&nbsp;for&nbsp;it&nbsp;to&nbsp;serve&nbsp;as&nbsp;the&nbsp;base&nbsp;class&nbsp;for&nbsp;VAE&nbsp;<br>
&nbsp;(Variational&nbsp;Auto-Encoder).&nbsp;&nbsp;That&nbsp;way,&nbsp;the&nbsp;VAE&nbsp;class&nbsp;can&nbsp;focus&nbsp;exclusively&nbsp;on&nbsp;the&nbsp;random-sampling&nbsp;logic&nbsp;<br>
&nbsp;specific&nbsp;to&nbsp;variational&nbsp;encoding&nbsp;while&nbsp;the&nbsp;base&nbsp;class&nbsp;Autoencoder&nbsp;does&nbsp;the&nbsp;convolutional&nbsp;and&nbsp;<br>
&nbsp;transpose-convolutional&nbsp;heavy&nbsp;lifting&nbsp;associated&nbsp;with&nbsp;the&nbsp;usual&nbsp;encoding-decoding&nbsp;of&nbsp;image&nbsp;data.<br>
&nbsp;<br>
Class&nbsp;Path:&nbsp;&nbsp;&nbsp;DLStudio&nbsp;&nbsp;-&gt;&nbsp;&nbsp;Autoencoder</tt></dl>

<dl><dt><strong>BMEnet</strong> = &lt;class 'DLStudio.DLStudio.BMEnet'&gt;<dd><tt>This&nbsp;educational&nbsp;class&nbsp;is&nbsp;meant&nbsp;for&nbsp;illustrating&nbsp;the&nbsp;concepts&nbsp;related&nbsp;to&nbsp;the&nbsp;<br>
use&nbsp;of&nbsp;skip&nbsp;connections&nbsp;in&nbsp;neural&nbsp;network.&nbsp;&nbsp;It&nbsp;is&nbsp;now&nbsp;well&nbsp;known&nbsp;that&nbsp;deep<br>
networks&nbsp;are&nbsp;difficult&nbsp;to&nbsp;train&nbsp;because&nbsp;of&nbsp;the&nbsp;vanishing&nbsp;gradients&nbsp;problem.<br>
What&nbsp;that&nbsp;means&nbsp;is&nbsp;that&nbsp;as&nbsp;the&nbsp;depth&nbsp;of&nbsp;network&nbsp;increases,&nbsp;the&nbsp;loss&nbsp;gradients<br>
calculated&nbsp;for&nbsp;the&nbsp;early&nbsp;layers&nbsp;become&nbsp;more&nbsp;and&nbsp;more&nbsp;muted,&nbsp;which&nbsp;suppresses<br>
the&nbsp;learning&nbsp;of&nbsp;the&nbsp;parameters&nbsp;in&nbsp;those&nbsp;layers.&nbsp;&nbsp;An&nbsp;important&nbsp;mitigation<br>
strategy&nbsp;for&nbsp;addressing&nbsp;this&nbsp;problem&nbsp;consists&nbsp;of&nbsp;creating&nbsp;a&nbsp;CNN&nbsp;using&nbsp;blocks<br>
with&nbsp;skip&nbsp;connections.<br>
&nbsp;<br>
With&nbsp;the&nbsp;code&nbsp;shown&nbsp;in&nbsp;this&nbsp;inner&nbsp;class&nbsp;of&nbsp;the&nbsp;module,&nbsp;you&nbsp;can&nbsp;now&nbsp;experiment&nbsp;with<br>
skip&nbsp;connections&nbsp;in&nbsp;a&nbsp;CNN&nbsp;to&nbsp;see&nbsp;how&nbsp;a&nbsp;deep&nbsp;network&nbsp;with&nbsp;this&nbsp;feature&nbsp;might&nbsp;improve<br>
the&nbsp;classification&nbsp;results.&nbsp;&nbsp;As&nbsp;you&nbsp;will&nbsp;see&nbsp;in&nbsp;the&nbsp;code&nbsp;shown&nbsp;below,&nbsp;the&nbsp;network<br>
that&nbsp;allows&nbsp;you&nbsp;to&nbsp;construct&nbsp;a&nbsp;CNN&nbsp;with&nbsp;skip&nbsp;connections&nbsp;is&nbsp;named&nbsp;BMEnet.&nbsp;&nbsp;As&nbsp;shown<br>
in&nbsp;the&nbsp;script&nbsp;playing_with_skip_connections.py&nbsp;in&nbsp;the&nbsp;Examples&nbsp;directory&nbsp;of&nbsp;the<br>
distribution,&nbsp;you&nbsp;can&nbsp;easily&nbsp;create&nbsp;a&nbsp;CNN&nbsp;with&nbsp;arbitrary&nbsp;depth&nbsp;just&nbsp;by&nbsp;using&nbsp;the<br>
"depth"&nbsp;constructor&nbsp;option&nbsp;for&nbsp;the&nbsp;BMEnet&nbsp;class.&nbsp;&nbsp;The&nbsp;basic&nbsp;block&nbsp;of&nbsp;the&nbsp;network<br>
constructed&nbsp;by&nbsp;BMEnet&nbsp;is&nbsp;called&nbsp;SkipBlock&nbsp;which,&nbsp;very&nbsp;much&nbsp;like&nbsp;the&nbsp;BasicBlock&nbsp;in<br>
ResNet-18,&nbsp;has&nbsp;a&nbsp;couple&nbsp;of&nbsp;convolutional&nbsp;layers&nbsp;whose&nbsp;output&nbsp;is&nbsp;combined&nbsp;with&nbsp;the<br>
input&nbsp;to&nbsp;the&nbsp;block.<br>
&nbsp;<br>
Note&nbsp;that&nbsp;the&nbsp;value&nbsp;given&nbsp;to&nbsp;the&nbsp;"depth"&nbsp;constructor&nbsp;option&nbsp;for&nbsp;the&nbsp;BMEnet&nbsp;class<br>
does&nbsp;NOT&nbsp;translate&nbsp;directly&nbsp;into&nbsp;the&nbsp;actual&nbsp;depth&nbsp;of&nbsp;the&nbsp;CNN.&nbsp;[Again,&nbsp;see&nbsp;the&nbsp;script<br>
playing_with_skip_connections.py&nbsp;in&nbsp;the&nbsp;Examples&nbsp;directory&nbsp;for&nbsp;how&nbsp;to&nbsp;use&nbsp;this<br>
option.]&nbsp;The&nbsp;value&nbsp;of&nbsp;"depth"&nbsp;is&nbsp;translated&nbsp;into&nbsp;how&nbsp;many&nbsp;"same&nbsp;input&nbsp;and&nbsp;output<br>
channels"&nbsp;and&nbsp;the&nbsp;"same&nbsp;input&nbsp;and&nbsp;output&nbsp;sizes"&nbsp;instances&nbsp;of&nbsp;SkipBlock&nbsp;to&nbsp;use<br>
between&nbsp;successive&nbsp;instances&nbsp;of&nbsp;downsampling&nbsp;and&nbsp;channel-doubling&nbsp;instances&nbsp;of<br>
SkipBlock.<br>
&nbsp;<br>
Class&nbsp;Path:&nbsp;DLStudio&nbsp;-&gt;&nbsp;BMEnet</tt></dl>

<dl><dt><strong>BasicDecoderWithMasking</strong> = &lt;class 'DLStudio.DLStudio.BasicDecoderWithMasking'&gt;<dd><tt>Borrowed&nbsp;from&nbsp;the&nbsp;Transformers&nbsp;module&nbsp;of&nbsp;DLStudio</tt></dl>

<dl><dt><strong>CustomDataLoading</strong> = &lt;class 'DLStudio.DLStudio.CustomDataLoading'&gt;<dd><tt>This&nbsp;is&nbsp;a&nbsp;testbed&nbsp;for&nbsp;experimenting&nbsp;with&nbsp;a&nbsp;completely&nbsp;grounds-up&nbsp;attempt&nbsp;at<br>
designing&nbsp;a&nbsp;custom&nbsp;data&nbsp;loader.&nbsp;&nbsp;Ordinarily,&nbsp;if&nbsp;the&nbsp;basic&nbsp;format&nbsp;of&nbsp;how&nbsp;the&nbsp;dataset<br>
is&nbsp;stored&nbsp;is&nbsp;similar&nbsp;to&nbsp;one&nbsp;of&nbsp;the&nbsp;datasets&nbsp;that&nbsp;the&nbsp;Torchvision&nbsp;module&nbsp;knows&nbsp;about,<br>
you&nbsp;can&nbsp;go&nbsp;ahead&nbsp;and&nbsp;use&nbsp;that&nbsp;for&nbsp;your&nbsp;own&nbsp;dataset.&nbsp;&nbsp;At&nbsp;worst,&nbsp;you&nbsp;may&nbsp;need&nbsp;to&nbsp;carry<br>
out&nbsp;some&nbsp;light&nbsp;customizations&nbsp;depending&nbsp;on&nbsp;the&nbsp;number&nbsp;of&nbsp;classes&nbsp;involved,&nbsp;etc.<br>
&nbsp;<br>
However,&nbsp;if&nbsp;the&nbsp;underlying&nbsp;dataset&nbsp;is&nbsp;stored&nbsp;in&nbsp;a&nbsp;manner&nbsp;that&nbsp;does&nbsp;not&nbsp;look&nbsp;like<br>
anything&nbsp;in&nbsp;Torchvision,&nbsp;you&nbsp;have&nbsp;no&nbsp;choice&nbsp;but&nbsp;to&nbsp;supply&nbsp;yourself&nbsp;all&nbsp;of&nbsp;the&nbsp;data<br>
loading&nbsp;infrastructure.&nbsp;&nbsp;That&nbsp;is&nbsp;what&nbsp;this&nbsp;inner&nbsp;class&nbsp;of&nbsp;the&nbsp;main&nbsp;DLStudio&nbsp;class&nbsp;<br>
is&nbsp;all&nbsp;about.<br>
&nbsp;<br>
The&nbsp;custom&nbsp;data&nbsp;loading&nbsp;exercise&nbsp;here&nbsp;is&nbsp;related&nbsp;to&nbsp;a&nbsp;dataset&nbsp;called&nbsp;PurdueShapes5<br>
that&nbsp;contains&nbsp;32x32&nbsp;images&nbsp;of&nbsp;binary&nbsp;shapes&nbsp;belonging&nbsp;to&nbsp;the&nbsp;following&nbsp;five&nbsp;classes:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;&nbsp;rectangle<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;&nbsp;triangle<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.&nbsp;&nbsp;disk<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.&nbsp;&nbsp;oval<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.&nbsp;&nbsp;star<br>
&nbsp;<br>
The&nbsp;dataset&nbsp;was&nbsp;generated&nbsp;by&nbsp;randomizing&nbsp;the&nbsp;sizes&nbsp;and&nbsp;the&nbsp;orientations&nbsp;of&nbsp;these<br>
five&nbsp;patterns.&nbsp;&nbsp;Since&nbsp;the&nbsp;patterns&nbsp;are&nbsp;rotated&nbsp;with&nbsp;a&nbsp;very&nbsp;simple&nbsp;non-interpolating<br>
transform,&nbsp;just&nbsp;the&nbsp;act&nbsp;of&nbsp;random&nbsp;rotations&nbsp;can&nbsp;introduce&nbsp;boundary&nbsp;and&nbsp;even&nbsp;interior<br>
noise&nbsp;in&nbsp;the&nbsp;patterns.<br>
&nbsp;<br>
Each&nbsp;32x32&nbsp;image&nbsp;is&nbsp;stored&nbsp;in&nbsp;the&nbsp;dataset&nbsp;as&nbsp;the&nbsp;following&nbsp;list:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[R,&nbsp;G,&nbsp;B,&nbsp;Bbox,&nbsp;Label]<br>
where<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;R&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:&nbsp;&nbsp;&nbsp;is&nbsp;a&nbsp;1024&nbsp;element&nbsp;list&nbsp;of&nbsp;the&nbsp;values&nbsp;for&nbsp;the&nbsp;red&nbsp;component<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;the&nbsp;color&nbsp;at&nbsp;all&nbsp;the&nbsp;pixels<br>
&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:&nbsp;&nbsp;&nbsp;the&nbsp;same&nbsp;as&nbsp;above&nbsp;but&nbsp;for&nbsp;the&nbsp;green&nbsp;component&nbsp;of&nbsp;the&nbsp;color<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;G&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:&nbsp;&nbsp;&nbsp;the&nbsp;same&nbsp;as&nbsp;above&nbsp;but&nbsp;for&nbsp;the&nbsp;blue&nbsp;component&nbsp;of&nbsp;the&nbsp;color<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bbox&nbsp;&nbsp;:&nbsp;&nbsp;&nbsp;a&nbsp;list&nbsp;like&nbsp;[x1,y1,x2,y2]&nbsp;that&nbsp;defines&nbsp;the&nbsp;bounding&nbsp;box&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;the&nbsp;object&nbsp;in&nbsp;the&nbsp;image<br>
&nbsp;&nbsp;&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Label&nbsp;:&nbsp;&nbsp;&nbsp;the&nbsp;shape&nbsp;of&nbsp;the&nbsp;object<br>
&nbsp;<br>
I&nbsp;serialize&nbsp;the&nbsp;dataset&nbsp;with&nbsp;Python's&nbsp;pickle&nbsp;module&nbsp;and&nbsp;then&nbsp;compress&nbsp;it&nbsp;with&nbsp;the<br>
gzip&nbsp;module.<br>
&nbsp;<br>
You&nbsp;will&nbsp;find&nbsp;the&nbsp;following&nbsp;dataset&nbsp;directories&nbsp;in&nbsp;the&nbsp;"data"&nbsp;subdirectory&nbsp;of<br>
Examples&nbsp;in&nbsp;the&nbsp;DLStudio&nbsp;distro:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PurdueShapes5-10000-train.gz<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PurdueShapes5-1000-test.gz<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PurdueShapes5-20-train.gz<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PurdueShapes5-20-test.gz&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>
&nbsp;<br>
The&nbsp;number&nbsp;that&nbsp;follows&nbsp;the&nbsp;main&nbsp;name&nbsp;string&nbsp;"PurdueShapes5-"&nbsp;is&nbsp;for&nbsp;the&nbsp;number&nbsp;of<br>
images&nbsp;in&nbsp;the&nbsp;dataset.<br>
&nbsp;<br>
You&nbsp;will&nbsp;find&nbsp;the&nbsp;last&nbsp;two&nbsp;datasets,&nbsp;with&nbsp;20&nbsp;images&nbsp;each,&nbsp;useful&nbsp;for&nbsp;debugging&nbsp;your<br>
logic&nbsp;for&nbsp;object&nbsp;detection&nbsp;and&nbsp;bounding-box&nbsp;regression.<br>
&nbsp;<br>
Class&nbsp;Path:&nbsp;&nbsp;&nbsp;DLStudio&nbsp;&nbsp;-&gt;&nbsp;&nbsp;CustomDataLoading</tt></dl>

<dl><dt><strong>DetectAndLocalize</strong> = &lt;class 'DLStudio.DLStudio.DetectAndLocalize'&gt;<dd><tt>The&nbsp;purpose&nbsp;of&nbsp;this&nbsp;inner&nbsp;class&nbsp;is&nbsp;to&nbsp;focus&nbsp;on&nbsp;object&nbsp;detection&nbsp;in&nbsp;images&nbsp;---&nbsp;as<br>
opposed&nbsp;to&nbsp;image&nbsp;classification.&nbsp;&nbsp;Most&nbsp;people&nbsp;would&nbsp;say&nbsp;that&nbsp;object&nbsp;detection&nbsp;is&nbsp;a<br>
more&nbsp;challenging&nbsp;problem&nbsp;than&nbsp;image&nbsp;classification&nbsp;because,&nbsp;in&nbsp;general,&nbsp;the&nbsp;former<br>
also&nbsp;requires&nbsp;localization.&nbsp;&nbsp;The&nbsp;simplest&nbsp;interpretation&nbsp;of&nbsp;what&nbsp;is&nbsp;meant&nbsp;by<br>
localization&nbsp;is&nbsp;that&nbsp;the&nbsp;code&nbsp;that&nbsp;carries&nbsp;out&nbsp;object&nbsp;detection&nbsp;must&nbsp;also&nbsp;output&nbsp;a<br>
bounding-box&nbsp;rectangle&nbsp;for&nbsp;the&nbsp;object&nbsp;that&nbsp;was&nbsp;detected.<br>
&nbsp;<br>
You&nbsp;will&nbsp;find&nbsp;in&nbsp;this&nbsp;inner&nbsp;class&nbsp;some&nbsp;examples&nbsp;of&nbsp;LOADnet&nbsp;classes&nbsp;meant&nbsp;for&nbsp;solving<br>
the&nbsp;object&nbsp;detection&nbsp;and&nbsp;localization&nbsp;problem.&nbsp;&nbsp;The&nbsp;acronym&nbsp;"LOAD"&nbsp;in&nbsp;"LOADnet"<br>
stands&nbsp;for<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"LOcalization&nbsp;And&nbsp;Detection"<br>
&nbsp;<br>
The&nbsp;different&nbsp;network&nbsp;examples&nbsp;included&nbsp;here&nbsp;are&nbsp;LOADnet1,&nbsp;LOADnet2,&nbsp;and&nbsp;LOADnet3.<br>
For&nbsp;now,&nbsp;only&nbsp;pay&nbsp;attention&nbsp;to&nbsp;LOADnet2&nbsp;since&nbsp;that's&nbsp;the&nbsp;class&nbsp;I&nbsp;have&nbsp;worked&nbsp;with<br>
the&nbsp;most&nbsp;for&nbsp;the&nbsp;1.0.7&nbsp;distribution.<br>
&nbsp;<br>
Class&nbsp;Path:&nbsp;&nbsp;&nbsp;DLStudio&nbsp;&nbsp;-&gt;&nbsp;&nbsp;DetectAndLocalize</tt></dl>

<dl><dt><strong>EmbeddingGenerator</strong> = &lt;class 'DLStudio.DLStudio.EmbeddingGenerator'&gt;</dl>

<dl><dt><strong>ExperimentsWithCIFAR</strong> = &lt;class 'DLStudio.DLStudio.ExperimentsWithCIFAR'&gt;<dd><tt>Class&nbsp;Path:&nbsp;&nbsp;DLStudio&nbsp;&nbsp;-&gt;&nbsp;&nbsp;ExperimentsWithCIFAR</tt></dl>

<dl><dt><strong>ExperimentsWithSequential</strong> = &lt;class 'DLStudio.DLStudio.ExperimentsWithSequential'&gt;<dd><tt>Demonstrates&nbsp;how&nbsp;to&nbsp;use&nbsp;the&nbsp;torch.nn.Sequential&nbsp;container&nbsp;class<br>
&nbsp;<br>
Class&nbsp;Path:&nbsp;&nbsp;DLStudio&nbsp;&nbsp;-&gt;&nbsp;&nbsp;ExperimentsWithSequential</tt></dl>

<dl><dt><strong>MasterDecoderWithMasking</strong> = &lt;class 'DLStudio.DLStudio.MasterDecoderWithMasking'&gt;<dd><tt>Borrowed&nbsp;from&nbsp;the&nbsp;Transformers&nbsp;module&nbsp;of&nbsp;DLStudio</tt></dl>

<dl><dt><strong>Net</strong> = &lt;class 'DLStudio.DLStudio.Net'&gt;</dl>

<dl><dt><strong>ScheduledOptim</strong> = &lt;class 'DLStudio.DLStudio.ScheduledOptim'&gt;<dd><tt>As&nbsp;in&nbsp;the&nbsp;Transformers&nbsp;module&nbsp;of&nbsp;DLStudio,&nbsp;for&nbsp;the&nbsp;scheduling&nbsp;of&nbsp;the&nbsp;learning&nbsp;rate<br>
during&nbsp;the&nbsp;warm-up&nbsp;phase&nbsp;of&nbsp;training&nbsp;TransformerFG,&nbsp;I&nbsp;have&nbsp;borrowed&nbsp;the&nbsp;class&nbsp;shown&nbsp;below<br>
from&nbsp;the&nbsp;GitHub&nbsp;code&nbsp;made&nbsp;available&nbsp;by&nbsp;Yu-Hsiang&nbsp;Huang&nbsp;at:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch">https://github.com/jadore801120/attention-is-all-you-need-pytorch</a></tt></dl>

<dl><dt><strong>SelfAttention</strong> = &lt;class 'DLStudio.DLStudio.SelfAttention'&gt;<dd><tt>Borrowed&nbsp;from&nbsp;the&nbsp;Transformers&nbsp;module&nbsp;of&nbsp;DLStudio</tt></dl>

<dl><dt><strong>SemanticSegmentation</strong> = &lt;class 'DLStudio.DLStudio.SemanticSegmentation'&gt;<dd><tt>The&nbsp;purpose&nbsp;of&nbsp;this&nbsp;inner&nbsp;class&nbsp;is&nbsp;to&nbsp;be&nbsp;able&nbsp;to&nbsp;use&nbsp;the&nbsp;DLStudio&nbsp;platform&nbsp;for<br>
experiments&nbsp;with&nbsp;semantic&nbsp;segmentation.&nbsp;&nbsp;At&nbsp;its&nbsp;simplest&nbsp;level,&nbsp;the&nbsp;purpose&nbsp;of<br>
semantic&nbsp;segmentation&nbsp;is&nbsp;to&nbsp;assign&nbsp;correct&nbsp;labels&nbsp;to&nbsp;the&nbsp;different&nbsp;objects&nbsp;in&nbsp;a<br>
scene,&nbsp;while&nbsp;localizing&nbsp;them&nbsp;at&nbsp;the&nbsp;same&nbsp;time.&nbsp;&nbsp;At&nbsp;a&nbsp;more&nbsp;sophisticated&nbsp;level,&nbsp;a<br>
system&nbsp;that&nbsp;carries&nbsp;out&nbsp;semantic&nbsp;segmentation&nbsp;should&nbsp;also&nbsp;output&nbsp;a&nbsp;symbolic<br>
expression&nbsp;based&nbsp;on&nbsp;the&nbsp;objects&nbsp;found&nbsp;in&nbsp;the&nbsp;image&nbsp;and&nbsp;their&nbsp;spatial&nbsp;relationships<br>
with&nbsp;one&nbsp;another.<br>
&nbsp;<br>
The&nbsp;workhorse&nbsp;of&nbsp;this&nbsp;inner&nbsp;class&nbsp;is&nbsp;the&nbsp;mUNet&nbsp;network&nbsp;that&nbsp;is&nbsp;based&nbsp;on&nbsp;the&nbsp;UNET<br>
network&nbsp;that&nbsp;was&nbsp;first&nbsp;proposed&nbsp;by&nbsp;Ronneberger,&nbsp;Fischer&nbsp;and&nbsp;Brox&nbsp;in&nbsp;the&nbsp;paper<br>
"U-Net:&nbsp;Convolutional&nbsp;Networks&nbsp;for&nbsp;Biomedical&nbsp;Image&nbsp;Segmentation".&nbsp;&nbsp;Their&nbsp;Unet<br>
extracts&nbsp;binary&nbsp;masks&nbsp;for&nbsp;the&nbsp;cell&nbsp;pixel&nbsp;blobs&nbsp;of&nbsp;interest&nbsp;in&nbsp;biomedical&nbsp;images.<br>
The&nbsp;output&nbsp;of&nbsp;their&nbsp;Unet&nbsp;can&nbsp;therefore&nbsp;be&nbsp;treated&nbsp;as&nbsp;a&nbsp;pixel-wise&nbsp;binary&nbsp;classifier<br>
at&nbsp;each&nbsp;pixel&nbsp;position.&nbsp;&nbsp;The&nbsp;mUnet&nbsp;class,&nbsp;on&nbsp;the&nbsp;other&nbsp;hand,&nbsp;is&nbsp;intended&nbsp;for<br>
segmenting&nbsp;out&nbsp;multiple&nbsp;objects&nbsp;simultaneously&nbsp;form&nbsp;an&nbsp;image.&nbsp;[A&nbsp;weaker&nbsp;reason&nbsp;for<br>
"Multi"&nbsp;in&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;class&nbsp;is&nbsp;that&nbsp;it&nbsp;uses&nbsp;skip&nbsp;connections&nbsp;not&nbsp;only&nbsp;across<br>
the&nbsp;two&nbsp;arms&nbsp;of&nbsp;the&nbsp;"U",&nbsp;but&nbsp;also&nbsp;also&nbsp;along&nbsp;the&nbsp;arms.&nbsp;&nbsp;The&nbsp;skip&nbsp;connections&nbsp;in&nbsp;the<br>
original&nbsp;Unet&nbsp;are&nbsp;only&nbsp;between&nbsp;the&nbsp;two&nbsp;arms&nbsp;of&nbsp;the&nbsp;U.&nbsp;&nbsp;In&nbsp;mUnet,&nbsp;each&nbsp;object&nbsp;type&nbsp;is<br>
assigned&nbsp;a&nbsp;separate&nbsp;channel&nbsp;in&nbsp;the&nbsp;output&nbsp;of&nbsp;the&nbsp;network.<br>
&nbsp;<br>
This&nbsp;version&nbsp;of&nbsp;DLStudio&nbsp;also&nbsp;comes&nbsp;with&nbsp;a&nbsp;new&nbsp;dataset,&nbsp;PurdueShapes5MultiObject,<br>
for&nbsp;experimenting&nbsp;with&nbsp;mUnet.&nbsp;&nbsp;Each&nbsp;image&nbsp;in&nbsp;this&nbsp;dataset&nbsp;contains&nbsp;a&nbsp;random&nbsp;number<br>
of&nbsp;selections&nbsp;from&nbsp;five&nbsp;different&nbsp;shapes,&nbsp;with&nbsp;the&nbsp;shapes&nbsp;being&nbsp;randomly&nbsp;scaled,<br>
oriented,&nbsp;and&nbsp;located&nbsp;in&nbsp;each&nbsp;image.&nbsp;&nbsp;The&nbsp;five&nbsp;different&nbsp;shapes&nbsp;are:&nbsp;rectangle,<br>
triangle,&nbsp;disk,&nbsp;oval,&nbsp;and&nbsp;star.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;Class&nbsp;Path:&nbsp;&nbsp;&nbsp;DLStudio&nbsp;&nbsp;-&gt;&nbsp;&nbsp;SemanticSegmentation</tt></dl>

<dl><dt><strong>TextClassification</strong> = &lt;class 'DLStudio.DLStudio.TextClassification'&gt;<dd><tt>The&nbsp;purpose&nbsp;of&nbsp;this&nbsp;inner&nbsp;class&nbsp;is&nbsp;to&nbsp;be&nbsp;able&nbsp;to&nbsp;use&nbsp;the&nbsp;DLStudio&nbsp;platform&nbsp;for&nbsp;simple&nbsp;<br>
experiments&nbsp;in&nbsp;text&nbsp;classification.&nbsp;&nbsp;Consider,&nbsp;for&nbsp;example,&nbsp;the&nbsp;problem&nbsp;of&nbsp;automatic&nbsp;<br>
classification&nbsp;of&nbsp;variable-length&nbsp;user&nbsp;feedback:&nbsp;you&nbsp;want&nbsp;to&nbsp;create&nbsp;a&nbsp;neural&nbsp;network<br>
that&nbsp;can&nbsp;label&nbsp;an&nbsp;uploaded&nbsp;product&nbsp;review&nbsp;of&nbsp;arbitrary&nbsp;length&nbsp;as&nbsp;positive&nbsp;or&nbsp;negative.&nbsp;&nbsp;<br>
One&nbsp;way&nbsp;to&nbsp;solve&nbsp;this&nbsp;problem&nbsp;is&nbsp;with&nbsp;a&nbsp;recurrent&nbsp;neural&nbsp;network&nbsp;in&nbsp;which&nbsp;you&nbsp;use&nbsp;a&nbsp;<br>
hidden&nbsp;state&nbsp;for&nbsp;characterizing&nbsp;a&nbsp;variable-length&nbsp;product&nbsp;review&nbsp;with&nbsp;a&nbsp;fixed-length&nbsp;<br>
state&nbsp;vector.&nbsp;&nbsp;This&nbsp;inner&nbsp;class&nbsp;allows&nbsp;you&nbsp;to&nbsp;carry&nbsp;out&nbsp;such&nbsp;experiments.<br>
&nbsp;<br>
Class&nbsp;Path:&nbsp;&nbsp;DLStudio&nbsp;-&gt;&nbsp;TextClassification</tt></dl>

<dl><dt><strong>TextClassificationWithEmbeddings</strong> = &lt;class 'DLStudio.DLStudio.TextClassificationWithEmbeddings'&gt;<dd><tt>The&nbsp;text&nbsp;processing&nbsp;class&nbsp;described&nbsp;previously,&nbsp;TextClassification,&nbsp;was&nbsp;based&nbsp;on<br>
using&nbsp;one-hot&nbsp;vectors&nbsp;for&nbsp;representing&nbsp;the&nbsp;words.&nbsp;&nbsp;The&nbsp;main&nbsp;challenge&nbsp;we&nbsp;faced<br>
with&nbsp;one-hot&nbsp;vectors&nbsp;was&nbsp;that&nbsp;the&nbsp;larger&nbsp;the&nbsp;size&nbsp;of&nbsp;the&nbsp;training&nbsp;dataset,&nbsp;the<br>
larger&nbsp;the&nbsp;size&nbsp;of&nbsp;the&nbsp;vocabulary,&nbsp;and,&nbsp;therefore,&nbsp;the&nbsp;larger&nbsp;the&nbsp;size&nbsp;of&nbsp;the<br>
one-hot&nbsp;vectors.&nbsp;&nbsp;The&nbsp;increase&nbsp;in&nbsp;the&nbsp;size&nbsp;of&nbsp;the&nbsp;one-hot&nbsp;vectors&nbsp;led&nbsp;to&nbsp;a<br>
model&nbsp;with&nbsp;a&nbsp;significantly&nbsp;larger&nbsp;number&nbsp;of&nbsp;learnable&nbsp;parameters&nbsp;---&nbsp;and,&nbsp;that,<br>
in&nbsp;turn,&nbsp;created&nbsp;a&nbsp;need&nbsp;for&nbsp;a&nbsp;still&nbsp;larger&nbsp;training&nbsp;dataset.&nbsp;&nbsp;Sounds&nbsp;like&nbsp;a&nbsp;classic<br>
example&nbsp;of&nbsp;a&nbsp;vicious&nbsp;circle.&nbsp;&nbsp;In&nbsp;this&nbsp;section,&nbsp;I&nbsp;use&nbsp;the&nbsp;idea&nbsp;of&nbsp;word&nbsp;embeddings<br>
to&nbsp;break&nbsp;out&nbsp;of&nbsp;this&nbsp;vicious&nbsp;circle.<br>
&nbsp;<br>
Word&nbsp;embeddings&nbsp;are&nbsp;fixed-sized&nbsp;numerical&nbsp;representations&nbsp;for&nbsp;words&nbsp;that&nbsp;are<br>
learned&nbsp;on&nbsp;the&nbsp;basis&nbsp;of&nbsp;the&nbsp;similarity&nbsp;of&nbsp;word&nbsp;contexts.&nbsp;&nbsp;The&nbsp;original&nbsp;and&nbsp;still<br>
the&nbsp;most&nbsp;famous&nbsp;of&nbsp;these&nbsp;representations&nbsp;are&nbsp;known&nbsp;as&nbsp;the&nbsp;word2vec<br>
embeddings.&nbsp;The&nbsp;embeddings&nbsp;that&nbsp;I&nbsp;use&nbsp;in&nbsp;this&nbsp;section&nbsp;consist&nbsp;of&nbsp;pre-trained<br>
300-element&nbsp;word&nbsp;vectors&nbsp;for&nbsp;3&nbsp;million&nbsp;words&nbsp;and&nbsp;phrases&nbsp;as&nbsp;learned&nbsp;from&nbsp;Google<br>
News&nbsp;reports.&nbsp;&nbsp;I&nbsp;access&nbsp;these&nbsp;embeddings&nbsp;through&nbsp;the&nbsp;popular&nbsp;Gensim&nbsp;library.<br>
&nbsp;<br>
Class&nbsp;Path:&nbsp;&nbsp;DLStudio&nbsp;-&gt;&nbsp;TextClassificationWithEmbeddings</tt></dl>

<dl><dt><strong>TransformerFG</strong> = &lt;class 'DLStudio.DLStudio.TransformerFG'&gt;<dd><tt>I&nbsp;have&nbsp;borrowed&nbsp;from&nbsp;the&nbsp;DLStudio's&nbsp;Transformers&nbsp;module.&nbsp;&nbsp;"FG"&nbsp;stands&nbsp;for&nbsp;"First&nbsp;Generation"&nbsp;---&nbsp;which&nbsp;is&nbsp;the&nbsp;Transformer<br>
as&nbsp;originally&nbsp;proposed&nbsp;by&nbsp;Vaswani&nbsp;et&nbsp;al.</tt></dl>

<dl><dt><strong>VAE</strong> = &lt;class 'DLStudio.DLStudio.VAE'&gt;<dd><tt>VAE&nbsp;stands&nbsp;for&nbsp;"Variational&nbsp;Auto&nbsp;Encoder".&nbsp;&nbsp;These&nbsp;days,&nbsp;you&nbsp;are&nbsp;more&nbsp;likely&nbsp;to&nbsp;see&nbsp;it<br>
written&nbsp;as&nbsp;"variational&nbsp;autoencoder".&nbsp;&nbsp;I&nbsp;consider&nbsp;VAE&nbsp;as&nbsp;one&nbsp;of&nbsp;the&nbsp;foundational&nbsp;neural<br>
architectures&nbsp;in&nbsp;Deep&nbsp;Learning.&nbsp;&nbsp;VAE&nbsp;is&nbsp;based&nbsp;on&nbsp;the&nbsp;new&nbsp;celebrated&nbsp;2014&nbsp;paper&nbsp;<br>
"Auto-Encoding&nbsp;Variational&nbsp;Bayes"&nbsp;by&nbsp;Kingma&nbsp;and&nbsp;Welling.&nbsp;&nbsp;The&nbsp;idea&nbsp;is&nbsp;for&nbsp;the&nbsp;Encoder&nbsp;<br>
part&nbsp;of&nbsp;an&nbsp;Encoder-Decoder&nbsp;pair&nbsp;to&nbsp;learn&nbsp;the&nbsp;probability&nbsp;distribution&nbsp;for&nbsp;the&nbsp;Latent&nbsp;<br>
Space&nbsp;Representation&nbsp;of&nbsp;a&nbsp;training&nbsp;dataset.&nbsp;&nbsp;Described&nbsp;loosely,&nbsp;the&nbsp;latent&nbsp;vector&nbsp;z&nbsp;for&nbsp;<br>
an&nbsp;input&nbsp;image&nbsp;x&nbsp;would&nbsp;be&nbsp;the&nbsp;"essence"&nbsp;of&nbsp;what&nbsp;x&nbsp;is&nbsp;depicting.&nbsp;&nbsp;Presumably,&nbsp;after&nbsp;the<br>
latent&nbsp;distribution&nbsp;has&nbsp;been&nbsp;learned,&nbsp;the&nbsp;Decoder&nbsp;should&nbsp;be&nbsp;able&nbsp;to&nbsp;transform&nbsp;any&nbsp;"noise"&nbsp;<br>
vector&nbsp;sampled&nbsp;from&nbsp;the&nbsp;latent&nbsp;distribution&nbsp;and&nbsp;convert&nbsp;it&nbsp;into&nbsp;the&nbsp;sort&nbsp;of&nbsp;output&nbsp;you&nbsp;<br>
would&nbsp;see&nbsp;during&nbsp;the&nbsp;training&nbsp;process.<br>
&nbsp;<br>
In&nbsp;case&nbsp;you&nbsp;are&nbsp;wondering&nbsp;about&nbsp;the&nbsp;dimensionality&nbsp;of&nbsp;the&nbsp;Latent&nbsp;Space,&nbsp;consider&nbsp;the&nbsp;case<br>
that&nbsp;the&nbsp;input&nbsp;images&nbsp;are&nbsp;eventually&nbsp;converted&nbsp;into&nbsp;8x8&nbsp;pixel&nbsp;arrays,&nbsp;with&nbsp;each&nbsp;pixel<br>
represented&nbsp;by&nbsp;a&nbsp;128-dimensional&nbsp;embedding.&nbsp;&nbsp;In&nbsp;a&nbsp;vectorized&nbsp;representation,&nbsp;this&nbsp;implies<br>
an&nbsp;8192-dimensional&nbsp;space&nbsp;for&nbsp;the&nbsp;Latent&nbsp;Distribution.&nbsp;&nbsp;The&nbsp;mean&nbsp;(mu)&nbsp;and&nbsp;the&nbsp;log-variance<br>
values&nbsp;(logvar)&nbsp;values&nbsp;learned&nbsp;by&nbsp;the&nbsp;Encoder&nbsp;would&nbsp;represent&nbsp;vectors&nbsp;in&nbsp;an&nbsp;8,192&nbsp;<br>
dimensional&nbsp;space.&nbsp;&nbsp;The&nbsp;Decoder's&nbsp;job&nbsp;would&nbsp;be&nbsp;sample&nbsp;this&nbsp;distribution&nbsp;and&nbsp;attempt&nbsp;a<br>
reconstruction&nbsp;of&nbsp;what&nbsp;the&nbsp;user&nbsp;wants&nbsp;to&nbsp;see&nbsp;at&nbsp;the&nbsp;output&nbsp;of&nbsp;the&nbsp;Decoder.<br>
&nbsp;<br>
As&nbsp;you&nbsp;can&nbsp;see,&nbsp;the&nbsp;VAE&nbsp;class&nbsp;is&nbsp;derived&nbsp;from&nbsp;the&nbsp;parent&nbsp;class&nbsp;Autoencoder.&nbsp;&nbsp;Bulk&nbsp;of&nbsp;the<br>
computing&nbsp;in&nbsp;VAE&nbsp;is&nbsp;done&nbsp;through&nbsp;the&nbsp;functionality&nbsp;packed&nbsp;into&nbsp;the&nbsp;Autoencoder&nbsp;class.<br>
Therefore,&nbsp;in&nbsp;order&nbsp;to&nbsp;fully&nbsp;understand&nbsp;the&nbsp;VAE&nbsp;implementation&nbsp;here,&nbsp;your&nbsp;starting&nbsp;point<br>
should&nbsp;be&nbsp;the&nbsp;code&nbsp;for&nbsp;the&nbsp;Autoencoder&nbsp;class.&nbsp;&nbsp;<br>
&nbsp;<br>
Class&nbsp;Path:&nbsp;&nbsp;&nbsp;DLStudio&nbsp;&nbsp;-&gt;&nbsp;&nbsp;VAE</tt></dl>

<dl><dt><strong>VQGAN</strong> = &lt;class 'DLStudio.DLStudio.VQGAN'&gt;</dl>

<dl><dt><strong>VQVAE</strong> = &lt;class 'DLStudio.DLStudio.VQVAE'&gt;<dd><tt>VQVAE&nbsp;is&nbsp;an&nbsp;important&nbsp;architecture&nbsp;in&nbsp;deep&nbsp;learning&nbsp;because&nbsp;it&nbsp;teaches&nbsp;us&nbsp;about&nbsp;what<br>
has&nbsp;come&nbsp;to&nbsp;be&nbsp;known&nbsp;as&nbsp;"Codebook&nbsp;Learning"&nbsp;for&nbsp;more&nbsp;efficient&nbsp;discrete&nbsp;representation<br>
of&nbsp;images&nbsp;with&nbsp;a&nbsp;finite&nbsp;vocabulary&nbsp;of&nbsp;embedding&nbsp;vectors.<br>
&nbsp;<br>
VQVAE&nbsp;stands&nbsp;for&nbsp;"Vector&nbsp;Quantized&nbsp;Variational&nbsp;Auto&nbsp;Encoder",&nbsp;which&nbsp;is&nbsp;also&nbsp;frequently<br>
represented&nbsp;by&nbsp;the&nbsp;acronym&nbsp;VQ-VAE.&nbsp;&nbsp;The&nbsp;concept&nbsp;of&nbsp;VQ-VAE&nbsp;was&nbsp;formulated&nbsp;in&nbsp;the&nbsp;2018<br>
publication&nbsp;"Neural&nbsp;Discrete&nbsp;Representation&nbsp;Learning"&nbsp;by&nbsp;van&nbsp;den&nbsp;Oord,&nbsp;Vinyals,&nbsp;and<br>
Kavukcuoglu.<br>
&nbsp;<br>
For&nbsp;the&nbsp;case&nbsp;of&nbsp;images,&nbsp;VQ-VAE&nbsp;means&nbsp;that&nbsp;we&nbsp;want&nbsp;to&nbsp;represent&nbsp;an&nbsp;input&nbsp;image&nbsp;using&nbsp;a<br>
user-specified&nbsp;number&nbsp;of&nbsp;embedding&nbsp;vectors.&nbsp;&nbsp;You&nbsp;could&nbsp;think&nbsp;of&nbsp;the&nbsp;set&nbsp;of&nbsp;embedding<br>
vectors&nbsp;as&nbsp;constituting&nbsp;a&nbsp;fixed-size&nbsp;vocabulary&nbsp;for&nbsp;representing&nbsp;the&nbsp;input&nbsp;data.<br>
&nbsp;<br>
To&nbsp;make&nbsp;the&nbsp;definition&nbsp;of&nbsp;Codebook&nbsp;Learning&nbsp;more&nbsp;specific,&nbsp;say&nbsp;we&nbsp;are&nbsp;using&nbsp;an<br>
Encoder-Decoder&nbsp;to&nbsp;create&nbsp;such&nbsp;a&nbsp;fixed-vocabulary&nbsp;based&nbsp;representation&nbsp;for&nbsp;the&nbsp;images.<br>
Let's&nbsp;assume&nbsp;that&nbsp;the&nbsp;Encoder&nbsp;converts&nbsp;each&nbsp;input&nbsp;batch&nbsp;of&nbsp;images&nbsp;into&nbsp;a&nbsp;(B,C,H,W)<br>
shaped&nbsp;tensor&nbsp;where&nbsp;the&nbsp;height&nbsp;H&nbsp;and&nbsp;the&nbsp;width&nbsp;W&nbsp;are&nbsp;likely&nbsp;to&nbsp;be&nbsp;small&nbsp;numbers,&nbsp;say&nbsp;8<br>
each,&nbsp;and&nbsp;C&nbsp;is&nbsp;likely&nbsp;to&nbsp;be,&nbsp;say,&nbsp;128.&nbsp;&nbsp;Let's&nbsp;also&nbsp;say&nbsp;that&nbsp;the&nbsp;batch&nbsp;size&nbsp;is&nbsp;256.<br>
&nbsp;<br>
The&nbsp;total&nbsp;number&nbsp;of&nbsp;pixels&nbsp;in&nbsp;all&nbsp;the&nbsp;batch&nbsp;instances&nbsp;at&nbsp;the&nbsp;output&nbsp;of&nbsp;the&nbsp;Encoder&nbsp;will<br>
be&nbsp;B*H*W.&nbsp;&nbsp;I'll&nbsp;represent&nbsp;this&nbsp;number&nbsp;of&nbsp;pixels&nbsp;with&nbsp;the&nbsp;notation&nbsp;BHW.&nbsp;&nbsp;For&nbsp;the&nbsp;example<br>
numbers&nbsp;used&nbsp;above,&nbsp;BHW&nbsp;will&nbsp;be&nbsp;equal&nbsp;to&nbsp;256*8*8&nbsp;=&nbsp;16384.<br>
&nbsp;<br>
Taking&nbsp;cognizance&nbsp;of&nbsp;the&nbsp;channel&nbsp;axis,&nbsp;we&nbsp;can&nbsp;say&nbsp;that&nbsp;each&nbsp;of&nbsp;the&nbsp;16,384&nbsp;pixels&nbsp;at&nbsp;the<br>
output&nbsp;of&nbsp;the&nbsp;Encoder&nbsp;is&nbsp;represented&nbsp;by&nbsp;a&nbsp;128&nbsp;element&nbsp;vector&nbsp;along&nbsp;the&nbsp;channel&nbsp;axis.<br>
&nbsp;<br>
As&nbsp;things&nbsp;stand,&nbsp;each&nbsp;C-dimensional&nbsp;pixel&nbsp;based&nbsp;vector&nbsp;at&nbsp;the&nbsp;output&nbsp;of&nbsp;the&nbsp;Encoder&nbsp;will<br>
be&nbsp;a&nbsp;continuous&nbsp;valued&nbsp;vector.<br>
&nbsp;<br>
The&nbsp;goal&nbsp;of&nbsp;VQ-VAE&nbsp;is&nbsp;define&nbsp;a&nbsp;Codebook&nbsp;of&nbsp;K&nbsp;vectors,&nbsp;each&nbsp;of&nbsp;dimension&nbsp;D,&nbsp;with&nbsp;the<br>
idea&nbsp;that&nbsp;each&nbsp;of&nbsp;the&nbsp;C-dimensional&nbsp;BHW&nbsp;vectors&nbsp;at&nbsp;the&nbsp;output&nbsp;of&nbsp;the&nbsp;Encode&nbsp;will&nbsp;be<br>
replaced&nbsp;by&nbsp;the&nbsp;closest&nbsp;of&nbsp;the&nbsp;K&nbsp;D-dimensional&nbsp;vectors&nbsp;in&nbsp;the&nbsp;Codebook.&nbsp;&nbsp;For&nbsp;practical<br>
reasons,&nbsp;we&nbsp;require&nbsp;D=C.<br>
&nbsp;<br>
The&nbsp;Decoder's&nbsp;job&nbsp;then&nbsp;is&nbsp;to&nbsp;try&nbsp;its&nbsp;best&nbsp;to&nbsp;recreate&nbsp;the&nbsp;input&nbsp;using&nbsp;the&nbsp;Codebook<br>
approximations&nbsp;at&nbsp;the&nbsp;output&nbsp;of&nbsp;the&nbsp;Encoder.<br>
&nbsp;<br>
The&nbsp;goal&nbsp;of&nbsp;VQ-VAE&nbsp;is&nbsp;to&nbsp;demonstrate&nbsp;that&nbsp;it&nbsp;is&nbsp;possible&nbsp;to&nbsp;learn&nbsp;a&nbsp;Codebook&nbsp;with&nbsp;K<br>
elements&nbsp;that&nbsp;can&nbsp;subsequently&nbsp;be&nbsp;used&nbsp;to&nbsp;represent&nbsp;any&nbsp;input.<br>
&nbsp;<br>
You&nbsp;can&nbsp;think&nbsp;of&nbsp;the&nbsp;learned&nbsp;Codebook&nbsp;vectors&nbsp;as&nbsp;the&nbsp;quantized&nbsp;versions&nbsp;of&nbsp;what&nbsp;the<br>
Encoder&nbsp;presents&nbsp;at&nbsp;its&nbsp;output.<br>
&nbsp;<br>
As&nbsp;you&nbsp;can&nbsp;see,&nbsp;the&nbsp;VQVAE&nbsp;class&nbsp;is&nbsp;derived&nbsp;from&nbsp;the&nbsp;parent&nbsp;class&nbsp;Autoencoder.&nbsp;&nbsp;Bulk&nbsp;of&nbsp;the<br>
computing&nbsp;in&nbsp;VQVAE&nbsp;is&nbsp;done&nbsp;through&nbsp;the&nbsp;functionality&nbsp;packed&nbsp;into&nbsp;the&nbsp;Autoencoder&nbsp;class.<br>
Therefore,&nbsp;in&nbsp;order&nbsp;to&nbsp;fully&nbsp;understand&nbsp;the&nbsp;VQVAE&nbsp;implementation&nbsp;here,&nbsp;your&nbsp;starting&nbsp;point<br>
should&nbsp;be&nbsp;the&nbsp;code&nbsp;for&nbsp;the&nbsp;Autoencoder&nbsp;class.&nbsp;&nbsp;<br>
&nbsp;<br>
Note&nbsp;that&nbsp;the&nbsp;VQVAE&nbsp;code&nbsp;presented&nbsp;here&nbsp;is&nbsp;still&nbsp;tentative.&nbsp;&nbsp;Most&nbsp;of&nbsp;the&nbsp;heavy&nbsp;lifting<br>
at&nbsp;the&nbsp;moment&nbsp;is&nbsp;done&nbsp;by&nbsp;the&nbsp;two&nbsp;Vector&nbsp;Representation&nbsp;classes&nbsp;I&nbsp;have&nbsp;borrowed&nbsp;from<br>
"zalandoresearch"&nbsp;at&nbsp;GitHub:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://github.com/zalandoresearch/pytorch-vq-vae">https://github.com/zalandoresearch/pytorch-vq-vae</a><br>
&nbsp;<br>
Class&nbsp;Path:&nbsp;&nbsp;&nbsp;DLStudio&nbsp;&nbsp;-&gt;&nbsp;&nbsp;VQVAE</tt></dl>

</td></tr></table></td></tr></table><p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#eeaa77">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial"><big><strong>Functions</strong></big></font></td></tr>
    
<tr><td bgcolor="#eeaa77"><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</tt></td><td>&nbsp;</td>
<td width="100%"><dl><dt><a name="-static_var"><strong>static_var</strong></a>(varname, value)</dt><dd><tt>##&nbsp;Python&nbsp;does&nbsp;not&nbsp;have&nbsp;a&nbsp;decorator&nbsp;for&nbsp;declaring&nbsp;static&nbsp;vars.&nbsp;&nbsp;But&nbsp;you&nbsp;can&nbsp;use<br>
##&nbsp;the&nbsp;following&nbsp;for&nbsp;achieving&nbsp;the&nbsp;same&nbsp;effect.&nbsp;&nbsp;I&nbsp;believe&nbsp;I&nbsp;saw&nbsp;it&nbsp;at&nbsp;stackoverflow.com:</tt></dd></dl>
</td></tr></table><p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#55aa55">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial"><big><strong>Data</strong></big></font></td></tr>
    
<tr><td style="bgcolor:#55aa55;"></td><td>&nbsp;</td>
<td style="width:100%;"><strong>__author__</strong> = 'Avinash Kak (kak@purdue.edu)'<br>
<strong>__copyright__</strong> = '(C) 2025 Avinash Kak. Python Software Foundation.'<br>
<strong>__date__</strong> = '2025-May-28'<br>
<strong>__url__</strong> = 'https://engineering.purdue.edu/kak/distDT/DLStudio-2.5.5.html'<br>
<strong>__version__</strong> = '2.5.5'</td></tr></table>
<table style="width:100%; border-collapse:collapse; border-spacking:0; padding:2; border:0;">
<tr style="bgcolor:#7799ee;">
<td style="colspan:3; vertical-align:bottom;">&nbsp;<br>
<span style="color:#ffffff; font-family:helvetica, arial; font-size:large;"><strong>Author</strong></big></span></td></tr>
<tr><td style="bgcolor:#7799ee;"></td><td>&nbsp;</td>
<td style="width:100%;">Avinash&nbsp;Kak&nbsp;(kak@purdue.edu)</td></tr></table>
</body></html>

